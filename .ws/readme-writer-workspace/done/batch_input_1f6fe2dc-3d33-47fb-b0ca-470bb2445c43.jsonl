{"custom_id":"request-0","method":"POST","url":"/v1/chat/completions","body":{"max_completion_tokens":8192,"messages":[{"content":"We would like you to write a README.md for a rust crate with maximally helpful content.\nAdditionally, we would like you to generate several fields from the Cargo.toml package section for this crate.\nWe will show you the strict json output format we would like you to provide.\nPlease tailor your response for an apex consumer of maximal intelligence.\nA reader of this README.md should have a good sense of what the crate does and how to use it after reading what you write.\nPlease include a messaage indicating that this README.md file was generated by an AI model and may not be 100% accurate however it should be pretty good.\n\n\nCarefully read and parse the information we sent you.\n\nWe want to generate all information items requested by our schema:\n\n- Read the description of each field. Use it, along with the content we sent you to generate an appropriate value for each.\n- Ensure that each generated field optimally serves its desired purpose.\n- Each item should be deep, detailed, and specific. Use optimally descriptive and useful language. Do not be too verbose.\n- Do not add any extra keys to the generated object. Generate precisely what we ask of you: no more, no less.\n- For fields which are of an enum type: pick *exactly one variant*. Do not generate any information for the unselected variants.\n- For array fields: supply a JSON list [ ... ] of items, each conforming to the correct item type.\n- For map fields: Provide a JSON object { \"key_as_string\": <value>, ... }. Keys must be valid JSON strings. Keys and Values must conform to the key and value schemas we provide to you. Fill all subfields of each and every map value.\n- If we ask you to justify a map field, provide your justification/confidence score for the entire map. Do not attempt to separately justify each map element.\n- Likewise, if we ask you to justify an array field, provide your justification/confidence score for the entire array. Do not attempt to separately justify each element.\n- Provide numeric fields as real JSON numbers (not strings).\n- For string fields: provide the value precisely as a string literal. If optional, you may set it to null.\n- If we ask you to justify or provide confidence scores for the fields of a structure, please do so carefully. For each field we ask you to justify, please explain or justify your choice clearly. Confidence scores must be specified as real JSON numbers in the range `[0..1]`.\n- If we ask to you generate an optional field, either fill it or set it to null.\n- If we ask you to justify your choice of an enum variant, please explain your choice clearly. Do not justify your decision to leave a variant unselected. Provide your justification as a JSON string. Provide your confidence score as a real JSON number in the range `[0..1]`.\n- In terms of the JSON format we would like you to provide, the object itself will be deserialied based on the following schema. \n- We begin exactly at the top: the data should not be nested under a parent named 'fields' or anything similar.\n\n## Here is the schema to use:\n\nObject {\n    \"fields\": Object {\n        \"crate_name\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be the name of our crate, *verbatim*, trimmed for use as a toml field\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"full_readme_markdown\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be valid markdown representing the readme for this crate.\\\\n\\\\n Please make it terse, useful, and designed for an industrious commercially savy consumer of industrious nature. \\\\n\\\\n Advanced vocabulary and concepts are welcome. \\\\n\\\\n If this crate involves concepts from mathematics and/or physics, please describe and document them here.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_categories\": Object {\n            \"generation_instructions\": String(\"\\\"These categories will be used in our Cargo.toml file.\\\\n\\\\n It is important that users of this crate can find it on crates.io.\\\\n\\\\n Choose maximum 5 categories and make sure they are actually meaningful for our crate.\\\\n\\\\n These categories should only be chosen from among the legal crates.io categories. \\\\n\\\\n The ONLY legal categoreis are:\\\\n\\\\n``\\\\n accessibility\\\\n aerospace\\\\n algorithms\\\\n api-bindings\\\\n asynchronous\\\\n authentication\\\\n caching\\\\n command-line-interface\\\\n command-line-utilities\\\\n compilers\\\\n compression\\\\n computer-vision\\\\n concurrency\\\\n config\\\\n cryptography\\\\n data-structures\\\\n database\\\\n database-implementations\\\\n date-and-time\\\\n development-tools\\\\n email\\\\n embedded\\\\n emulators\\\\n encoding\\\\n external-ffi-bindings\\\\n filesystem\\\\n finance\\\\n game-development\\\\n game-engines\\\\n games\\\\n graphics\\\\n gui\\\\n hardware-support\\\\n internationalization\\\\n localization\\\\n mathematics\\\\n memory-management\\\\n multimedia\\\\n network-programming\\\\n no-std\\\\n os\\\\n parser-implementations\\\\n parsing\\\\n rendering\\\\n rust-patterns\\\\n science\\\\n simulation\\\\n template-engine\\\\n text-editors\\\\n text-processing\\\\n value-formatting\\\\n virtualization\\\\n visualization\\\\n wasm\\\\n web-programming\\\\n ``\\\\n\\\\n DO NOT specify a category that is not on this list.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n        \"package_description\": Object {\n            \"generation_instructions\": String(\"\\\"We will place this package_description in our Cargo.toml file so the users of this crate\\\\n know what it does.\\\\n\\\\n The discription should be technical, clear, and useful. It should be terse.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_keywords\": Object {\n            \"generation_instructions\": String(\"\\\"These keywords will be used in our Cargo.toml file so the users of this crate can easily\\\\n find it in the package system. Keywords should be useful, varied, meaningful, and reflect\\\\n the contents of our crate. They should make it easy to find. Please provide exactly five.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n    },\n    \"struct_docs\": String(\" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\"),\n    \"struct_name\": String(\"AiReadmeWriterDesiredOutput\"),\n    \"type\": String(\"struct\"),\n}\n\n## Please keep the following in mind:\n\nEntry phrasing:\n\n- Ensure entries are concise and focused.\n- Ensure there are no vague phrases (e.g., avoid starting with \"Illustrates the something something...\"; instead, use direct details like \"The hanging green vines on the garden wall\").\n\nEnsure Clarity and Consistency:\n\n- Use clear, grammatically correct sentences.\n- Maintain a consistent tone and style throughout.\n\nEnrich Content:\n\n- Communicate additional information where appropriate to enhance depth and value.\n- If a mathematical, physical, or otherwise technical background is helpful, please provide it.\n- Ensure that the content is comprehensive and meets all requirements.\n\nFocus Language:\n\n- Use deliberate and precise language.\n- Avoid vague verbs and keep the maximally intelligent and detail oriented audience in mind. \n- We explicitly ban the words `hush`, `subtle`, `mythic`, and `aura`. Please keep your vocabulary choices rigorous.\n- Do not use modern cultural references \n- Do not generically reference ideas which do not belong in the global environment.\n\n## Present the Final JSON:\n\n- Format the output as a JSON object.\n- Include all fields properly named from the schema we sent you.\n\nEnsure Proper Formatting:\n\n- Use proper JSON syntax with keys and arrays.\n- Ensure that all entries are correctly placed in their respective places.\n\nProvide only the final JSON data as specified by the schema. \n\nDo not include any schema generation instructions, meta-fields, nested-templates (unless directly asked) or extraneous information. \n\nProvide only the populated fields and their values, conforming exactly to the required structure without embedding any intermediate annotations or placeholders.\n\nYour output should only consist of the JSON object. do *not* include a preamble or postamble to your response. \n\nWe seek to successfully parse your response directly as JSON.","role":"system"},{"content":"We have a rust crate named 'bitcoin-get-json-token' 0.1.0. \n\nHere is the interface:\n#[derive(Copy,Clone,Debug,PartialEq,Eq,Hash)]\npub enum JTokenType {\n    JTOK_ERR,\n    JTOK_NONE,\n    JTOK_OBJ_OPEN,\n    JTOK_OBJ_CLOSE,\n    JTOK_ARR_OPEN,\n    JTOK_ARR_CLOSE,\n    JTOK_COLON,\n    JTOK_COMMA,\n    JTOK_KW_NULL,\n    JTOK_KW_TRUE,\n    JTOK_KW_FALSE,\n    JTOK_NUMBER,\n    JTOK_STRING,\n}\n\n/// Return `true` when the byte (expressed as `i32`)\n/// is one of the four JSON‑defined whitespace\n/// characters: space, horizontal‑tab, line‑feed, or carriage‑return.\n///\n/// See RFC 7159 §2.\n#[instrument(level = \"trace\", skip_all)]\npub fn json_isspace(ch: i32) -> bool {\n    let is_space = matches!(ch, 0x20 | 0x09 | 0x0A | 0x0D);\n    trace!(chr = ch, is_space);\n    is_space\n}\n\n\n/// Lightweight helper – returns **true** iff `s` is *exactly* one\n/// syntactically valid JSON number.\n#[instrument(level = \"trace\", skip_all)]\npub fn valid_num_str(s: &str) -> bool {\n    let mut tok_val  = String::new();\n    let mut consumed = 0u32;\n    let tok = get_json_token(\n        &mut tok_val,\n        &mut consumed,\n        s.as_ptr(),\n        unsafe { s.as_ptr().add(s.len()) },\n    );\n\n    let ok = tok == JTokenType::JTOK_NUMBER && consumed as usize == s.len();\n    trace!(input = s, ?tok, consumed, ok, \"valid_num_str verdict\");\n    ok\n}\n\n\n/**\n| Convert a hexadecimal string (given by\n| *first* … *last*) into an unsigned `u32`.\n| The function writes the parsed value into\n| *out* and returns a pointer to the first\n| byte that was **not** consumed.\n*/\n#[instrument(level = \"trace\", skip_all)]\npub fn hatoui(first: *const u8, last: *const u8, out: &mut u32) -> *const u8 {\n    let mut result: u32 = 0;\n    let mut ptr = first;\n\n    unsafe {\n        while ptr != last {\n            let ch = *ptr as char;\n            let digit_opt = match ch {\n                '0'..='9' => Some(ch as u32 - '0' as u32),\n                'a'..='f' => Some(ch as u32 - 'a' as u32 + 10),\n                'A'..='F' => Some(ch as u32 - 'A' as u32 + 10),\n                _ => None,\n            };\n\n            match digit_opt {\n                Some(d) => {\n                    result = result.wrapping_mul(16).wrapping_add(d);\n                    ptr = ptr.add(1);\n                }\n                None => break,\n            }\n        }\n\n        *out = result;\n        ptr\n    }\n}\n\n\n#[instrument(level = \"trace\", skip(token_val, consumed))]\npub fn get_json_token(\n    token_val: &mut String,\n    consumed:  &mut u32,\n    raw:       *const u8,\n    end:       *const u8\n) -> JTokenType\n{\n    unsafe {\n        token_val.clear();\n        *consumed = 0;\n\n        /* ---------- 1 / 5  skip leading whitespace ---------- */\n        let mut p        = skip_ws_nul(raw, end);\n        let raw_start    = p;\n\n        /* ---------- 2 / 5  try structural ---------- */\n        /* record how many bytes were skipped */\n        let skipped = (raw_start as usize - raw as usize) as u32;\n\n        /* ---------- 2 / 5  try structural ---------- */\n        if let Some((tok, after, _)) = lex_structural(p, end) {\n            *consumed = bytes_consumed(raw, after);          // <── NEW\n            return tok;\n        }\n\n        /* ---------- 3 / 5  try keyword ---------- */\n        if let Some((tok, after, n)) = lex_keyword(p, end) {\n            *consumed = n;\n            return tok;\n        }\n\n        /* ---------- 4 / 5  try number ---------- */\n        if let Some((tok, after, n)) = lex_number(token_val, p, end) {\n            *consumed = n;\n            return tok;\n        }\n\n        /* ---------- 5 / 5  try string ---------- */\n        if let Some((tok, after, n)) = lex_string(token_val, p, end) {\n            *consumed = n;\n            return tok;\n        }\n\n        /* ---------- no match ⇒ error / eof ---------- */\n        if p >= end {\n            JTokenType::JTOK_NONE\n        } else {\n            trace!(byte = *p, \"unrecognised input ⇒ JTOK_ERR\");\n            JTokenType::JTOK_ERR\n        }\n    }\n}\n\n\npub fn json_isdigit(ch: i32) -> bool {\n    let ch: u8 = ch.try_into().unwrap();\n\n    let ch = char::from(ch);\n\n    (ch >= '0') && (ch <= '9')\n}\n\n\n#[inline]\npub fn parse_string_escape(backslash: *const u8, end: *const u8) -> Option<(String, *const u8)> {\n    //  backslash points at the '\\', so we need the char _after_ it\n    let mut cur = backslash.add(1);\n    if cur >= end {\n        return None;\n    }\n    let mut out = String::new();\n    match *cur as char {\n        '\"'  => out.push('\"'),\n        '\\\\' => out.push('\\\\'),\n        '/'  => out.push('/'),\n        'b'  => out.push('\\u{0008}'),\n        'f'  => out.push('\\u{000C}'),\n        'n'  => out.push('\\n'),\n        'r'  => out.push('\\r'),\n        't'  => out.push('\\t'),\n        'u'  => {\n            /* ensure at least uXXXX available */\n            if cur.add(5) > end {\n                return None;\n            }\n            let mut cp = 0u32;\n            let next = hatoui(cur.add(1), cur.add(5), &mut cp);\n            if next != cur.add(5) {\n                return None;\n            }\n            /* -------- surrogate pair handling (unchanged) -------- */\n            match cp {\n                0xD800..=0xDBFF => {\n                    if cur.add(11) > end || *cur.add(5) != b'\\\\' || *cur.add(6) != b'u' {\n                        return None;\n                    }\n                    let mut low = 0u32;\n                    let next2 = hatoui(cur.add(7), cur.add(11), &mut low);\n                    if next2 != cur.add(11) || !(0xDC00..=0xDFFF).contains(&low) {\n                        return None;\n                    }\n                    let full = 0x10000 + ((cp - 0xD800) << 10) + (low - 0xDC00);\n                    out.push(char::from_u32(full)?);\n                    cur = next2;\n                }\n                0xDC00..=0xDFFF => return None,\n                _ => {\n                    out.push(char::from_u32(cp)?);\n                    cur = next;\n                }\n            }\n            cur = cur.sub(1);          // compensate for main loop’s +1\n        }\n        _ => return None,\n    }\n    cur = cur.add(1);\n    Some((out, cur))\n}\n\n\n/// Return `true` when *jtt* represents one of the five\n/// terminal JSON value token types (null / true / false / number / string).\n#[inline]\n#[instrument(level = \"trace\", skip_all)]\npub fn json_token_is_value(jtt: JTokenType) -> bool {\n    matches!(\n        jtt,\n        JTokenType::JTOK_KW_NULL\n            | JTokenType::JTOK_KW_TRUE\n            | JTokenType::JTOK_KW_FALSE\n            | JTokenType::JTOK_NUMBER\n            | JTokenType::JTOK_STRING\n    )\n}\n\n\n/// Try to lex a single‑byte structural token.  On success returns\n/// `(token, ptr_after, consumed)`.\n#[inline]\n#[instrument(level = \"trace\", skip_all)]\npub fn lex_structural(p: *const u8, end: *const u8) -> Option<(JTokenType, *const u8, u32)> {\n    if p >= end {\n        return None;\n    }\n    let t = match *p as char {\n        '{' => JTokenType::JTOK_OBJ_OPEN,\n        '}' => JTokenType::JTOK_OBJ_CLOSE,\n        '[' => JTokenType::JTOK_ARR_OPEN,\n        ']' => JTokenType::JTOK_ARR_CLOSE,\n        ':' => JTokenType::JTOK_COLON,\n        ',' => JTokenType::JTOK_COMMA,\n        _   => return None,\n    };\n    let after = p.add(1);\n    Some((t, after, 1))\n}\n\n\n/// Try to lex one of the three JSON keywords (`null / true / false`).\n#[inline]\n#[instrument(level = \"trace\", skip_all)]\npub fn lex_keyword(p: *const u8, end: *const u8) -> Option<(JTokenType, *const u8, u32)> {\n    if p.add(4) <= end {\n        let lit4 = std::slice::from_raw_parts(p, 4);\n        if lit4 == b\"null\" {\n            return Some((\n                JTokenType::JTOK_KW_NULL,\n                p.add(4),\n                4,\n            ));\n        }\n        if lit4 == b\"true\" {\n            return Some((\n                JTokenType::JTOK_KW_TRUE,\n                p.add(4),\n                4,\n            ));\n        }\n    }\n    if p.add(5) <= end && std::slice::from_raw_parts(p, 5) == b\"false\" {\n        return Some((\n            JTokenType::JTOK_KW_FALSE,\n            p.add(5),\n            5,\n        ));\n    }\n    None\n}\n\n\n/// Try to lex a JSON number.  Returns `(token, after, consumed, literal)`.\n#[inline]\n#[instrument(level = \"trace\", skip(token_val))]\npub fn lex_number(token_val: &mut String, p: *const u8, end: *const u8) -> Option<(JTokenType, *const u8, u32)> {\n    use std::ptr::copy_nonoverlapping;\n\n    /* ---------- 1/7  basic pre‑filter ----------------------------------- */\n    if p >= end {\n        return None;\n    }\n    let first = *p as char;\n    if !matches!(first, '-' | '0'..='9') {\n        return None;\n    }\n\n    /* ---------- 2/7  “‑” MUST be followed by a digit -------------------- */\n    if first == '-' {\n        if p.add(1) >= end || !json_isdigit(*p.add(1) as i32) {\n            //  lone “‑”, “‑e…”, “‑.…” ⇒ *not* a valid number\n            return None;\n        }\n    }\n\n    /* ---------- 3/7  fast scan over the integer part ------------------- */\n    let start = p;\n    let mut cur = p.add(1);                       // we already looked at the 1st char\n    while cur < end && json_isdigit(*cur as i32) {\n        cur = cur.add(1);\n    }\n\n    /* ---------- 4/7  reject leading‑zero violations -------------------- */\n    {\n        let len   = (cur as usize - start as usize) as usize;\n        let slice = std::slice::from_raw_parts(start, len);\n        if (slice[0] == b'0' && len > 1 && slice[1].is_ascii_digit())\n            || (slice[0] == b'-'\n                && len > 2\n                && slice[1] == b'0'\n                && slice[2].is_ascii_digit())\n        {\n            return None;\n        }\n    }\n\n    /* ---------- 5/7  optional fraction --------------------------------- */\n    if cur < end && *cur as char == '.' {\n        cur = cur.add(1);\n        if cur >= end || !json_isdigit(*cur as i32) {\n            return None;                          // “1.” is illegal\n        }\n        while cur < end && json_isdigit(*cur as i32) {\n            cur = cur.add(1);\n        }\n    }\n\n    /* ---------- 6/7  optional exponent ---------------------------------- */\n    if cur < end && matches!(*cur as char, 'e' | 'E') {\n        cur = cur.add(1);\n        if cur < end && matches!(*cur as char, '+' | '-') {\n            cur = cur.add(1);\n        }\n        if cur >= end || !json_isdigit(*cur as i32) {\n            return None;                          // “1e”, “1e+” …\n        }\n        while cur < end && json_isdigit(*cur as i32) {\n            cur = cur.add(1);\n        }\n    }\n\n    /* ---------- 7/7  materialise literal into `token_val` --------------- */\n    let len = (cur as usize - start as usize) as usize;\n    token_val.clear();\n    token_val.reserve(len);\n    token_val.as_mut_vec().resize(len, 0);\n    copy_nonoverlapping(start, token_val.as_mut_ptr(), len);\n    token_val.as_mut_vec().set_len(len);\n\n    let consumed = bytes_consumed(start, cur);\n    trace!(value = %token_val, consumed, \"number token\");\n    Some((JTokenType::JTOK_NUMBER, cur, consumed))\n}\n\n\n/// Try to lex a JSON string.  Returns `(token, after, consumed, literal)`.\n#[inline]\n#[instrument(level = \"trace\", skip(token_val))]\npub fn lex_string(token_val: &mut String, p: *const u8, end: *const u8) -> Option<(JTokenType, *const u8, u32)> {\n    if p >= end || *p as char != '\"' {\n        return None;\n    }\n    let start = p;\n    let mut cur = p.add(1);            // after opening quote\n    token_val.clear();\n\n    while cur < end {\n        let ch = *cur;\n        if ch < 0x20 {\n            return None;\n        }\n        if ch == b'\"' {\n            cur = cur.add(1);          // position after closing quote\n            break;\n        }\n        if ch == b'\\\\' {\n            /*  Re‑use existing escape handling by delegating to the\n                original (now private) code – moved into its own helper\n                for reuse.                                                */\n            if let Some((parsed, after)) = parse_string_escape(cur, end) {\n                token_val.push_str(&parsed);\n                cur = after;\n                continue;\n            } else {\n                return None;\n            }\n        }\n        token_val.push(ch as char);\n        cur = cur.add(1);\n    }\n\n    if cur > end {\n        return None;                   // unterminated\n    }\n\n    let consumed = bytes_consumed(start, cur);\n    trace!(value = %token_val, consumed, \"string token\");\n    Some((JTokenType::JTOK_STRING, cur, consumed))\n}\n\n\n#[inline]\n#[instrument(level = \"trace\", skip(p, consumed))]\npub fn single_byte_token(\n    p:        &mut *const u8,\n    start:    *const u8,\n    consumed: &mut u32,\n    kind:     JTokenType\n) -> JTokenType\n{\n    *p = (*p).add(1);                                       // advance cursor\n    *consumed = (*p as usize - start as usize) as u32;\n    trace!(?kind, consumed, \"single_byte_token emitted\");\n    kind\n}\n\n\n/// Skip JSON whitespace (`SP`, `HT`, `LF`, `CR`) **and** trailing NUL\n/// padding.  Returns the first non‑whitespace, non‑NUL position.\n#[inline]\n#[instrument(level = \"trace\", skip_all)]\npub fn skip_ws_nul(mut p: *const u8, end: *const u8) -> *const u8 {\n    while p < end && (json_isspace(*p as i32) || *p == 0) {\n        p = p.add(1);\n    }\n    p\n}\n\n\n/// Helper used by *all* sub‑lexers to compute the byte count that was\n/// consumed for the current token.\n#[inline]\npub fn bytes_consumed(start: *const u8, after: *const u8) -> u32 {\n    (after as usize - start as usize) as u32\n}\n\n\n\nThis crate has the following authors:\n[\n    \"klebs6 <tpk3.mx@gmail.com>\",\n]\nThis crate has the following rust edition:\n2024\nThis crate has the following license:\nMIT\n","role":"user"}],"model":"gpt-4o"}}
