{"custom_id":"bitcoin-tx-confirm-stats-ai-readme-request","method":"POST","url":"/v1/chat/completions","body":{"max_completion_tokens":8192,"messages":[{"content":"We would like you to write a README.md for a rust crate with maximally helpful content.\nAdditionally, we would like you to generate several fields from the Cargo.toml package section for this crate.\nWe will show you the strict json output format we would like you to provide.\nPlease tailor your response for an apex consumer of maximal intelligence.\nA reader of this README.md should have a good sense of what the crate does and how to use it after reading what you write.\nPlease include a messaage indicating that this README.md file was generated by an AI model and may not be 100% accurate however it should be pretty good.\n\n\nCarefully read and parse the information we sent you.\n\nWe want to generate all information items requested by our schema:\n\n- Read the description of each field. Use it, along with the content we sent you to generate an appropriate value for each.\n- Ensure that each generated field optimally serves its desired purpose.\n- Each item should be deep, detailed, and specific. Use optimally descriptive and useful language. Do not be too verbose.\n- Do not add any extra keys to the generated object. Generate precisely what we ask of you: no more, no less.\n- For fields which are of an enum type: pick *exactly one variant*. Do not generate any information for the unselected variants.\n- For array fields: supply a JSON list [ ... ] of items, each conforming to the correct item type.\n- For map fields: Provide a JSON object { \"key_as_string\": <value>, ... }. Keys must be valid JSON strings. Keys and Values must conform to the key and value schemas we provide to you. Fill all subfields of each and every map value.\n- If we ask you to justify a map field, provide your justification/confidence score for the entire map. Do not attempt to separately justify each map element.\n- Likewise, if we ask you to justify an array field, provide your justification/confidence score for the entire array. Do not attempt to separately justify each element.\n- Provide numeric fields as real JSON numbers (not strings).\n- For string fields: provide the value precisely as a string literal. If optional, you may set it to null.\n- If we ask you to justify or provide confidence scores for the fields of a structure, please do so carefully. For each field we ask you to justify, please explain or justify your choice clearly. Confidence scores must be specified as real JSON numbers in the range `[0..1]`.\n- If we ask to you generate an optional field, either fill it or set it to null.\n- If we ask you to justify your choice of an enum variant, please explain your choice clearly. Do not justify your decision to leave a variant unselected. Provide your justification as a JSON string. Provide your confidence score as a real JSON number in the range `[0..1]`.\n- In terms of the JSON format we would like you to provide, the object itself will be deserialied based on the following schema. \n- We begin exactly at the top: the data should not be nested under a parent named 'fields' or anything similar.\n\n## Here is the schema to use:\n\nObject {\n    \"fields\": Object {\n        \"crate_name\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be the name of our crate, *verbatim*, trimmed for use as a toml field\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"full_readme_markdown\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be valid markdown representing the readme for this crate.\\\\n\\\\n Please make it terse, useful, and designed for an industrious commercially savy consumer of industrious nature. \\\\n\\\\n Advanced vocabulary and concepts are welcome. \\\\n\\\\n If this crate involves concepts from mathematics and/or physics, please describe and document them here.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_categories\": Object {\n            \"generation_instructions\": String(\"\\\"These categories will be used in our Cargo.toml file.\\\\n\\\\n It is important that users of this crate can find it on crates.io.\\\\n\\\\n Choose maximum 5 categories and make sure they are actually meaningful for our crate.\\\\n\\\\n These categories should only be chosen from among the legal crates.io categories. \\\\n\\\\n The ONLY legal categoreis are:\\\\n\\\\n``\\\\n accessibility\\\\n aerospace\\\\n algorithms\\\\n api-bindings\\\\n asynchronous\\\\n authentication\\\\n caching\\\\n command-line-interface\\\\n command-line-utilities\\\\n compilers\\\\n compression\\\\n computer-vision\\\\n concurrency\\\\n config\\\\n cryptography\\\\n data-structures\\\\n database\\\\n database-implementations\\\\n date-and-time\\\\n development-tools\\\\n email\\\\n embedded\\\\n emulators\\\\n encoding\\\\n external-ffi-bindings\\\\n filesystem\\\\n finance\\\\n game-development\\\\n game-engines\\\\n games\\\\n graphics\\\\n gui\\\\n hardware-support\\\\n internationalization\\\\n localization\\\\n mathematics\\\\n memory-management\\\\n multimedia\\\\n network-programming\\\\n no-std\\\\n os\\\\n parser-implementations\\\\n parsing\\\\n rendering\\\\n rust-patterns\\\\n science\\\\n simulation\\\\n template-engine\\\\n text-editors\\\\n text-processing\\\\n value-formatting\\\\n virtualization\\\\n visualization\\\\n wasm\\\\n web-programming\\\\n ``\\\\n\\\\n DO NOT specify a category that is not on this list.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n        \"package_description\": Object {\n            \"generation_instructions\": String(\"\\\"We will place this package_description in our Cargo.toml file so the users of this crate\\\\n know what it does.\\\\n\\\\n The discription should be technical, clear, and useful. It should be terse.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_keywords\": Object {\n            \"generation_instructions\": String(\"\\\"These keywords will be used in our Cargo.toml file so the users of this crate can easily\\\\n find it in the package system. Keywords should be useful, varied, meaningful, and reflect\\\\n the contents of our crate. They should make it easy to find. Please provide exactly five.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n    },\n    \"struct_docs\": String(\" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\"),\n    \"struct_name\": String(\"AiReadmeWriterDesiredOutput\"),\n    \"type\": String(\"struct\"),\n}\n\n## Please keep the following in mind:\n\nEntry phrasing:\n\n- Ensure entries are concise and focused.\n- Ensure there are no vague phrases (e.g., avoid starting with \"Illustrates the something something...\"; instead, use direct details like \"The hanging green vines on the garden wall\").\n\nEnsure Clarity and Consistency:\n\n- Use clear, grammatically correct sentences.\n- Maintain a consistent tone and style throughout.\n\nEnrich Content:\n\n- Communicate additional information where appropriate to enhance depth and value.\n- If a mathematical, physical, or otherwise technical background is helpful, please provide it.\n- Ensure that the content is comprehensive and meets all requirements.\n\nFocus Language:\n\n- Use deliberate and precise language.\n- Avoid vague verbs and keep the maximally intelligent and detail oriented audience in mind. \n- We explicitly ban the words `hush`, `subtle`, `mythic`, and `aura`. Please keep your vocabulary choices rigorous.\n- Do not use modern cultural references \n- Do not generically reference ideas which do not belong in the global environment.\n\n## Present the Final JSON:\n\n- Format the output as a JSON object.\n- Include all fields properly named from the schema we sent you.\n\nEnsure Proper Formatting:\n\n- Use proper JSON syntax with keys and arrays.\n- Ensure that all entries are correctly placed in their respective places.\n\nProvide only the final JSON data as specified by the schema. \n\nDo not include any schema generation instructions, meta-fields, nested-templates (unless directly asked) or extraneous information. \n\nProvide only the populated fields and their values, conforming exactly to the required structure without embedding any intermediate annotations or placeholders.\n\nYour output should only consist of the JSON object. do *not* include a preamble or postamble to your response. \n\nWe seek to successfully parse your response directly as JSON.\n\nFor the final answer, output the result only as a valid JSON object, wrapped inside triple backticks with json immediately after the opening backticks.\nThe JSON must be parser-ready:\n– No additional text before or after the block\n– No markdown or prose outside the fenced block\n– No HTML escaping of quotes, slashes, or other characters\n– Indentation and whitespace preserved exactly as written\n– The content between the fences must be valid JSON as per RFC 8259","role":"system"},{"content":"We have a rust crate named 'bitcoin-tx-confirm-stats' 0.1.1. \n\nHere is the interface:\n/**\n| Used to return detailed information\n| about a feerate bucket\n|\n*/\n#[derive(Copy,Clone,Debug,Getters,Setters,MutGetters)]\n#[getset(get=\"pub\",get_mut=\"pub\",set=\"pub\")]\npub struct FeeRateEstimatorBucket  {\n    start:           f64,\n    end:             f64,\n    within_target:   f64,\n    total_confirmed: f64,\n    in_mempool:      f64,\n    left_mempool:    f64,\n}\n\n\n#[derive(Debug,Getters,Setters,MutGetters)]\n#[getset(get=\"pub\",get_mut=\"pub\",set=\"pub\")]\npub(crate) struct FeeRateEstimatorState  {\n    n_conf:           f64,\n    total_num:        f64,\n    fail_num:         f64,\n    extra_num:        i32,\n    cur_near_bucket:  usize,\n    cur_far_bucket:   usize,\n    best_near_bucket: usize,\n    best_far_bucket:  usize,\n    new_bucket_range: bool,\n    passing:          bool,\n    found_answer:     bool,\n    pass_bucket:      FeeRateEstimatorBucket,\n    fail_bucket:      FeeRateEstimatorBucket,\n}\n\n\n/// We will instantiate an instance of this class to track transactions that\n/// were included in a block. \n///\n/// We will lump transactions into a bucket according to their approximate\n/// feerate and then track how long it took for those txs to be included in\n/// a block\n/// \n/// The tracking of unconfirmed (mempool) transactions is completely independent\n/// of the historical tracking of transactions that have been confirmed in\n/// a block.\n#[derive(Debug,Getters,Setters,MutGetters)]\n#[getset(get=\"pub\",get_mut=\"pub\",set=\"pub\")]\npub struct TxConfirmStats  {\n    buckets:        Arc<Vec<f64>>,\n    bucket_map:     Arc<HashMap<f64,u32>>,\n    tx_ct_avg:      Vec<f64>,\n    conf_avg:       Vec<Vec<f64>>,\n    fail_avg:       Vec<Vec<f64>>,\n    feerate_avg:    Vec<f64>,\n    decay:          f64,\n    scale:          u32,\n    unconf_txs:     Vec<Vec<i32>>,\n    old_unconf_txs: Vec<i32>,\n}\n\n\n/**\n| Used to return detailed information\n| about a fee estimate calculation\n|\n*/\n#[derive(Getters,Setters,MutGetters,Default)]\n#[getset(get=\"pub\",get_mut=\"pub\",set=\"pub\")]\npub struct FeeRateEstimationResult  {\n    pass:  FeeRateEstimatorBucket,\n    fail:  FeeRateEstimatorBucket,\n    decay: f64,\n    scale: u32,\n}\n\npub fn has_sufficient_samples(total_num: f64, sufficient_tx_val: f64, decay: f64) -> bool {\n    total_num >= (sufficient_tx_val / (1.0 - decay))\n}\n\n\npub fn compute_success_ratio(\n    n_conf:    f64,\n    total_num: f64,\n    fail_num:  f64,\n    extra_num: i32\n) -> f64\n{\n    n_conf / (total_num + fail_num + (extra_num as f64))\n}\n\nimpl TxConfirmStats {\n\n    pub fn find_median_feerate(\n        &self,\n        min_bucket: usize,\n        max_bucket: usize,\n        mut tx_sum: f64\n    ) -> f64\n    {\n        // No transactions in this range => no median\n        if tx_sum == 0.0 {\n            return 0.0;\n        }\n    \n        tx_sum /= 2.0;\n        for j in min_bucket..=max_bucket {\n            let tx_ct = self.tx_ct_avg()[j];\n            if tx_ct < tx_sum {\n                // Consume this bucket and keep searching\n                tx_sum -= tx_ct;\n            } else {\n                // We're in the median-containing bucket; avoid div-by-zero just in case\n                if tx_ct == 0.0 {\n                    continue;\n                }\n                return self.feerate_avg()[j] / tx_ct;\n            }\n        }\n        0.0\n    }\n}\n\nimpl TxConfirmStats {\n\n    /// Read saved state; panics on corruption (Core throws).\n    ///\n    /// Read data file and do some very basic sanity checking buckets and bucketMap are not updated\n    /// yet, so don't access them\n    /// If there is a read failure, we'll just discard this entire object anyway\n    /// The current version will store the decay with each individual TxConfirmStats and also keep\n    /// a scale factor\n    pub fn read<R: Read>(\n        &mut self,\n        filein:          &mut R,\n        _n_file_version: i32,\n        num_buckets:     usize\n    )\n    {\n        // decay (EncodedDoubleFormatter, LE)\n        let mut df = EncodedDoubleFormatter::default();\n        let mut decay = 0.0;\n        df.unser(filein, &mut decay);\n        if !(decay > 0.0 && decay < 1.0) {\n            panic!(\"Corrupt estimates file. Decay must be between 0 and 1 (non-inclusive)\");\n        }\n    \n        // scale (u32, LE)\n        let scale = read_u32_le(filein).expect(\"read(scale): io error\");\n        if scale == 0 {\n            panic!(\"Corrupt estimates file. Scale must be non-zero\");\n        }\n    \n        // vectors of f64 via VectorFormatter<EncodedDoubleFormatter>\n        let mut vf = VectorFormatter::<EncodedDoubleFormatter>::default();\n    \n        let mut feerate_avg = Vec::<f64>::new();\n        vf.unser(filein, &mut feerate_avg);\n        if feerate_avg.len() != num_buckets {\n            panic!(\"Corrupt estimates file. Mismatch in feerate average bucket count\");\n        }\n    \n        let mut tx_ct_avg = Vec::<f64>::new();\n        vf.unser(filein, &mut tx_ct_avg);\n        if tx_ct_avg.len() != num_buckets {\n            panic!(\"Corrupt estimates file. Mismatch in tx count bucket count\");\n        }\n    \n        // matrices of f64 via VectorFormatter<VectorFormatter<EncodedDoubleFormatter>>\n        let mut mf = VectorFormatter::<VectorFormatter<EncodedDoubleFormatter>>::default();\n    \n        let mut conf_avg: Vec<Vec<f64>> = Vec::new();\n        mf.unser(filein, &mut conf_avg);\n        let max_periods  = conf_avg.len();\n        let max_confirms = (scale as usize).saturating_mul(max_periods);\n    \n        if max_confirms == 0 || max_confirms > 6 * 24 * 7 {\n            panic!(\"Corrupt estimates file.  Must maintain estimates for between 1 and 1008 (one week) confirms\");\n        }\n        for (i, row) in conf_avg.iter().enumerate() {\n            if row.len() != num_buckets {\n                panic!(\"Corrupt estimates file. Mismatch in feerate conf average bucket count (row {i})\");\n            }\n        }\n    \n        let mut fail_avg: Vec<Vec<f64>> = Vec::new();\n        mf.unser(filein, &mut fail_avg);\n        if fail_avg.len() != max_periods {\n            panic!(\"Corrupt estimates file. Mismatch in confirms tracked for failures\");\n        }\n        for (i, row) in fail_avg.iter().enumerate() {\n            if row.len() != num_buckets {\n                panic!(\"Corrupt estimates file. Mismatch in one of failure average bucket counts (row {i})\");\n            }\n        }\n    \n        // Commit to self only after successful read/validation\n        self.set_decay(decay);\n        self.set_scale(scale);\n        self.set_feerate_avg(feerate_avg);\n        self.set_tx_ct_avg(tx_ct_avg);\n        self.set_conf_avg(conf_avg);\n        self.set_fail_avg(fail_avg);\n    \n        // Resize in-memory rolling structures to match the file\n        self.resize_in_memory_counters(num_buckets);\n    \n        trace!(\n            target: \"estimatefee\",\n            \"Reading estimates: {} buckets counting confirms up to {} blocks\",\n            num_buckets, max_confirms\n        );\n    }\n\n    /// Write saved state; panics on IO error (Core throws).\n    pub fn write<W: Write>(&self, fileout: &mut W) {\n        // decay (EncodedDoubleFormatter, LE)\n        let mut df = EncodedDoubleFormatter::default();\n        df.ser(fileout, &self.decay());\n    \n        // scale (u32, LE)\n        write_u32_le(fileout, *self.scale()).expect(\"write(scale): io error\");\n    \n        // vectors of f64\n        let mut vf = VectorFormatter::<EncodedDoubleFormatter>::default();\n        vf.ser(fileout, &self.feerate_avg());\n        vf.ser(fileout, &self.tx_ct_avg());\n    \n        // matrices of f64\n        let mut mf = VectorFormatter::<VectorFormatter<EncodedDoubleFormatter>>::default();\n        mf.ser(fileout, &self.conf_avg());\n        mf.ser(fileout, &self.fail_avg());\n    }\n}\n\nimpl TxConfirmStats {\n\n    /// Calculate a feerate estimate. \n    ///\n    /// Find the lowest value bucket (or range of buckets to make sure we have\n    /// enough data points) whose transactions still have sufficient likelihood\n    /// of being confirmed within the target number of confirmations\n    /// \n    /// -----------\n    /// @param confTarget\n    /// target number of confirmations\n    /// ----------\n    /// @param sufficientTxVal\n    /// required average number of transactions per block in a bucket range\n    /// @param minSuccess\n    /// the success probability we require\n    /// @param nBlockHeight\n    /// the current block height\n    /// returns -1 on error conditions\n    pub fn estimate_median_val(\n        &self,\n        conf_target:         i32,\n        sufficient_tx_val:   f64,\n        success_break_point: f64,\n        n_block_height:      u32,\n        result:              *mut FeeRateEstimationResult\n    ) -> f64\n    {\n        // Map conf_target to the period index\n        let period_target = ((conf_target + *self.scale() as i32 - 1) / *self.scale() as i32) as usize;\n    \n        if period_target == 0 || period_target > self.conf_avg().len() {\n            return -1.0; // out of range — Core returns -1 on error\n        }\n    \n        // Run the core scan\n        let mut st = self.scan_buckets(\n            conf_target,\n            sufficient_tx_val,\n            success_break_point,\n            n_block_height,\n            period_target,\n        );\n    \n        // Compute the “median” feerate as in Core (average in the bucket\n        // containing the median tx)\n        //\n        let mut median = -1.0;\n    \n        // Snapshot best range as VALUES to end immutable borrows before we take &mut to st\n        let (min_bucket, max_bucket) = {\n            let near = *st.best_near_bucket();\n            let far  = *st.best_far_bucket();\n            (near.min(far), near.max(far))\n        };\n    \n        let tx_sum = self.sum_tx_ct_avg(min_bucket, max_bucket);\n    \n        if *st.found_answer() && tx_sum != 0.0 {\n            median = self.find_median_feerate(min_bucket, max_bucket, tx_sum);\n            st.set_pass_range(&self.buckets()[..], min_bucket, max_bucket);\n        }\n    \n        // If we were passing until we ran out of data, record the trailing failing range\n        st.finalize_trailing_failure(&self.buckets()[..]);\n    \n        // (Optional) logging — identical content/ordering to Core\n        let passed_within_target_perc = st.pass_bucket().calc_within_target_percentage();\n        let failed_within_target_perc = st.fail_bucket().calc_within_target_percentage();\n    \n        trace!(\n            target: \"estimatefee\",\n            \"FeeEst: {} > {:.0}% decay {:.5}: feerate: {:.6} from ({:.6} - {:.6}) {:.2}% \\\n            {:.1}/({:.1} {:.1} mem {:.1} out) \\\n            Fail: ({:.6} - {:.6}) {:.2}% {:.1}/({:.1} {:.1} mem {:.1} out)\",\n            conf_target,\n            100.0 * success_break_point,\n            self.decay(),\n            median,\n            st.pass_bucket().start(),\n            st.pass_bucket().end(),\n            passed_within_target_perc,\n            st.pass_bucket().within_target(),\n            st.pass_bucket().total_confirmed(),\n            st.pass_bucket().in_mempool(),\n            st.pass_bucket().left_mempool(),\n            st.fail_bucket().start(),\n            st.fail_bucket().end(),\n            failed_within_target_perc,\n            st.fail_bucket().within_target(),\n            st.fail_bucket().total_confirmed(),\n            st.fail_bucket().in_mempool(),\n            st.fail_bucket().left_mempool(),\n            );\n    \n        // Export results if requested (bit-matched to C++)\n        if !result.is_null() {\n            unsafe {\n                (*result).set_pass(*st.pass_bucket());\n                (*result).set_fail(*st.fail_bucket());\n                (*result).set_decay(*self.decay());\n                (*result).set_scale(*self.scale());\n            }\n        }\n    \n        median\n    }\n}\n\nimpl TxConfirmStats {\n\n    /// Compute the number of transactions still unconfirmed in this bucket\n    /// for `conf_target` or longer, including old unconfirmed.\n    ///\n    /// Bit-for-bit equivalent to Core's extraNum loop.\n    pub fn calc_extra_unconfirmed(\n        &self,\n        bucket_index:   usize,\n        conf_target:    usize,\n        n_block_height: u32,\n        bins:           usize\n    ) -> i32\n    {\n        let mut extra = 0;\n    \n        // extraNum: txs still in mempool for confTarget or longer\n        for confct in conf_target..self.get_max_confirms() as usize {\n    \n            // emulate (nBlockHeight - confct) % bins with unsigned wraparound\n            let idx = ((n_block_height as isize - confct as isize)\n                .rem_euclid(bins as isize)) as usize;\n    \n            extra += self.unconf_txs()[idx][bucket_index];\n        }\n    \n        extra + self.old_unconf_txs()[bucket_index]\n    }\n}\n\nimpl Default for FeeRateEstimatorBucket {\n\n    fn default() -> Self {\n        Self {\n            start:          -1.0,\n            end:            -1.0,\n            within_target:   0.0,\n            total_confirmed: 0.0,\n            in_mempool:      0.0,\n            left_mempool:    0.0,\n        }\n    }\n}\n\nimpl FeeRateEstimatorBucket {\n\n    pub fn record_failure_bucket(\n        &mut self,\n        cur_near_bucket: usize,\n        cur_far_bucket:  usize,\n        buckets:         &[f64],\n        n_conf:          f64,\n        total_num:       f64,\n        fail_num:        f64,\n        extra_num:       i32\n    )\n    {\n        // First failure — record the failing bucket range\n        let fail_min = cur_near_bucket.min(cur_far_bucket);\n        let fail_max = cur_near_bucket.max(cur_far_bucket);\n        self.start           = if fail_min > 0 { buckets[fail_min - 1] } else { 0.0 };\n        self.end             = buckets[fail_max];\n        self.within_target   = n_conf;\n        self.total_confirmed = total_num;\n        self.in_mempool      = extra_num as f64;\n        self.left_mempool    = fail_num;\n    }\n\n    pub fn record_passing_bucket(\n        &mut self,\n        n_conf:    f64,\n        total_num: f64,\n        fail_num:  f64,\n        extra_num: i32\n    )\n    {\n        self.within_target   = n_conf;\n        self.total_confirmed = total_num;\n        self.in_mempool      = extra_num as f64;\n        self.left_mempool    = fail_num;\n    }\n\n    pub fn calc_within_target_percentage(&self) -> f64 {\n        let denom = self.total_confirmed + self.in_mempool + self.left_mempool;\n        if denom != 0.0 {\n            100.0 * self.within_target / denom\n        } else {\n            0.0\n        }\n    }\n}\n\nimpl TxConfirmStats {\n\n    pub fn set_pass_bucket_range(\n        &self,\n        pass_bucket: &mut FeeRateEstimatorBucket,\n        min_bucket:  usize,\n        max_bucket:  usize\n    )\n    {\n        pass_bucket.set_start(if min_bucket > 0 {\n            self.buckets()[min_bucket - 1]\n        } else {\n            0.0\n        });\n    \n        pass_bucket.set_end(self.buckets()[max_bucket]);\n    }\n}\n\nimpl TxConfirmStats {\n\n    /// Compute the sum of tx count averages between the given bucket range (inclusive).\n    ///\n    /// # Arguments\n    /// * `min_bucket` - inclusive lower index\n    /// * `max_bucket` - inclusive upper index\n    /// # Returns\n    /// The summed tx count average across the specified bucket range.\n    pub fn sum_tx_ct_avg(&self, min_bucket: usize, max_bucket: usize) -> f64 {\n        self.tx_ct_avg()[min_bucket..=max_bucket].iter().sum()\n    }\n\n    /// Accumulate confirmation and failure statistics for a given bucket index.\n    pub fn accumulate_bucket_data(\n        &self,\n        period_target: usize,\n        bucket_index:  usize,\n        n_conf:        &mut f64,\n        total_num:     &mut f64,\n        fail_num:      &mut f64\n    )\n    {\n        *n_conf    += self.conf_avg()[period_target - 1][bucket_index];\n        *total_num += self.tx_ct_avg()[bucket_index];\n        *fail_num  += self.fail_avg()[period_target - 1][bucket_index];\n    }\n}\n\nimpl FeeRateEstimatorState {\n\n    pub fn reset_counters(&mut self) {\n        self.n_conf    = 0.0;\n        self.total_num = 0.0;\n        self.fail_num  = 0.0;\n        self.extra_num = 0;\n    }\n\n    pub fn new(max_bucket_index: usize) -> Self {\n        Self {\n            n_conf: 0.0,\n            total_num: 0.0,\n            extra_num: 0,\n            fail_num: 0.0,\n    \n            cur_near_bucket:  max_bucket_index,\n            cur_far_bucket:   max_bucket_index,\n            best_near_bucket: max_bucket_index,\n            best_far_bucket:  max_bucket_index,\n    \n            new_bucket_range: true,\n            passing: true,\n            found_answer: false,\n    \n            pass_bucket: FeeRateEstimatorBucket::default(),\n            fail_bucket: FeeRateEstimatorBucket::default(),\n        }\n    }\n\n    #[inline]\n    pub fn begin_or_extend_range(&mut self, b: usize) {\n        if self.new_bucket_range {\n            self.cur_near_bucket = b;\n            self.new_bucket_range = false;\n        }\n        self.cur_far_bucket = b;\n    }\n\n    #[inline]\n    pub fn is_passing(&self) -> bool {\n        self.passing \n    }\n\n    #[inline]\n    pub fn best_range_minmax(&self) -> (usize, usize) {\n        let near = self.best_near_bucket;\n        let far  = self.best_far_bucket;\n        (near.min(far), near.max(far))\n    }\n\n    /// Accumulate tx/conf/fail deltas from stats for bucket `b`.\n    #[inline]\n    pub fn accumulate_from_stats(\n        &mut self,\n        stats:         &TxConfirmStats,\n        period_target: usize,\n        b:             usize\n    )\n    {\n        self.n_conf    += stats.conf_avg()[period_target - 1][b];\n        self.total_num += stats.tx_ct_avg()[b];\n        self.fail_num  += stats.fail_avg()[period_target - 1][b];\n    }\n\n    /// Add extra unconfirmed from stats context into this state.\n    #[inline]\n    pub fn add_extra_from_stats(\n        &mut self,\n        stats:          &TxConfirmStats,\n        b:              usize,\n        conf_target:    usize,\n        n_block_height: u32,\n        bins:           usize\n    )\n    {\n        self.extra_num += stats.calc_extra_unconfirmed(b, conf_target, n_block_height, bins);\n    }\n\n    /// Delegate sufficient sample check using this state's totals.\n    #[inline]\n    pub fn has_sufficient(&self, sufficient_tx_val: f64, decay: f64) -> bool {\n        has_sufficient_samples(self.total_num, sufficient_tx_val, decay)\n    }\n\n    /// Compute the current success ratio using the state's counters.\n    #[inline]\n    pub fn cur_pct(&self) -> f64 {\n        compute_success_ratio(self.n_conf, self.total_num, self.fail_num, self.extra_num)\n    }\n\n    /// First failure in a passing streak: snapshot & record fail bucket, mark not passing.\n    pub fn on_first_failure(&mut self, buckets: &[f64]) {\n        let near = self.cur_near_bucket;\n        let far  = self.cur_far_bucket;\n        let fail_min = near.min(far);\n        let fail_max = near.max(far);\n    \n        self.fail_bucket.set_start(if fail_min > 0 { buckets[fail_min - 1] } else { 0.0 });\n        self.fail_bucket.set_end(buckets[fail_max]);\n        self.fail_bucket.set_within_target(self.n_conf);\n        self.fail_bucket.set_total_confirmed(self.total_num);\n        self.fail_bucket.set_in_mempool(self.extra_num as f64);\n        self.fail_bucket.set_left_mempool(self.fail_num);\n    \n        self.set_passing(false);\n    }\n\n    /// Passing: reset fail bucket, mark answer found, record pass bucket, reset counters,\n    /// memorize the best range, and mark that a new range should begin.\n    pub fn on_passing_reset_and_remember(&mut self) {\n        self.fail_bucket  = FeeRateEstimatorBucket::default();\n        self.found_answer = true;\n        self.passing      = true;\n    \n        self.pass_bucket.record_passing_bucket(\n            self.n_conf, self.total_num, self.fail_num, self.extra_num\n        );\n    \n        self.reset_counters();\n    \n        self.best_near_bucket = self.cur_near_bucket;\n        self.best_far_bucket  = self.cur_far_bucket;\n        self.new_bucket_range = true;\n    }\n\n    /// Set the pass bucket [start, end] from bucket indices.\n    #[inline]\n    pub fn set_pass_range(\n        &mut self,\n        buckets:    &[f64],\n        min_bucket: usize,\n        max_bucket: usize\n    )\n    {\n        let start = if min_bucket > 0 { buckets[min_bucket - 1] } else { 0.0 };\n        let end   = buckets[max_bucket];\n    \n        self.pass_bucket.set_start(start);\n        self.pass_bucket.set_end(end);\n    }\n\n    /// If we were passing until we ran out of data (and we had a non-empty current range),\n    /// record the trailing failing range.\n    pub fn finalize_trailing_failure(&mut self, buckets: &[f64]) {\n        if self.passing && !self.new_bucket_range {\n            let near = self.cur_near_bucket;\n            let far  = self.cur_far_bucket;\n            let fail_min = near.min(far);\n            let fail_max = near.max(far);\n    \n            self.fail_bucket.set_start(if fail_min > 0 { buckets[fail_min - 1] } else { 0.0 });\n            self.fail_bucket.set_end(buckets[fail_max]);\n            self.fail_bucket.set_within_target(self.n_conf);\n            self.fail_bucket.set_total_confirmed(self.total_num);\n            self.fail_bucket.set_in_mempool(self.extra_num as f64);\n            self.fail_bucket.set_left_mempool(self.fail_num);\n        }\n    }\n}\n\nimpl TxConfirmStats {\n\n    /// Create new TxConfirmStats. \n    ///\n    /// This is called by BlockPolicyEstimator's constructor with default\n    /// values.\n    /// \n    /// -----------\n    /// @param defaultBuckets\n    /// contains the upper limits for the bucket boundaries\n    /// ----------\n    /// @param maxPeriods\n    /// max number of periods to track\n    /// @param decay\n    /// how much to decay the historical moving average per block\n    pub fn new(\n        default_buckets:    &Vec<f64>,\n        default_bucket_map: &HashMap<f64, u32>,\n        max_periods:        u32,\n        decay:              f64,\n        scale:              u32\n    ) -> Self\n    {\n        assert!(scale != 0, \"_scale must be non-zero\");\n    \n        let buckets = Arc::new(default_buckets.clone());\n        let bucket_map = Arc::new(default_bucket_map.clone());\n    \n        let nb = buckets.len();\n        let mp = max_periods as usize;\n    \n        let mut conf_avg    = vec![vec![0.0f64; nb]; mp];\n        let mut fail_avg    = vec![vec![0.0f64; nb]; mp];\n        let mut tx_ct_avg   = vec![0.0f64; nb];\n        let mut feerate_avg = vec![0.0f64; nb];\n    \n        let mut s = Self {\n            buckets,\n            bucket_map,\n            tx_ct_avg,\n            conf_avg,\n            fail_avg,\n            feerate_avg,\n            decay,\n            scale,\n            unconf_txs: vec![],\n            old_unconf_txs: vec![],\n        };\n    \n        s.resize_in_memory_counters(nb);\n        s\n    }\n\n    /// Return the max number of confirms we're tracking\n    /// \n    #[inline]\n    pub fn get_max_confirms(&self) -> u32 {\n        // return scale * confAvg.size();\n        self.scale * (self.conf_avg.len() as u32)\n    }\n\n    pub fn resize_in_memory_counters(&mut self, newbuckets: usize) {\n        // newbuckets must be passed in because the buckets referred to during Read have not been updated yet.\n        // unconfTxs.resize(GetMaxConfirms()); each row has `newbuckets`\n        let maxc            = self.get_max_confirms() as usize;\n        self.unconf_txs     = vec![vec![0i32; newbuckets]; maxc];\n        self.old_unconf_txs = vec![0i32; newbuckets];\n    }\n\n    /**\n    | Roll the circular buffer for unconfirmed\n    | txs\n    |\n    */\n    pub fn clear_current(&mut self, n_block_height: u32) {\n        let bins = self.unconf_txs.len();\n        let idx  = (n_block_height as usize) % bins;\n    \n        for j in 0..self.buckets.len() {\n            self.old_unconf_txs[j] += self.unconf_txs[idx][j];\n            self.unconf_txs[idx][j] = 0;\n        }\n    }\n\n    /// Record a new transaction data point in the current block stats\n    /// \n    /// -----------\n    /// @param blocksToConfirm\n    /// the number of blocks it took this transaction to confirm\n    ///\n    /// ----------\n    /// @param val\n    /// the feerate of the transaction\n    /// @warning\n    /// blocksToConfirm is 1-based and has to be >= 1\n    pub fn record(&mut self, blocks_to_confirm: i32, feerate: f64) {\n        // blocksToConfirm is 1-based\n        if blocks_to_confirm < 1 {\n            return;\n        }\n        let scale = self.scale as i32;\n        let periods_to_confirm = (blocks_to_confirm + scale - 1) / scale;\n    \n        // bucketindex = bucketMap.lower_bound(feerate)->second;\n        let bucketindex = self.bucket_index_for(feerate);\n    \n        for i in (periods_to_confirm as usize)..=self.conf_avg.len() {\n            self.conf_avg[i - 1][bucketindex] += 1.0;\n        }\n        self.tx_ct_avg[bucketindex] += 1.0;\n        self.feerate_avg[bucketindex] += feerate;\n    }\n\n    /// Update our estimates by decaying our historical moving average and\n    /// updating with the data gathered from the current block\n    /// \n    pub fn update_moving_averages(&mut self) {\n        assert_eq!(self.conf_avg.len(), self.fail_avg.len());\n        let decay = self.decay;\n        for j in 0..self.buckets.len() {\n            for i in 0..self.conf_avg.len() {\n                self.conf_avg[i][j] *= decay;\n                self.fail_avg[i][j] *= decay;\n            }\n            self.feerate_avg[j] *= decay;\n            self.tx_ct_avg[j] *= decay;\n        }\n    }\n\n    /**\n    | Record a new transaction entering the\n    | mempool\n    |\n    */\n    pub fn new_tx(&mut self, n_block_height: u32, val: f64) -> u32 {\n        // bucketindex = bucketMap.lower_bound(val)->second;  (via buckets)\n        let bucketindex = self.bucket_index_for(val) as u32;\n        let idx = (n_block_height as usize) % self.unconf_txs.len();\n        self.unconf_txs[idx][bucketindex as usize] += 1;\n        bucketindex\n    }\n}\n\nimpl TxConfirmStats {\n\n    /**\n    | Remove a transaction from mempool tracking\n    | stats\n    |\n    */\n    pub fn remove_tx(\n        &mut self,\n        entry_height:       u32,\n        n_best_seen_height: u32,\n        bucketindex:        u32,\n        in_block:           bool\n    )\n    {\n        let b = bucketindex as usize;\n    \n        // blocksAgo may be 0 if no blocks seen yet\n        let mut blocks_ago = n_best_seen_height as i32 - entry_height as i32;\n        if n_best_seen_height == 0 {\n            blocks_ago = 0;\n        }\n        if blocks_ago < 0 {\n            trace!(\"Blockpolicy: blocks_ago negative; ignoring remove_tx\");\n            return;\n        }\n    \n        let bins = self.unconf_txs().len();\n        if (blocks_ago as usize) >= bins {\n            if self.old_unconf_txs()[b] > 0 {\n                self.old_unconf_txs_mut()[b] -= 1;\n            } else {\n                trace!(\"Blockpolicy: mempool tx removed from > bins already (bucket={})\", b);\n            }\n        } else {\n            let block_idx = (entry_height as usize) % bins;\n            if self.unconf_txs()[block_idx][b] > 0 {\n                self.unconf_txs_mut()[block_idx][b] -= 1;\n            } else {\n                trace!(\"Blockpolicy: mempool tx double-removed (block_idx={}, bucket={})\", block_idx, b);\n            }\n        }\n    \n        if !in_block && (blocks_ago as u32) >= *self.scale() {\n            let periods_ago = (blocks_ago as u32) / self.scale();\n            for i in 0..periods_ago.min(self.fail_avg().len() as u32) as usize {\n                self.fail_avg_mut()[i][b] += 1.0;\n            }\n        }\n    }\n}\n\nimpl TxConfirmStats {\n\n    pub fn scan_buckets(\n        &self,\n        conf_target:         i32,\n        sufficient_tx_val:   f64,\n        success_break_point: f64,\n        n_block_height:      u32,\n        period_target:       usize\n    ) -> FeeRateEstimatorState\n    {\n        let bins = self.unconf_txs().len();\n        let max_bucket_index = self.buckets().len() - 1;\n    \n        let mut st = FeeRateEstimatorState::new(max_bucket_index);\n    \n        // Start counting from highest feerate transactions\n        for b in (0..=max_bucket_index).rev() {\n            st.begin_or_extend_range(b);\n    \n            // Accumulate tx/conf/fail moving averages (no overlapping &mut borrows)\n            st.accumulate_from_stats(self, period_target, b);\n    \n            // Add mempool 'extra' counts for confTarget..GetMaxConfirms\n            st.add_extra_from_stats(self, b, conf_target as usize, n_block_height, bins);\n    \n            // Evaluate only once we have sufficient samples\n            if st.has_sufficient(sufficient_tx_val, *self.decay()) {\n                let cur_pct = st.cur_pct();\n    \n                if cur_pct < success_break_point {\n                    if st.is_passing() {\n                        st.on_first_failure(&self.buckets()[..]);\n                    }\n                    continue;\n                } else {\n                    st.on_passing_reset_and_remember();\n                }\n            }\n        }\n    \n        st\n    }\n}\n\n\nThis crate has the following authors:\n[\n    \"YourName <you@example.com>\",\n]\nThis crate has the following rust edition:\n2024\nThis crate has the following license:\nMIT\n","role":"user"}],"model":"gpt-5.1"}}
