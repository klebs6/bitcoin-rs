{"custom_id":"bitcoin-cuckoo-cache-ai-readme-request","method":"POST","url":"/v1/chat/completions","body":{"max_completion_tokens":8192,"messages":[{"content":"We would like you to write a README.md for a rust crate with maximally helpful content.\nAdditionally, we would like you to generate several fields from the Cargo.toml package section for this crate.\nWe will show you the strict json output format we would like you to provide.\nPlease tailor your response for an apex consumer of maximal intelligence.\nA reader of this README.md should have a good sense of what the crate does and how to use it after reading what you write.\nPlease include a messaage indicating that this README.md file was generated by an AI model and may not be 100% accurate however it should be pretty good.\n\n\nCarefully read and parse the information we sent you.\n\nWe want to generate all information items requested by our schema:\n\n- Read the description of each field. Use it, along with the content we sent you to generate an appropriate value for each.\n- Ensure that each generated field optimally serves its desired purpose.\n- Each item should be deep, detailed, and specific. Use optimally descriptive and useful language. Do not be too verbose.\n- Do not add any extra keys to the generated object. Generate precisely what we ask of you: no more, no less.\n- For fields which are of an enum type: pick *exactly one variant*. Do not generate any information for the unselected variants.\n- For array fields: supply a JSON list [ ... ] of items, each conforming to the correct item type.\n- For map fields: Provide a JSON object { \"key_as_string\": <value>, ... }. Keys must be valid JSON strings. Keys and Values must conform to the key and value schemas we provide to you. Fill all subfields of each and every map value.\n- If we ask you to justify a map field, provide your justification/confidence score for the entire map. Do not attempt to separately justify each map element.\n- Likewise, if we ask you to justify an array field, provide your justification/confidence score for the entire array. Do not attempt to separately justify each element.\n- Provide numeric fields as real JSON numbers (not strings).\n- For string fields: provide the value precisely as a string literal. If optional, you may set it to null.\n- If we ask you to justify or provide confidence scores for the fields of a structure, please do so carefully. For each field we ask you to justify, please explain or justify your choice clearly. Confidence scores must be specified as real JSON numbers in the range `[0..1]`.\n- If we ask to you generate an optional field, either fill it or set it to null.\n- If we ask you to justify your choice of an enum variant, please explain your choice clearly. Do not justify your decision to leave a variant unselected. Provide your justification as a JSON string. Provide your confidence score as a real JSON number in the range `[0..1]`.\n- In terms of the JSON format we would like you to provide, the object itself will be deserialied based on the following schema. \n- We begin exactly at the top: the data should not be nested under a parent named 'fields' or anything similar.\n\n## Here is the schema to use:\n\nObject {\n    \"fields\": Object {\n        \"crate_name\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be the name of our crate, *verbatim*, trimmed for use as a toml field\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"full_readme_markdown\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be valid markdown representing the readme for this crate.\\\\n\\\\n Please make it terse, useful, and designed for an industrious commercially savy consumer of industrious nature. \\\\n\\\\n Advanced vocabulary and concepts are welcome. \\\\n\\\\n If this crate involves concepts from mathematics and/or physics, please describe and document them here.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_categories\": Object {\n            \"generation_instructions\": String(\"\\\"These categories will be used in our Cargo.toml file.\\\\n\\\\n It is important that users of this crate can find it on crates.io.\\\\n\\\\n Choose maximum 5 categories and make sure they are actually meaningful for our crate.\\\\n\\\\n These categories should only be chosen from among the legal crates.io categories. \\\\n\\\\n The ONLY legal categoreis are:\\\\n\\\\n``\\\\n accessibility\\\\n aerospace\\\\n algorithms\\\\n api-bindings\\\\n asynchronous\\\\n authentication\\\\n caching\\\\n command-line-interface\\\\n command-line-utilities\\\\n compilers\\\\n compression\\\\n computer-vision\\\\n concurrency\\\\n config\\\\n cryptography\\\\n data-structures\\\\n database\\\\n database-implementations\\\\n date-and-time\\\\n development-tools\\\\n email\\\\n embedded\\\\n emulators\\\\n encoding\\\\n external-ffi-bindings\\\\n filesystem\\\\n finance\\\\n game-development\\\\n game-engines\\\\n games\\\\n graphics\\\\n gui\\\\n hardware-support\\\\n internationalization\\\\n localization\\\\n mathematics\\\\n memory-management\\\\n multimedia\\\\n network-programming\\\\n no-std\\\\n os\\\\n parser-implementations\\\\n parsing\\\\n rendering\\\\n rust-patterns\\\\n science\\\\n simulation\\\\n template-engine\\\\n text-editors\\\\n text-processing\\\\n value-formatting\\\\n virtualization\\\\n visualization\\\\n wasm\\\\n web-programming\\\\n ``\\\\n\\\\n DO NOT specify a category that is not on this list.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n        \"package_description\": Object {\n            \"generation_instructions\": String(\"\\\"We will place this package_description in our Cargo.toml file so the users of this crate\\\\n know what it does.\\\\n\\\\n The discription should be technical, clear, and useful. It should be terse.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_keywords\": Object {\n            \"generation_instructions\": String(\"\\\"These keywords will be used in our Cargo.toml file so the users of this crate can easily\\\\n find it in the package system. Keywords should be useful, varied, meaningful, and reflect\\\\n the contents of our crate. They should make it easy to find. Please provide exactly five.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n    },\n    \"struct_docs\": String(\" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\"),\n    \"struct_name\": String(\"AiReadmeWriterDesiredOutput\"),\n    \"type\": String(\"struct\"),\n}\n\n## Please keep the following in mind:\n\nEntry phrasing:\n\n- Ensure entries are concise and focused.\n- Ensure there are no vague phrases (e.g., avoid starting with \"Illustrates the something something...\"; instead, use direct details like \"The hanging green vines on the garden wall\").\n\nEnsure Clarity and Consistency:\n\n- Use clear, grammatically correct sentences.\n- Maintain a consistent tone and style throughout.\n\nEnrich Content:\n\n- Communicate additional information where appropriate to enhance depth and value.\n- If a mathematical, physical, or otherwise technical background is helpful, please provide it.\n- Ensure that the content is comprehensive and meets all requirements.\n\nFocus Language:\n\n- Use deliberate and precise language.\n- Avoid vague verbs and keep the maximally intelligent and detail oriented audience in mind. \n- We explicitly ban the words `hush`, `subtle`, `mythic`, and `aura`. Please keep your vocabulary choices rigorous.\n- Do not use modern cultural references \n- Do not generically reference ideas which do not belong in the global environment.\n\n## Present the Final JSON:\n\n- Format the output as a JSON object.\n- Include all fields properly named from the schema we sent you.\n\nEnsure Proper Formatting:\n\n- Use proper JSON syntax with keys and arrays.\n- Ensure that all entries are correctly placed in their respective places.\n\nProvide only the final JSON data as specified by the schema. \n\nDo not include any schema generation instructions, meta-fields, nested-templates (unless directly asked) or extraneous information. \n\nProvide only the populated fields and their values, conforming exactly to the required structure without embedding any intermediate annotations or placeholders.\n\nYour output should only consist of the JSON object. do *not* include a preamble or postamble to your response. \n\nWe seek to successfully parse your response directly as JSON.\n\nFor the final answer, output the result only as a valid JSON object, wrapped inside triple backticks with json immediately after the opening backticks.\nThe JSON must be parser-ready:\n– No additional text before or after the block\n– No markdown or prose outside the fenced block\n– No HTML escaping of quotes, slashes, or other characters\n– Indentation and whitespace preserved exactly as written\n– The content between the fences must be valid JSON as per RFC 8259","role":"system"},{"content":"We have a rust crate named 'bitcoin-cuckoo-cache' 0.1.19. \n\nHere is the interface:\n/// A trait providing *eight* **independent** 32‑bit hashes for a value.\n///\n/// The reference C++ implementation uses a templated `operator()<k>(e)` to\n/// obtain the *k*‑th hash; in Rust we expose all eight at once.\npub trait EightWayHasher<E> {\n    fn hashes(&self, e: &E) -> [u32; 8];\n}\n\n/// @ref cache implements a cache with properties similar to a cuckoo-set.\n/// \n/// The cache is able to hold up to `(~(uint32_t)0) - 1` elements.\n///  Read Operations:\n///      - contains() for `erase=false`\n///  Read+Erase Operations:\n///      - contains() for `erase=true`\n///  Erase Operations:\n///      - allow_erase()\n///  Write Operations:\n///      - setup()\n///      - setup_bytes()\n///      - insert()\n///      - please_keep()\n///  Synchronization Free Operations:\n///      - invalid()\n///      - compute_hashes()\n/// User Must Guarantee:\n/// -1. Write requires synchronized access (e.g. a lock)\n/// -2. Read requires no concurrent Write, synchronized with last insert.\n/// -3. Erase requires no concurrent Write, synchronized with last insert.\n/// -4. An Erase caller must release all memory before allowing a new Writer.\n/// Note on function names:\n///   - The name \"allow_erase\" is used because the real discard happens later.\n///   - The name \"please_keep\" is used because elements may be erased anyways on insert.\n/// @tparam Element should be a movable and copyable type\n/// @tparam Hash should be a function/callable which takes a template parameter\n/// hash_select and an Element and extracts a hash from it. Should return high-entropy uint32_t\n/// hashes for `Hash h; h<0>(e) ... h<7>(e)`.\n///\n/// The Rust implementation keeps the exact algorithmic behaviour – including\n/// lazy reclamation & epoch aging – while relying on safe interior mutability\n/// (`AtomicU8` and `BitPackedAtomicFlags`) in place of the C++ `mutable` hack.\n#[derive(Getters, Debug)]\n#[get = \"pub(crate)\"]\npub struct Cache<Element, Hash> where\n    Element: PartialEq + Clone,\n    Hash: EightWayHasher<Element> {\n    table:                   Vec<Option<Element>>,\n    size:                    u32,\n    collection_flags:        BitPackedAtomicFlags,\n    epoch_flags:             Vec<bool>,\n    epoch_heuristic_counter: u32,\n    epoch_size:              u32,\n    depth_limit:             u8,\n    hash_function:           Hash,\n}\n\n\n/// @ref bit_packed_atomic_flags implements a container for garbage collection flags that is only\n/// thread unsafe on calls to setup. This class bit-packs collection flags for memory efficiency.\n/// \n/// All operations are `std::memory_order_relaxed` so external mechanisms must ensure that writes\n/// and reads are properly synchronized.\n/// On setup(n), all bits up to `n` are marked as collected.\n/// Under the hood, because it is an 8-bit type, it makes sense to use a multiple of 8 for setup,\n/// but it will be safe if that is not the case as well.\n/// No default constructor, as there must be some size.\n/// Thread‑unsafe **only** during [`BitPackedAtomicFlags::setup`] and `new`.\n///\n/// Internally stores one `AtomicU8` for every eight user‑visible bits, so its capacity is always\n/// rounded _up_ to the next multiple of eight.\n#[derive(Getters, Debug)]\npub struct BitPackedAtomicFlags  {\n    mem: Box<[AtomicU8]>,\n}\n\nimpl<E, H> Default for Cache<E, H> where\n    E: PartialEq + Clone,\n    H: EightWayHasher<E> + Default {\n\n    /// You must always construct a cache with some elements via a subsequent call to setup or\n    /// setup_bytes, otherwise operations may segfault.\n    fn default() -> Self {\n        // A completely empty cache – *must* be `setup()` before use.\n        Self {\n            table: Vec::new(),\n            size: 0,\n            collection_flags: BitPackedAtomicFlags::new(0),\n            epoch_flags: Vec::new(),\n            epoch_heuristic_counter: 0,\n            epoch_size: 0,\n            depth_limit: 0,\n            hash_function: H::default(),\n        }\n    }\n}\n\nimpl<E, H> Cache<E, H> where\n    E: PartialEq + Clone,\n    H: EightWayHasher<E> {\n\n    /// Map the eight raw hashes for `e` onto bucket indices in `[0, size)`.\n    ///\n    /// compute_hashes is convenience for not having to write out this expression everywhere we use\n    /// the hash values of an Element.\n    /// \n    /// We need to map the 32-bit input hash onto a hash bucket in a range [0, size) in a manner\n    /// which preserves as much of the hash's uniformity as possible. Ideally this would be done by\n    /// bitmasking but the size is usually not a power of two.\n    /// The naive approach would be to use a mod -- which isn't perfectly uniform but so long as\n    /// the hash is much larger than size it is not that bad. Unfortunately, mod/division is fairly\n    /// slow on ordinary microprocessors (e.g. 90-ish cycles on haswell, ARM doesn't even have an\n    /// instruction for it.); when the divisor is a constant the compiler will do clever tricks to\n    /// turn it into a multiply+add+shift, but size is a run-time value so the compiler can't do\n    /// that here.\n    /// One option would be to implement the same trick the compiler uses and compute the constants\n    /// for exact division based on the size, as described in \"{N}-bit Unsigned\n    /// Division via {N}-bit Multiply-Add\" by Arch D. Robison in 2005. But that code is somewhat\n    /// complicated and the result is still slower than other options:\n    /// Instead we treat the 32-bit random number as a Q32 fixed-point number in the range [0, 1)\n    /// and simply multiply it by the size. Then we just shift the result down by 32-bits to get\n    /// our bucket number. The result has non-uniformity the same as a mod, but it is much faster\n    /// to compute. More about this technique can be found at\n    /// https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/ .\n    /// The resulting non-uniformity is also more equally distributed which would be advantageous\n    /// for something like linear probing, though it shouldn't matter one way or the other for\n    /// a cuckoo table.\n    /// The primary disadvantage of this approach is increased intermediate precision is required\n    /// but for a 32-bit random number we only need the high 32 bits of a 32*32->64 multiply, which\n    /// means the operation is reasonably fast even on a typical 32-bit processor.\n    /// -----------\n    /// @param e\n    /// The element whose hashes will be returned\n    /// @return\n    /// Deterministic hashes derived from\n    /// `e` uniformly mapped onto the range\n    /// [0, size)\n    #[inline]\n    pub fn compute_hashes(&self, e: &E) -> [u32; 8] {\n        let size = self.size as u64;\n        self.hash_function\n            .hashes(e)\n            .map(|h| ((h as u64 * size) >> 32) as u32)\n    }\n\n    /// A bucket index that is *never* produced by [`compute_hashes`].\n    ///\n    /// invalid returns a special index that can never be inserted to\n    /// \n    /// ----------- @return\n    /// the special constexpr index that can never be inserted to\n    #[inline]\n    pub fn invalid() -> u32 {\n        u32::MAX\n    }\n\n    /// Mark bucket `n` as **discardable** – it may be overwritten later.\n    ///\n    /// allow_erase marks the element at index `n` as discardable. \n    /// Threadsafe without any concurrent insert.\n    /// \n    /// ----------- @param n\n    /// the index to allow erasure of\n    #[inline]\n    pub fn allow_erase(&self, n: u32) {\n        self.collection_flags.bit_set(n);\n        trace!(bucket = n, \"allow_erase()\");\n    }\n\n    /// Mark bucket `n` as **protected** – it must not be overwritten.\n    ///\n    /// please_keep marks the element at index `n` as an entry that should be\n    /// kept.\n    /// \n    /// Threadsafe without any concurrent insert.\n    /// ----------- @param n\n    /// the index to prioritize keeping\n    ////\n    #[inline]\n    pub fn please_keep(&self, n: u32) {\n        self.collection_flags.bit_unset(n);\n        trace!(bucket = n, \"please_keep()\");\n    }\n\n    /// Prepare the cache to hold *at most* `new_size` elements.\n    ///\n    /// Returns the *actual* table size (≥ 2).\n    /// setup initializes the container to store no more than new_size elements. setup should only\n    /// be called once.\n    /// \n    /// ----------- @param new_size\n    /// the desired number of elements to store\n    /// ----------- @return\n    /// the maximum number of elements storable\n    pub fn setup(&mut self, new_size: u32) -> u32 {\n        // depth_limit must be at least one otherwise errors can occur.\n        self.depth_limit = ((cmp::max(2, new_size) as f32).log2()) as u8;\n    \n        self.size        = cmp::max(2, new_size);\n    \n        self.table             = vec![None; self.size as usize];\n        self.collection_flags  .setup(self.size);\n        self.epoch_flags       = vec![false; self.size as usize];\n    \n        // Set to 45% as described above\n        self.epoch_size             = cmp::max(1, (45 * self.size) / 100);\n    \n        // Initially set to wait for a whole epoch\n        self.epoch_heuristic_counter = self.epoch_size;\n    \n        debug!(\n            size         = self.size,\n            depth_limit  = self.depth_limit,\n            epoch_size   = self.epoch_size,\n            \"cache setup() completed\"\n        );\n        self.size\n    }\n\n    /// Convenience wrapper translating *bytes* of storage into element count.\n    ///\n    /// setup_bytes is a convenience function which accounts for internal memory usage when\n    /// deciding how many elements to store. It isn't perfect because it doesn't account for any\n    /// overhead (struct size, MallocUsage, collection and epoch flags). This was done to simplify\n    /// selecting a power of two size. In the expected use case, an extra two bits per entry should\n    /// be negligible compared to the size of the elements.\n    /// \n    /// ----------- @param bytes\n    /// the approximate number of bytes to use for this data structure\n    /// ----------- @return\n    /// the maximum number of elements storable (see setup() documentation for more detail)\n    #[inline]\n    pub fn setup_bytes(&mut self, bytes: usize) -> u32 {\n        let elem_sz = cmp::max(mem::size_of::<E>(), 1);\n        self.setup((bytes / elem_sz) as u32)\n    }\n\n    /// Insert `e`, potentially evicting another entry according to the standard\n    /// 8‑way Cuckoo algorithm with bounded search depth.\n    ///\n    /// insert loops at most depth_limit times trying to insert a hash at various locations in the\n    /// table via a variant of the Cuckoo Algorithm with eight hash locations.\n    /// \n    /// It drops the last tried element if it runs out of depth before encountering an open slot.\n    /// Thus:\n    /// ----------- @param e\n    /// the element to insert @post one of the following: All previously inserted elements and\n    /// e are now in the table, one previously inserted element is evicted from the table, the\n    /// entry attempted to be inserted is evicted.\n    /// ----------- @code\n    /// ```no-test\n    /// insert(x);\n    /// return contains(x, false);\n    /// ```\n    /// is not guaranteed to return true.\n    #[inline]\n    pub fn insert(&mut self, mut e: E) {\n        self.epoch_check();\n    \n        let mut last_loc  = Self::invalid();\n        let mut last_epoch = true;\n        let mut locs      = self.compute_hashes(&e);\n    \n        // Already present?\n        //\n        // Make sure we have not already inserted this element\n        // If we have, make sure that it does not get deleted\n        for &loc in &locs {\n            if let Some(ref existing) = self.table[loc as usize] {\n                if existing == &e {\n                    self.please_keep(loc);\n                    self.epoch_flags[loc as usize] = last_epoch;\n                    return;\n                }\n            }\n        }\n    \n        for _depth in 0..self.depth_limit {\n            // First try to insert to an empty slot, if one exists\n            for &loc in &locs {\n                if self.collection_flags.bit_is_set(loc) {\n                    self.table[loc as usize] = Some(e);\n                    self.please_keep(loc);\n                    self.epoch_flags[loc as usize] = last_epoch;\n                    return;\n                }\n            }\n    \n            // Evict an entry – choose “next after last_loc” in `locs`.\n            let idx = locs\n                .iter()\n                .position(|&x| x == last_loc)\n                .map(|p| (p + 1) & 7)\n                .unwrap_or(0);\n    \n            /* Swap with the element at the location that was\n             * not the last one looked at. Example:\n             *\n             * 1. On first iteration, last_loc == invalid(), find returns last, so\n             *    last_loc defaults to locs[0].\n             * 2. On further iterations, where last_loc == locs[k], last_loc will\n             *    go to locs[k+1 % 8], i.e., next of the 8 indices wrapping around\n             *    to 0 if needed.\n             *\n             * This prevents moving the element we just put in.\n             *\n             * The swap is not a move -- we must switch onto the evicted element\n             * for the next iteration.\n             */\n            last_loc = locs[idx];\n    \n            let slot           = &mut self.table[last_loc as usize];\n            let mut victim     = slot.take().expect(\"victim must exist\");\n            mem::swap(&mut victim, &mut e); // move victim into `e`\n            *slot = Some(victim);\n    \n            let epoch_current = last_epoch;\n            last_epoch        = self.epoch_flags[last_loc as usize];\n            self.epoch_flags[last_loc as usize] = epoch_current;\n    \n            // Re‑hash the newly evicted element.\n            //\n            // Recompute the locs -- unfortunately happens one too many times!\n            locs = self.compute_hashes(&e);\n        }\n        // Dropped element `e` silently falls off the cache.\n        trace!(\"insert() evicted element after depth_limit\");\n    }\n\n    /// Check whether `e` is present, optionally marking its bucket discardable.\n    ///\n    /// contains iterates through the hash locations for a given element and checks to see if it\n    /// is present. contains does not check garbage collected state (in other words, garbage is\n    /// only collected when the space is needed), so:\n    /// \n    /// -----------\n    /// @param e\n    /// the element to check\n    /// ----------\n    /// @param erase\n    /// whether to attempt setting the garbage collect flag @post if erase is true and the\n    /// element is found, then the garbage collect flag is set\n    /// @code\n    /// ```no-test\n    /// insert(x);\n    /// if (contains(x, true))\n    ///     return contains(x, false);\n    /// else\n    ///     return true;\n    /// ```\n    /// executed on a single thread will always return true!\n    /// This is a great property for re-org performance for example. contains returns a bool set\n    /// true if the element was found.\n    /// @return\n    /// true if the element is found, false otherwise\n    #[inline]\n    pub fn contains(&self, e: &E, erase: bool) -> bool {\n        for &loc in &self.compute_hashes(e) {\n            if let Some(ref existing) = self.table[loc as usize] {\n                if existing == e {\n                    if erase {\n                        self.allow_erase(loc);\n                    }\n                    return true;\n                }\n            }\n        }\n        false\n    }\n}\n\nimpl BitPackedAtomicFlags {\n\n    /// bit_packed_atomic_flags constructor creates memory to sufficiently keep\n    /// track of garbage collection information for `size` entries.\n    /// \n    /// ----------- @param size\n    /// the number of elements to allocate space for @post bit_set, bit_unset,\n    /// and bit_is_set function properly forall x. x < size @post All calls to\n    /// bit_is_set (without subsequent bit_unset) will return true.\n    /// Construct a new flag container able to track at least `size` bits.\n    ///\n    /// All bits start **set** (`true` / _collectable_).\n    #[inline]\n    pub fn new(size: u32) -> Self {\n        trace!(size, \"initialising BitPackedAtomicFlags\");\n    \n        // Round up so that every element has a backing bit.\n        let cells = ((size + 7) / 8) as usize;\n        let boxed: Box<[AtomicU8]> = (0..cells)\n            .map(|_| AtomicU8::new(0xFF))\n            .collect::<Vec<_>>()\n            .into_boxed_slice();\n    \n        debug!(cells, \"allocated atomic byte array\");\n        Self { mem: boxed }\n    }\n\n    /// setup marks all entries and ensures that bit_packed_atomic_flags can\n    /// store at least `b` entries.\n    /// \n    /// ----------- @param b\n    /// the number of elements to allocate space for @post bit_set, bit_unset,\n    /// and bit_is_set function properly forall x. x < b @post All calls to\n    /// bit_is_set (without subsequent bit_unset) will return true.\n    /// (Re)initialise the container to hold *at least* `b` bits, setting them\n    /// all to **set** (`true`).\n    ///\n    /// Safe to call multiple times; any previous storage is dropped.\n    #[inline]\n    pub fn setup(&mut self, b: u32) {\n        trace!(b, \"re‑initialising BitPackedAtomicFlags via setup()\");\n        // Replace `self` in‑place to avoid double‑initialisation boilerplate.\n        *self = Self::new(b);\n    }\n\n    /// bit_set sets an entry as discardable.\n    /// \n    /// ----------- @param s\n    /// the index of the entry to bit_set @post immediately subsequent call\n    /// (assuming proper external memory ordering) to bit_is_set(s) == true.\n    /// Mark bit `s` as **set** (collectable).\n    ///\n    #[inline]\n    pub fn bit_set(&self, s: u32) {\n        let idx  = (s >> 3) as usize;\n        let mask = 1u8 << (s & 7);\n        self.mem[idx].fetch_or(mask, atomic::Ordering::Relaxed); // <‑‑‑ Ordering fixed\n        trace!(index = s, mask, \"bit_set()\");\n    }\n\n    /// bit_unset marks an entry as something that should not be overwritten.\n    /// \n    /// ----------- @param s\n    /// the index of the entry to bit_unset @post immediately subsequent call\n    /// (assuming proper external memory ordering) to bit_is_set(s) == false.\n    /// Mark bit `s` as **unset** (keep / not‑collectable).\n    #[inline]\n    pub fn bit_unset(&self, s: u32) {\n        let idx  = (s >> 3) as usize;\n        let mask = !(1u8 << (s & 7));\n        self.mem[idx].fetch_and(mask, atomic::Ordering::Relaxed); // <‑‑‑ Ordering fixed\n        trace!(index = s, mask, \"bit_unset()\");\n    }\n\n    /// bit_is_set queries the table for discardability at `s`.\n    /// \n    /// ----------- @param s\n    /// the index of the entry to read\n    /// ----------- @return\n    /// true if the bit at index `s` was set, false otherwise\n    /// Returns `true` iff bit `s` is currently **set**.\n    #[inline]\n    pub fn bit_is_set(&self, s: u32) -> bool {\n        let idx  = (s >> 3) as usize;\n        let mask = 1u8 << (s & 7);\n        let value = (self.mem[idx].load(atomic::Ordering::Relaxed) & mask) != 0; // <‑‑‑ Ordering fixed\n        trace!(index = s, mask, value, \"bit_is_set()\");\n        value\n    }\n}\n\n\nThis crate has the following authors:\n[\n    \"klebs <none>\",\n]\nThis crate has the following rust edition:\n2021\nThis crate has the following license:\nMIT\nThis crate is held in the following repository:\nhttps://github.com/klebs6/bitcoin-rs\n","role":"user"}],"model":"gpt-5.1"}}
