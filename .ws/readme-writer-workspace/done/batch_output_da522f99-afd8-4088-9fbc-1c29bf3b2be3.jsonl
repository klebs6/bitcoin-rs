{"id": "batch_req_692cdf4157a88190ba1a0ed1cfd3dccb", "custom_id": "bitcoin-get-json-token-ai-readme-request", "response": {"status_code": 200, "request_id": "2882a0f67818025e0b85c63d24c2369a", "body": {"id": "chatcmpl-Chlvv065bcxMkoSp4S2cOLuH0CG2j", "object": "chat.completion", "created": 1764548395, "model": "gpt-5.1-2025-11-13", "choices": [{"index": 0, "message": {"role": "assistant", "content": "```json\n{\n  \"fields\": {\n    \"crate_name\": \"bitcoin-get-json-token\",\n    \"full_readme_markdown\": \"# bitcoin-get-json-token\\n\\nA minimal, allocation-aware JSON lexer extracted from the Bitcoin Core codebase and re-engineered in safe(ish) Rust. It focuses on lexing \u2013 not building an AST \u2013 and is optimised for incremental, streaming inspection of JSON text in security\u2011sensitive and latency\u2011sensitive contexts (e.g. P2P protocol handling, RPC servers, and log ingestion).\\n\\n> Note: This README was generated by an AI model based on the public API. It may not be perfectly accurate, but it should be close and is intended to be technically useful.\\n\\n---\\n\\n## Design intent\\n\\nThis crate exposes a low\u2011level tokeniser for JSON, suitable when:\\n\\n- You already have allocation and ownership policies and only need a lexer.\\n- You want deterministic, spec\u2011compliant classification of JSON tokens without committing to a full DOM/AST.\\n- You are processing untrusted data (e.g. network payloads) and require strong guarantees around numeric syntax, escape semantics, and whitespace handling.\\n- You are porting / interoperating with Bitcoin Core infrastructure and want a functionally equivalent lexer in Rust.\\n\\nNo `serde`, no dynamic dispatch, no hidden allocations beyond explicit `String` outputs for token literals.\\n\\n---\\n\\n## Core concepts\\n\\nThe lexer operates on raw byte pointers `*const u8` delimited by `[raw, end)`. It performs a single forward pass and emits one token at a time. The caller is responsible for advancing the cursor using the returned `consumed` byte count.\\n\\n```rust\\n#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\\npub enum JTokenType {\\n    JTOK_ERR,\\n    JTOK_NONE,\\n    JTOK_OBJ_OPEN,\\n    JTOK_OBJ_CLOSE,\\n    JTOK_ARR_OPEN,\\n    JTOK_ARR_CLOSE,\\n    JTOK_COLON,\\n    JTOK_COMMA,\\n    JTOK_KW_NULL,\\n    JTOK_KW_TRUE,\\n    JTOK_KW_FALSE,\\n    JTOK_NUMBER,\\n    JTOK_STRING,\\n}\\n```\\n\\nThe lexer is **intentionally primitive**:\\n\\n- No implicit buffering beyond caller\u2011owned `String`s.\\n- No recursion, no tree building.\\n- Operates on byte slices and raw pointers for maximal control.\\n\\nThis makes it especially suitable for:\\n\\n- Network protocol parsers.\\n- Constrained runtimes where you want to avoid heavyweight JSON frameworks.\\n- Interfacing with foreign memory (FFI) where you primarily need syntax validation and token classification.\\n\\n---\\n\\n## High\u2011level API overview\\n\\n### `get_json_token`\\n\\n```rust\\npub fn get_json_token(\\n    token_val: &mut String,\\n    consumed:  &mut u32,\\n    raw:       *const u8,\\n    end:       *const u8,\\n) -> JTokenType\\n```\\n\\nLexes the **next JSON token** starting at `raw` and not past `end`.\\n\\nBehaviour:\\n\\n1. Skips leading JSON whitespace (`SP`, `HT`, `LF`, `CR`) and trailing `NUL` padding.\\n2. Attempts to lex, in order:\\n   - Structural tokens: `{ } [ ] : ,`\\n   - Keywords: `null`, `true`, `false`\\n   - Numbers (RFC 7159 compliant; strict about leading zeros, exponents, etc.)\\n   - Strings, including full handling of `\\\\uXXXX` and surrogate pairs\\n3. On success:\\n   - Returns the corresponding `JTokenType`.\\n   - Writes the token byte length into `*consumed` (excluding leading whitespace).\\n   - For `JTOK_NUMBER` and `JTOK_STRING`, writes the decoded literal into `token_val`.\\n4. On reaching the end of input, emits `JTOK_NONE`.\\n5. On syntactic violation, emits `JTOK_ERR`.\\n\\n#### Example: single\u2011pass tokenisation\\n\\n```rust\\nuse bitcoin_get_json_token::{\\n    get_json_token, JTokenType,\\n};\\n\\nfn lex_all(input: &str) -> Vec<(JTokenType, String)> {\\n    let bytes = input.as_bytes();\\n    let mut tokens = Vec::new();\\n    let mut offset = 0usize;\\n\\n    while offset < bytes.len() {\\n        let mut val = String::new();\\n        let mut consumed = 0u32;\\n        let raw = unsafe { bytes.as_ptr().add(offset) };\\n        let end = unsafe { bytes.as_ptr().add(bytes.len()) };\\n\\n        let kind = get_json_token(&mut val, &mut consumed, raw, end);\\n\\n        if kind == JTokenType::JTOK_NONE {\\n            break; // EOF\\n        }\\n        if kind == JTokenType::JTOK_ERR {\\n            panic!(\\\"lexing error at byte {}\\\", offset);\\n        }\\n\\n        tokens.push((kind, val));\\n        offset += consumed as usize;\\n    }\\n\\n    tokens\\n}\\n\\nfn main() {\\n    let json = r#\\\"{\\\"x\\\": 1.23e4, \\\"y\\\": \\\"hello\\\"}\\\"#;\\n    let tokens = lex_all(json);\\n    for (kind, val) in tokens {\\n        println!(\\\"{:?} => {:?}\\\", kind, val);\\n    }\\n}\\n```\\n\\n---\\n\\n## Numeric validation\\n\\n### `valid_num_str`\\n\\n```rust\\npub fn valid_num_str(s: &str) -> bool\\n```\\n\\nReturns **true** iff `s` is _exactly_ one syntactically valid JSON number per RFC 7159.\\n\\nInternally, this simply calls `get_json_token` on the string and checks:\\n\\n- that the token type is `JTOK_NUMBER`, and\\n- that the token consumes the entire string.\\n\\nThis function is useful when you:\\n\\n- Accept numeric fields as strings (e.g. to avoid precision loss) but still want strict JSON numeric syntax.\\n- Need to guard a numeric parser that assumes well\u2011formed input.\\n\\n#### Example\\n\\n```rust\\nuse bitcoin_get_json_token::valid_num_str;\\n\\nassert!(valid_num_str(\\\"0\\\"));\\nassert!(valid_num_str(\\\"-0.1\\\"));\\nassert!(valid_num_str(\\\"1e10\\\"));\\nassert!(!valid_num_str(\\\"01\\\"));      // leading zero\\nassert!(!valid_num_str(\\\"1.\\\"));      // missing fractional digits\\nassert!(!valid_num_str(\\\"1e\\\"));      // incomplete exponent\\nassert!(!valid_num_str(\\\"-\\\"));       // incomplete sign\\n```\\n\\n---\\n\\n## String and escape handling\\n\\n### `lex_string`\\n\\n```rust\\npub fn lex_string(\\n    token_val: &mut String,\\n    p: *const u8,\\n    end: *const u8,\\n) -> Option<(JTokenType, *const u8, u32)>\\n```\\n\\nLexes a JSON string beginning with `\\\"` at `p` and decodes escape sequences into `token_val`:\\n\\n- Simple escapes: `\\\\\\\"`, `\\\\\\\\`, `\\\\/`, `\\\\b`, `\\\\f`, `\\\\n`, `\\\\r`, `\\\\t`.\\n- Unicode escapes: `\\\\uXXXX` with correct handling of UTF\u201116 surrogate pairs.\\n- Rejects control characters `< 0x20` and unterminated strings.\\n\\nThe returned tuple contains:\\n\\n- `JTokenType::JTOK_STRING` on success.\\n- Pointer after the closing quote.\\n- Bytes consumed for the token.\\n\\n### `parse_string_escape`\\n\\n```rust\\npub fn parse_string_escape(\\n    backslash: *const u8,\\n    end: *const u8,\\n) -> Option<(String, *const u8)>\\n```\\n\\nLow\u2011level helper that parses a **single** escape sequence starting at a backslash:\\n\\n- `backslash` must point at `b'\\\\'`.\\n- On success, returns the decoded segment and the pointer after the escape.\\n- On error (truncated, bad hex, invalid surrogate structure), returns `None`.\\n\\nThis is useful if you are building a custom string parser but want to reuse the robust escape decoding logic, including surrogate\u2011pair composition:\\n\\n```rust\\nuse bitcoin_get_json_token::parse_string_escape;\\n\\nfn decode_escape_sequence(input: &str, idx_of_backslash: usize) -> Option<(String, usize)> {\\n    let bytes = input.as_bytes();\\n    let backslash = unsafe { bytes.as_ptr().add(idx_of_backslash) };\\n    let end = unsafe { bytes.as_ptr().add(bytes.len()) };\\n    let (decoded, ptr_after) = parse_string_escape(backslash, end)?;\\n    let new_offset = (ptr_after as usize) - (bytes.as_ptr() as usize);\\n    Some((decoded, new_offset))\\n}\\n```\\n\\n---\\n\\n## Structural and keyword lexers\\n\\nThese are composable primitives used by `get_json_token`. You can call them directly if you are assembling a custom state machine.\\n\\n### `lex_structural`\\n\\n```rust\\npub fn lex_structural(p: *const u8, end: *const u8) -> Option<(JTokenType, *const u8, u32)>;\\n```\\n\\nRecognises one of:\\n\\n- `{` \u2192 `JTOK_OBJ_OPEN`\\n- `}` \u2192 `JTOK_OBJ_CLOSE`\\n- `[` \u2192 `JTOK_ARR_OPEN`\\n- `]` \u2192 `JTOK_ARR_CLOSE`\\n- `:` \u2192 `JTOK_COLON`\\n- `,` \u2192 `JTOK_COMMA`\\n\\nReturns `None` if `p` is at `end` or if the byte is not a recognised structural character.\\n\\n### `lex_keyword`\\n\\n```rust\\npub fn lex_keyword(p: *const u8, end: *const u8) -> Option<(JTokenType, *const u8, u32)>;\\n```\\n\\nRecognises:\\n\\n- `null`  \u2192 `JTOK_KW_NULL`\\n- `true`  \u2192 `JTOK_KW_TRUE`\\n- `false` \u2192 `JTOK_KW_FALSE`\\n\\nThis function assumes ASCII input and does exact byte comparison; there is no case folding, as required by JSON.\\n\\n---\\n\\n## Character classification and helpers\\n\\n### `json_isspace`\\n\\n```rust\\npub fn json_isspace(ch: i32) -> bool\\n```\\n\\nReturns `true` if `ch` is one of the four JSON whitespace characters:\\n\\n- space (U+0020)\\n- horizontal tab (U+0009)\\n- line feed (U+000A)\\n- carriage return (U+000D)\\n\\n### `json_isdigit`\\n\\n```rust\\npub fn json_isdigit(ch: i32) -> bool\\n```\\n\\nReturns `true` if `ch` is an ASCII digit `0..=9`.\\n\\n### `skip_ws_nul`\\n\\n```rust\\npub fn skip_ws_nul(mut p: *const u8, end: *const u8) -> *const u8\\n```\\n\\nSkips JSON whitespace per `json_isspace` **and** any `NUL` padding bytes (`0x00`), returning the first non\u2011whitespace, non\u2011NUL pointer.\\n\\nThis is particularly useful for handling buffers padded with zeroes or assembled from fixed\u2011length frames.\\n\\n### `json_token_is_value`\\n\\n```rust\\npub fn json_token_is_value(jtt: JTokenType) -> bool\\n```\\n\\nReturns `true` if `jtt` is one of the **terminal** JSON value tokens:\\n\\n- `JTOK_KW_NULL`\\n- `JTOK_KW_TRUE`\\n- `JTOK_KW_FALSE`\\n- `JTOK_NUMBER`\\n- `JTOK_STRING`\\n\\nThis is a tiny convenience for parsers that distinguish container structure from scalar values.\\n\\n### `bytes_consumed`\\n\\n```rust\\npub fn bytes_consumed(start: *const u8, after: *const u8) -> u32\\n```\\n\\nComputes the difference in bytes between two pointers in the same buffer. All lexers use this for book\u2011keeping.\\n\\n### `single_byte_token`\\n\\n```rust\\npub fn single_byte_token(\\n    p:        &mut *const u8,\\n    start:    *const u8,\\n    consumed: &mut u32,\\n    kind:     JTokenType,\\n) -> JTokenType\\n```\\n\\nUtility for emitting a token corresponding to a single byte and advancing the pointer.\\n\\n---\\n\\n## Hex parsing primitive\\n\\n### `hatoui`\\n\\n```rust\\npub fn hatoui(first: *const u8, last: *const u8, out: &mut u32) -> *const u8\\n```\\n\\nConverts a hexadecimal slice `[first, last)` into a `u32`.\\n\\n- Parses contiguous hex digits `0\u20119`, `a\u2011f`, `A\u2011F`.\\n- Stops at the first non\u2011hex byte or at `last`.\\n- Writes the parsed value into `*out`.\\n- Returns a pointer to the first **unconsumed** byte.\\n\\nThis is used internally for `\\\\uXXXX` decoding and is independently useful for other protocol\u2011level hex parsing tasks.\\n\\n---\\n\\n## Safety and `unsafe` considerations\\n\\nThe API exposes several functions that accept raw pointers. The caller must ensure:\\n\\n- `raw` / `first` / `backslash` / `p` / `start` and `end` / `last` refer to the same valid allocated buffer.\\n- `raw <= end`, `first <= last`, and all intermediate pointer arithmetic stays within bounds or at most one\u2011past\u2011the\u2011end, as per Rust's pointer rules.\\n- The underlying data is immutable while parsing.\\n\\nThe crate itself attempts to:\\n\\n- Restrict `unsafe` usage to clear, small regions where pointer arithmetic is required.\\n- Avoid undefined behaviour by adhering to Rust's aliasing and lifetime constraints.\\n\\nIf you prefer a fully safe interface, you can wrap these primitives with slice\u2011based abstractions (`&[u8]`) and manage indices rather than raw pointers.\\n\\n---\\n\\n## Tracing and observability\\n\\nMany functions are annotated with `#[instrument]` and emit `trace!` events. This assumes you have `tracing` configured in your application. The emitted fields include:\\n\\n- Token kind and byte counts.\\n- For numbers and strings, the parsed literal.\\n- Decision points (e.g. why a candidate number or string was rejected).\\n\\nThis makes it straightforward to inspect malformed input from untrusted peers and to profile tokenisation behaviour.\\n\\n---\\n\\n## When to use this crate vs general JSON libraries\\n\\nUse this crate when you:\\n\\n- Already have a bespoke parser or state machine and just need a **lexical** front\u2011end.\\n- Need full transparency into numeric and string syntax acceptance.\\n- Care about tight control over allocations and do not want an implicit object model.\\n\\nPrefer `serde_json` or similar when you:\\n\\n- Want high\u2011level `Deserialize` / `Serialize` drives with minimal manual code.\\n- Do not need raw pointer\u2011level, incremental control.\\n\\nBoth approaches can coexist: you can pre\u2011screen or partially tokenise with this crate, and then hand selected slices to a higher\u2011level deserialiser.\\n\\n---\\n\\n## License and metadata\\n\\n- **Crate name:** `bitcoin-get-json-token`\\n- **Version:** 0.1.1\\n- **Edition:** 2024\\n- **License:** MIT\\n- **Author:** `klebs6 <tpk3.mx@gmail.com>`\\n\\nContributions and issue reports are encouraged, especially around edge\u2011case compatibility with Bitcoin Core's original lexer semantics.\\n\",\n    \"package_categories\": [\n      \"parsing\",\n      \"parser-implementations\",\n      \"algorithms\",\n      \"network-programming\",\n      \"development-tools\"\n    ],\n    \"package_description\": \"Low-level, Bitcoin-inspired JSON lexer/tokeniser for strict RFC-compliant number, string, and structural token parsing over raw byte buffers, with tracing instrumentation and minimal allocations.\",\n    \"package_keywords\": [\n      \"json\",\n      \"lexer\",\n      \"bitcoin\",\n      \"parsing\",\n      \"tokenizer\"\n    ]\n  },\n  \"struct_docs\": \" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\",\n  \"struct_name\": \"AiReadmeWriterDesiredOutput\",\n  \"type\": \"struct\"\n}\n```", "refusal": null, "annotations": []}, "finish_reason": "stop"}], "usage": {"prompt_tokens": 5755, "completion_tokens": 3736, "total_tokens": 9491, "prompt_tokens_details": {"cached_tokens": 1792, "audio_tokens": 0}, "completion_tokens_details": {"reasoning_tokens": 0, "audio_tokens": 0, "accepted_prediction_tokens": 0, "rejected_prediction_tokens": 0}}, "service_tier": "default", "system_fingerprint": null}}, "error": null}
