{"custom_id":"bitcoinleveldb-logreader-ai-readme-request","method":"POST","url":"/v1/chat/completions","body":{"max_completion_tokens":8192,"messages":[{"content":"We would like you to write a README.md for a rust crate with maximally helpful content.\nAdditionally, we would like you to generate several fields from the Cargo.toml package section for this crate.\nWe will show you the strict json output format we would like you to provide.\nPlease tailor your response for an apex consumer of maximal intelligence.\nA reader of this README.md should have a good sense of what the crate does and how to use it after reading what you write.\nPlease include a messaage indicating that this README.md file was generated by an AI model and may not be 100% accurate however it should be pretty good.\n\n\nCarefully read and parse the information we sent you.\n\nWe want to generate all information items requested by our schema:\n\n- Read the description of each field. Use it, along with the content we sent you to generate an appropriate value for each.\n- Ensure that each generated field optimally serves its desired purpose.\n- Each item should be deep, detailed, and specific. Use optimally descriptive and useful language. Do not be too verbose.\n- Do not add any extra keys to the generated object. Generate precisely what we ask of you: no more, no less.\n- For fields which are of an enum type: pick *exactly one variant*. Do not generate any information for the unselected variants.\n- For array fields: supply a JSON list [ ... ] of items, each conforming to the correct item type.\n- For map fields: Provide a JSON object { \"key_as_string\": <value>, ... }. Keys must be valid JSON strings. Keys and Values must conform to the key and value schemas we provide to you. Fill all subfields of each and every map value.\n- If we ask you to justify a map field, provide your justification/confidence score for the entire map. Do not attempt to separately justify each map element.\n- Likewise, if we ask you to justify an array field, provide your justification/confidence score for the entire array. Do not attempt to separately justify each element.\n- Provide numeric fields as real JSON numbers (not strings).\n- For string fields: provide the value precisely as a string literal. If optional, you may set it to null.\n- If we ask you to justify or provide confidence scores for the fields of a structure, please do so carefully. For each field we ask you to justify, please explain or justify your choice clearly. Confidence scores must be specified as real JSON numbers in the range `[0..1]`.\n- If we ask to you generate an optional field, either fill it or set it to null.\n- If we ask you to justify your choice of an enum variant, please explain your choice clearly. Do not justify your decision to leave a variant unselected. Provide your justification as a JSON string. Provide your confidence score as a real JSON number in the range `[0..1]`.\n- In terms of the JSON format we would like you to provide, the object itself will be deserialied based on the following schema. \n- We begin exactly at the top: the data should not be nested under a parent named 'fields' or anything similar.\n\n## Here is the schema to use:\n\nObject {\n    \"fields\": Object {\n        \"crate_name\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be the name of our crate, *verbatim*, trimmed for use as a toml field\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"full_readme_markdown\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be valid markdown representing the readme for this crate.\\\\n\\\\n Please make it terse, useful, and designed for an industrious commercially savy consumer of industrious nature. \\\\n\\\\n Advanced vocabulary and concepts are welcome. \\\\n\\\\n If this crate involves concepts from mathematics and/or physics, please describe and document them here.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_categories\": Object {\n            \"generation_instructions\": String(\"\\\"These categories will be used in our Cargo.toml file.\\\\n\\\\n It is important that users of this crate can find it on crates.io.\\\\n\\\\n Choose maximum 5 categories and make sure they are actually meaningful for our crate.\\\\n\\\\n These categories should only be chosen from among the legal crates.io categories. \\\\n\\\\n The ONLY legal categoreis are:\\\\n\\\\n``\\\\n accessibility\\\\n aerospace\\\\n algorithms\\\\n api-bindings\\\\n asynchronous\\\\n authentication\\\\n caching\\\\n command-line-interface\\\\n command-line-utilities\\\\n compilers\\\\n compression\\\\n computer-vision\\\\n concurrency\\\\n config\\\\n cryptography\\\\n data-structures\\\\n database\\\\n database-implementations\\\\n date-and-time\\\\n development-tools\\\\n email\\\\n embedded\\\\n emulators\\\\n encoding\\\\n external-ffi-bindings\\\\n filesystem\\\\n finance\\\\n game-development\\\\n game-engines\\\\n games\\\\n graphics\\\\n gui\\\\n hardware-support\\\\n internationalization\\\\n localization\\\\n mathematics\\\\n memory-management\\\\n multimedia\\\\n network-programming\\\\n no-std\\\\n os\\\\n parser-implementations\\\\n parsing\\\\n rendering\\\\n rust-patterns\\\\n science\\\\n simulation\\\\n template-engine\\\\n text-editors\\\\n text-processing\\\\n value-formatting\\\\n virtualization\\\\n visualization\\\\n wasm\\\\n web-programming\\\\n ``\\\\n\\\\n DO NOT specify a category that is not on this list.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n        \"package_description\": Object {\n            \"generation_instructions\": String(\"\\\"We will place this package_description in our Cargo.toml file so the users of this crate\\\\n know what it does.\\\\n\\\\n The discription should be technical, clear, and useful. It should be terse.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_keywords\": Object {\n            \"generation_instructions\": String(\"\\\"These keywords will be used in our Cargo.toml file so the users of this crate can easily\\\\n find it in the package system. Keywords should be useful, varied, meaningful, and reflect\\\\n the contents of our crate. They should make it easy to find. Please provide exactly five.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n    },\n    \"struct_docs\": String(\" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\"),\n    \"struct_name\": String(\"AiReadmeWriterDesiredOutput\"),\n    \"type\": String(\"struct\"),\n}\n\n## Please keep the following in mind:\n\nEntry phrasing:\n\n- Ensure entries are concise and focused.\n- Ensure there are no vague phrases (e.g., avoid starting with \"Illustrates the something something...\"; instead, use direct details like \"The hanging green vines on the garden wall\").\n\nEnsure Clarity and Consistency:\n\n- Use clear, grammatically correct sentences.\n- Maintain a consistent tone and style throughout.\n\nEnrich Content:\n\n- Communicate additional information where appropriate to enhance depth and value.\n- If a mathematical, physical, or otherwise technical background is helpful, please provide it.\n- Ensure that the content is comprehensive and meets all requirements.\n\nFocus Language:\n\n- Use deliberate and precise language.\n- Avoid vague verbs and keep the maximally intelligent and detail oriented audience in mind. \n- We explicitly ban the words `hush`, `subtle`, `mythic`, and `aura`. Please keep your vocabulary choices rigorous.\n- Do not use modern cultural references \n- Do not generically reference ideas which do not belong in the global environment.\n\n## Present the Final JSON:\n\n- Format the output as a JSON object.\n- Include all fields properly named from the schema we sent you.\n\nEnsure Proper Formatting:\n\n- Use proper JSON syntax with keys and arrays.\n- Ensure that all entries are correctly placed in their respective places.\n\nProvide only the final JSON data as specified by the schema. \n\nDo not include any schema generation instructions, meta-fields, nested-templates (unless directly asked) or extraneous information. \n\nProvide only the populated fields and their values, conforming exactly to the required structure without embedding any intermediate annotations or placeholders.\n\nYour output should only consist of the JSON object. do *not* include a preamble or postamble to your response. \n\nWe seek to successfully parse your response directly as JSON.\n\nFor the final answer, output the result only as a valid JSON object, wrapped inside triple backticks with json immediately after the opening backticks.\nThe JSON must be parser-ready:\n– No additional text before or after the block\n– No markdown or prose outside the fenced block\n– No HTML escaping of quotes, slashes, or other characters\n– Indentation and whitespace preserved exactly as written\n– The content between the fences must be valid JSON as per RFC 8259","role":"system"},{"content":"We have a rust crate named 'bitcoinleveldb-logreader' 0.1.1. \n\nHere is the interface:\n/**\n| Interface for reporting errors.\n|\n*/\npub trait LogReaderReporter {\n    fn corruption(&mut self, bytes: usize, status: &Status);\n}\n\n#[derive(Builder,Setters,Getters,MutGetters)]\n#[getset(set = \"pub\", get = \"pub\", get_mut = \"pub\")]\n#[builder(pattern = \"owned\", setter(into))]\npub struct LogReader  {\n    file:                 Box<dyn SequentialFile>,\n    reporter:             Box<dyn LogReaderReporter>,\n    checksum:             bool,\n    backing_store:        *const u8,\n    buffer:               Slice,\n    eof:                  bool,\n    last_record_offset:   u64,\n    end_of_buffer_offset: u64,\n    initial_offset:       u64,\n    resyncing:            bool,\n}\n\nimpl LogReader {\n\n    /// Create a reader that will return log records from \"*file\".  \"*file\" must\n    /// remain live while this Reader is in use.\n    /// \n    /// If \"reporter\" is non-null, it is notified whenever some data is dropped\n    /// due to a detected corruption.  \"*reporter\" must remain live while this\n    /// Reader is in use.\n    /// If \"checksum\" is true, verify checksums if available.\n    /// The Reader will start reading at the first record located at physical\n    /// position >= initial_offset within the file.\n    ///\n    pub fn new(\n        file:           Box<dyn SequentialFile>,\n        reporter:       Box<dyn LogReaderReporter>,\n        checksum:       bool,\n        initial_offset: u64\n    ) -> Self\n    {\n        info!(\n            \"LogReader::new: checksum={} initial_offset={}\",\n            checksum,\n            initial_offset\n        );\n    \n        let backing_box: Box<[u8; LOG_BLOCK_SIZE as usize]> =\n            Box::new([0u8; LOG_BLOCK_SIZE as usize]);\n        let backing_store = Box::into_raw(backing_box) as *const u8;\n    \n        LogReaderBuilder::default()\n            .file(file)\n            .reporter(reporter)\n            .checksum(checksum)\n            .backing_store(backing_store)\n            .buffer(Slice::default())\n            .eof(false)\n            .last_record_offset(0u64)\n            .end_of_buffer_offset(0u64)\n            .initial_offset(initial_offset)\n            .resyncing(initial_offset > 0)\n            .build()\n            .unwrap()\n    }\n}\n\nimpl LogReader {\n\n    /// Read the next logical record into `record`.\n    ///\n    /// Returns `true` if a record was read successfully and `false` on EOF.\n    /// The contents in `record` remain valid only until the next\n    /// mutating operation on this reader or the next mutation to `scratch`.\n    pub fn read_record(&mut self, record: &mut Slice, scratch: &mut Vec<u8>) -> bool {\n        trace!(\n            last_record_offset = *self.last_record_offset(),\n            initial_offset     = self.initial_log_offset(),\n            \"LogReader::read_record: enter\"\n        );\n    \n        if *self.last_record_offset() < self.initial_log_offset() {\n            if !self.skip_to_initial_block() {\n                info!(\n                    last_record_offset = *self.last_record_offset(),\n                    initial_offset     = self.initial_log_offset(),\n                    \"LogReader::read_record: skip_to_initial_block returned false (EOF or error)\"\n                );\n                return false;\n            }\n        }\n    \n        scratch.clear();\n        record.clear();\n    \n        let mut in_fragmented_record          = false;\n        let mut prospective_record_offset: u64 = 0;\n        let mut fragment                      = Slice::default();\n    \n        const PARTIAL_WITHOUT_END_1: &[u8] =\n            b\"partial record without end(1)\\0\";\n        const PARTIAL_WITHOUT_END_2: &[u8] =\n            b\"partial record without end(2)\\0\";\n        const MISSING_START_1: &[u8] =\n            b\"missing start of fragmented record(1)\\0\";\n        const MISSING_START_2: &[u8] =\n            b\"missing start of fragmented record(2)\\0\";\n        const ERROR_MIDDLE: &[u8] = b\"error in middle of record\\0\";\n        const UNKNOWN_RECORD_TYPE: &[u8] =\n            b\"unknown record type\\0\";\n    \n        loop {\n            let record_type =\n                self.read_physical_record(&mut fragment as *mut Slice);\n    \n            let fragment_len: u64 =\n                *fragment.size() as u64;\n            let buffer_remaining: u64 =\n                *self.buffer_slice().size() as u64;\n            let header_len: u64 = LOG_HEADER_SIZE as u64;\n    \n            let physical_record_offset = self\n                .end_of_buffer_offset_value()\n                .saturating_sub(buffer_remaining)\n                .saturating_sub(header_len)\n                .saturating_sub(fragment_len);\n    \n            trace!(\n                record_type,\n                physical_record_offset,\n                fragment_len,\n                buffer_remaining,\n                end_of_buffer_offset = self.end_of_buffer_offset_value(),\n                resyncing            = self.is_resyncing(),\n                in_fragmented_record,\n                \"LogReader::read_record: state after read_physical_record\"\n            );\n    \n            // Resync logic when starting from a non-zero initial offset.\n            if self.is_resyncing() {\n                if record_type == LogRecordType::Middle as u32 {\n                    trace!(\n                        \"LogReader::read_record: resyncing, skipping kMiddleType fragment\"\n                    );\n                    continue;\n                } else if record_type == LogRecordType::Last as u32 {\n                    trace!(\n                        \"LogReader::read_record: resyncing, skipping kLastType fragment and leaving resync mode\"\n                    );\n                    self.set_resyncing_flag(false);\n                    continue;\n                } else {\n                    trace!(\n                        \"LogReader::read_record: leaving resync mode at record_type={}\",\n                        record_type\n                    );\n                    self.set_resyncing_flag(false);\n                }\n            }\n    \n            if record_type == LogRecordType::Full as u32 {\n                // A complete record in a single physical entry.\n                if in_fragmented_record && !scratch.is_empty() {\n                    warn!(\n                        bytes = scratch.len(),\n                        \"LogReader::read_record: partial fragmented record without end (case 1)\"\n                    );\n                    self.report_corruption(\n                        scratch.len() as u64,\n                        PARTIAL_WITHOUT_END_1.as_ptr(),\n                    );\n                }\n    \n                prospective_record_offset = physical_record_offset;\n                scratch.clear();\n    \n                let data_ptr = *fragment.data();\n                let data_len = *fragment.size();\n    \n                if data_len == 0 || data_ptr.is_null() {\n                    record.clear();\n                } else {\n                    unsafe {\n                        *record = Slice::from_ptr_len(\n                            data_ptr,\n                            data_len,\n                        );\n                    }\n                }\n    \n                self.set_last_record_offset(prospective_record_offset);\n    \n                info!(\n                    last_record_offset = *self.last_record_offset(),\n                    record_len         = *record.size(),\n                    \"LogReader::read_record: returning Full record\"\n                );\n    \n                return true;\n            } else if record_type == LogRecordType::First as u32 {\n                // First fragment of a multi-fragment record.\n                if in_fragmented_record && !scratch.is_empty() {\n                    warn!(\n                        bytes = scratch.len(),\n                        \"LogReader::read_record: partial fragmented record without end (case 2)\"\n                    );\n                    self.report_corruption(\n                        scratch.len() as u64,\n                        PARTIAL_WITHOUT_END_2.as_ptr(),\n                    );\n                }\n    \n                prospective_record_offset = physical_record_offset;\n                scratch.clear();\n                append_slice_bytes(&fragment, scratch);\n                in_fragmented_record = true;\n    \n                trace!(\n                    prospective_record_offset,\n                    first_fragment_len = fragment_len,\n                    accumulated_bytes  = scratch.len(),\n                    \"LogReader::read_record: starting fragmented record (First fragment)\"\n                );\n            } else if record_type == LogRecordType::Middle as u32 {\n                // Middle fragment of a multi-fragment record.\n                if !in_fragmented_record {\n                    let dropped_bytes = fragment_len;\n                    warn!(\n                        fragment_len = dropped_bytes,\n                        \"LogReader::read_record: missing start of fragmented record (case 1)\"\n                    );\n                    self.report_corruption(\n                        dropped_bytes,\n                        MISSING_START_1.as_ptr(),\n                    );\n                } else {\n                    append_slice_bytes(&fragment, scratch);\n                    trace!(\n                        accumulated_bytes = scratch.len(),\n                        \"LogReader::read_record: appended Middle fragment\"\n                    );\n                }\n            } else if record_type == LogRecordType::Last as u32 {\n                // Final fragment of a multi-fragment record.\n                if !in_fragmented_record {\n                    let dropped_bytes = fragment_len;\n                    warn!(\n                        fragment_len = dropped_bytes,\n                        \"LogReader::read_record: missing start of fragmented record (case 2)\"\n                    );\n                    self.report_corruption(\n                        dropped_bytes,\n                        MISSING_START_2.as_ptr(),\n                    );\n                } else {\n                    append_slice_bytes(&fragment, scratch);\n    \n                    let total_len = scratch.len();\n                    if total_len == 0 {\n                        record.clear();\n                    } else {\n                        let record_ptr = scratch.as_ptr();\n                        unsafe {\n                            *record =\n                                Slice::from_ptr_len(record_ptr, total_len);\n                        }\n                    }\n    \n                    self.set_last_record_offset(prospective_record_offset);\n                    in_fragmented_record = false;\n    \n                    info!(\n                        last_record_offset = *self.last_record_offset(),\n                        record_len         = *record.size(),\n                        \"LogReader::read_record: returning assembled fragmented record (Last fragment)\"\n                    );\n    \n                    return true;\n                }\n            } else if record_type\n                == ExtendedRecordTypes::Eof.bits() as u32\n            {\n                // End of file.\n                if in_fragmented_record && !scratch.is_empty() {\n                    debug!(\n                        accumulated_bytes = scratch.len(),\n                        \"LogReader::read_record: EOF while in fragmented record; dropping partial record\"\n                    );\n                    scratch.clear();\n                }\n    \n                info!(\"LogReader::read_record: reached EOF\");\n                return false;\n            } else if record_type\n                == ExtendedRecordTypes::BadRecord.bits() as u32\n            {\n                // Corrupted physical record.\n                if in_fragmented_record && !scratch.is_empty() {\n                    warn!(\n                        accumulated_bytes = scratch.len(),\n                        \"LogReader::read_record: bad record while in fragmented record\"\n                    );\n                    self.report_corruption(\n                        scratch.len() as u64,\n                        ERROR_MIDDLE.as_ptr(),\n                    );\n                    in_fragmented_record = false;\n                    scratch.clear();\n                } else {\n                    debug!(\n                        \"LogReader::read_record: bad physical record outside fragmented record; continuing\"\n                    );\n                }\n            } else {\n                // Unknown record type.\n                let dropped_bytes =\n                    fragment_len\n                        + if in_fragmented_record {\n                            scratch.len() as u64\n                        } else {\n                            0\n                        };\n    \n                warn!(\n                    record_type,\n                    dropped_bytes,\n                    \"LogReader::read_record: unknown record type; treating as corruption\"\n                );\n    \n                self.report_corruption(\n                    dropped_bytes,\n                    UNKNOWN_RECORD_TYPE.as_ptr(),\n                );\n                in_fragmented_record = false;\n                scratch.clear();\n            }\n        }\n    }\n}\n\nimpl LogReader {\n\n    pub fn report_drop(&mut self, bytes: u64, reason: &Status) {\n        let buffer_size = (*self.buffer_slice().size()) as u64;\n    \n        let current_offset = self\n            .end_of_buffer_offset_value()\n            .saturating_sub(buffer_size)\n            .saturating_sub(bytes);\n    \n        trace!(\n            \"LogReader::report_drop: bytes={} end_of_buffer_offset={} buffer_size={} current_offset={} initial_offset={}\",\n            bytes,\n            self.end_of_buffer_offset_value(),\n            buffer_size,\n            current_offset,\n            self.initial_log_offset()\n        );\n    \n        if current_offset >= self.initial_log_offset() {\n            self.emit_corruption_to_reporter(bytes as usize, reason);\n        }\n    }\n}\n\nimpl LogReader {\n\n    pub fn read_into_buffer_from_file(&mut self, block_size: usize) -> Status {\n        let current_size = *self.buffer_slice().size();\n    \n        trace!(\n            \"LogReader::read_into_buffer_from_file: block_size={} current_buffer_size={}\",\n            block_size,\n            current_size\n        );\n    \n        // Clear the buffer and capture a raw pointer we can hand to the file implementation.\n        let result_ptr: *mut Slice = {\n            let buf_ref: &mut Slice = self.buffer_slice_mut();\n            buf_ref.clear();\n            buf_ref as *mut Slice\n        };\n    \n        let scratch_ptr: *mut u8 = self.backing_store_pointer() as *mut u8;\n    \n        let status = {\n            let file_ref: &mut Box<dyn SequentialFile> = self.file_mut();\n            file_ref.read(block_size, result_ptr, scratch_ptr)\n        };\n    \n        let new_size = *self.buffer_slice().size() as u64;\n        let new_end = self\n            .end_of_buffer_offset_value()\n            .saturating_add(new_size);\n    \n        self.set_end_of_buffer_offset_value(new_end);\n    \n        trace!(\n            \"LogReader::read_into_buffer_from_file: read {} bytes, end_of_buffer_offset={}\",\n            new_size,\n            self.end_of_buffer_offset_value()\n        );\n    \n        status\n    }\n}\n\nimpl LogReader {\n\n    pub fn buffer_slice(&self) -> &Slice {\n        &self.buffer\n    }\n\n    pub fn buffer_slice_mut(&mut self) -> &mut Slice {\n        &mut self.buffer\n    }\n\n    pub fn eof_flag(&self) -> bool {\n        self.eof\n    }\n\n    pub fn set_eof_flag(&mut self, eof: bool) {\n        debug!(\"LogReader::set_eof_flag: eof={}\", eof);\n        self.eof = eof;\n    }\n\n    pub fn end_of_buffer_offset_value(&self) -> u64 {\n        self.end_of_buffer_offset\n    }\n\n    pub fn set_end_of_buffer_offset_value(&mut self, offset: u64) {\n        debug!(\n            \"LogReader::set_end_of_buffer_offset_value: from {} to {}\",\n            self.end_of_buffer_offset,\n            offset\n        );\n        self.end_of_buffer_offset = offset;\n    }\n\n    pub fn initial_log_offset(&self) -> u64 {\n        self.initial_offset\n    }\n\n    pub fn is_resyncing(&self) -> bool {\n        self.resyncing\n    }\n\n    pub fn set_resyncing_flag(&mut self, resyncing: bool) {\n        debug!(\n            \"LogReader::set_resyncing_flag: resyncing={}\",\n            resyncing\n        );\n        self.resyncing = resyncing;\n    }\n\n    pub fn checksum_enabled(&self) -> bool {\n        self.checksum\n    }\n\n    pub fn backing_store_pointer(&self) -> *const u8 {\n        self.backing_store\n    }\n\n    pub fn skip_file_bytes(&mut self, n: u64) -> Status {\n        trace!(\"LogReader::skip_file_bytes: skipping {} bytes\", n);\n        self.file.skip(n)\n    }\n\n    pub fn emit_corruption_to_reporter(&mut self, bytes: usize, reason: &Status) {\n        trace!(\n            \"LogReader::emit_corruption_to_reporter: bytes={} reason={:?}\",\n            bytes,\n            reason\n        );\n        self.reporter.corruption(bytes, reason);\n    }\n}\n\nimpl Drop for LogReader {\n\n    fn drop(&mut self) {\n        trace!(\"LogReader::drop: releasing backing_store buffer\");\n        if !self.backing_store.is_null() {\n            unsafe {\n                let _ =\n                    Box::from_raw(self.backing_store as *mut [u8; LOG_BLOCK_SIZE as usize]);\n            }\n            self.backing_store = std::ptr::null();\n        }\n    }\n}\n\nimpl LogReader {\n\n    /// Skips all blocks that are completely before \"initial_offset_\".\n    ///\n    /// Returns true on success. Handles reporting.\n    pub fn skip_to_initial_block(&mut self) -> bool {\n        let block_size        = LOG_BLOCK_SIZE as u64;\n        let offset_in_block   = self.initial_log_offset() % block_size;\n        let mut block_start_location =\n            self.initial_log_offset() - offset_in_block;\n    \n        // Don't search a block if we'd be in the trailer\n        if offset_in_block > block_size - 6 {\n            block_start_location = block_start_location.saturating_add(block_size);\n        }\n    \n        self.set_end_of_buffer_offset_value(block_start_location);\n    \n        trace!(\n            \"LogReader::skip_to_initial_block: initial_offset={} offset_in_block={} block_start_location={}\",\n            self.initial_log_offset(),\n            offset_in_block,\n            block_start_location\n        );\n    \n        // Skip to start of first block that can contain the initial record\n        if block_start_location > 0 {\n            let status = self.skip_file_bytes(block_start_location);\n            if !status.is_ok() {\n                warn!(\n                    \"LogReader::skip_to_initial_block: skip({}) failed with status={:?}\",\n                    block_start_location,\n                    status\n                );\n    \n                // We attempted to position the underlying file at the first block\n                // that can contain the initial record and failed. Treat the bytes\n                // we tried to skip as dropped and report the corruption directly\n                // to the reporter, bypassing `report_drop`'s initial_offset gating.\n                self.emit_corruption_to_reporter(\n                    block_start_location as usize,\n                    &status,\n                );\n    \n                return false;\n            }\n        }\n    \n        true\n    }\n}\n\nimpl LogReader {\n\n    /// Reports dropped bytes to the reporter. buffer_ must be updated to remove\n    /// the dropped bytes prior to invocation.\n    /// \n    pub fn report_corruption(&mut self, bytes: u64, reason: *const u8) {\n        unsafe {\n            let reason_str = if reason.is_null() {\n                \"<unknown corruption>\"\n            } else {\n                match std::ffi::CStr::from_ptr(reason as *const libc::c_char)\n                    .to_str()\n                {\n                    Ok(s) => s,\n                    Err(_) => \"<non-utf8 corruption reason>\",\n                }\n            };\n    \n            warn!(\n                \"LogReader::report_corruption: bytes={} reason={}\",\n                bytes, reason_str\n            );\n    \n            let reason_bytes = reason_str.as_bytes();\n            let reason_slice =\n                Slice::from_ptr_len(reason_bytes.as_ptr(), reason_bytes.len());\n    \n            let status = Status::corruption(&reason_slice, None);\n            self.report_drop(bytes, &status);\n        }\n    }\n}\n\nimpl LogReader {\n\n    /// Return type, or one of the preceding special values\n    /// \n    pub fn read_physical_record(&mut self, result: *mut Slice) -> u32 {\n        trace!(\"LogReader::read_physical_record: enter\");\n    \n        const BAD_LENGTH_REASON: &[u8]        = b\"bad record length\\0\";\n        const CHECKSUM_MISMATCH_REASON: &[u8] = b\"checksum mismatch\\0\";\n    \n        if result.is_null() {\n            error!(\"LogReader::read_physical_record: null result pointer\");\n            return ExtendedRecordTypes::BadRecord.bits() as u32;\n        }\n    \n        loop {\n            let header_size = LOG_HEADER_SIZE as usize;\n            let block_size  = LOG_BLOCK_SIZE as usize;\n    \n            // Ensure we have at least a full header in the buffer.\n            if *self.buffer_slice().size() < header_size {\n                if !self.eof_flag() {\n                    debug!(\n                        \"LogReader::read_physical_record: need {} bytes for header, have {}; reading new block\",\n                        header_size,\n                        self.buffer_slice().size()\n                    );\n    \n                    let status = self.read_into_buffer_from_file(block_size);\n    \n                    if !status.is_ok() {\n                        self.buffer_slice_mut().clear();\n                        self.report_drop(block_size as u64, &status);\n                        self.set_eof_flag(true);\n                        debug!(\n                            \"LogReader::read_physical_record: read error while refilling; treating as EOF\"\n                        );\n                        return ExtendedRecordTypes::Eof.bits() as u32;\n                    } else if *self.buffer_slice().size() < block_size {\n                        debug!(\n                            \"LogReader::read_physical_record: short read {} (< {}), setting EOF\",\n                            self.buffer_slice().size(),\n                            block_size\n                        );\n                        self.set_eof_flag(true);\n                    }\n    \n                    continue;\n                } else {\n                    debug!(\n                        \"LogReader::read_physical_record: buffer too small ({}) and EOF already set; returning EOF\",\n                        self.buffer_slice().size()\n                    );\n                    self.buffer_slice_mut().clear();\n                    return ExtendedRecordTypes::Eof.bits() as u32;\n                }\n            }\n    \n            // Parse header from current buffer start.\n            let header_ptr_ptr = self.buffer_slice().data();\n            if header_ptr_ptr.is_null() {\n                error!(\"LogReader::read_physical_record: buffer has null data pointer\");\n                self.buffer_slice_mut().clear();\n                return ExtendedRecordTypes::BadRecord.bits() as u32;\n            }\n    \n            let header_ptr = unsafe { *header_ptr_ptr };\n            if header_ptr.is_null() {\n                error!(\"LogReader::read_physical_record: buffer has null inner data pointer\");\n                self.buffer_slice_mut().clear();\n                return ExtendedRecordTypes::BadRecord.bits() as u32;\n            }\n    \n            let a                = unsafe { (*header_ptr.add(4)) as u32 & 0xff };\n            let b                = unsafe { (*header_ptr.add(5)) as u32 & 0xff };\n            let record_type_byte = unsafe { (*header_ptr.add(6)) as u8 };\n            let length: u32      = a | (b << 8);\n    \n            let buffer_size = *self.buffer_slice().size();\n    \n            if header_size + (length as usize) > buffer_size {\n                let drop_size = buffer_size;\n                debug!(\n                    \"LogReader::read_physical_record: bad record length {} (buffer_size={})\",\n                    length,\n                    drop_size\n                );\n                self.buffer_slice_mut().clear();\n                if !self.eof_flag() {\n                    self.report_corruption(\n                        drop_size as u64,\n                        BAD_LENGTH_REASON.as_ptr(),\n                    );\n                    return ExtendedRecordTypes::BadRecord.bits() as u32;\n                } else {\n                    // If the end of the file has been reached without reading |length| bytes\n                    // of payload, assume the writer died in the middle of writing the record.\n                    // Don't report a corruption.\n                    return ExtendedRecordTypes::Eof.bits() as u32;\n                }\n            }\n    \n            if record_type_byte == LogRecordType::Zero as u8 && length == 0 {\n                // Skip zero length record without reporting any drops since\n                // such records are produced by the mmap based writing code in\n                // env_posix.cc that preallocates file regions.\n                debug!(\n                    \"LogReader::read_physical_record: skipping zero-length preallocated record\"\n                );\n                self.buffer_slice_mut().clear();\n                return ExtendedRecordTypes::BadRecord.bits() as u32;\n            }\n    \n            if self.checksum_enabled() {\n                let expected_crc = unsafe {\n                    let encoded = decode_fixed32(header_ptr);\n                    crc32c_unmask(encoded)\n                };\n                let actual_crc = unsafe {\n                    crc32c_value(\n                        header_ptr.add(6),\n                        1 + length as usize,\n                    )\n                };\n                if actual_crc != expected_crc {\n                    let drop_size = *self.buffer_slice().size();\n                    self.buffer_slice_mut().clear();\n                    self.report_corruption(\n                        drop_size as u64,\n                        CHECKSUM_MISMATCH_REASON.as_ptr(),\n                    );\n                    return ExtendedRecordTypes::BadRecord.bits() as u32;\n                }\n            }\n    \n            // Compute the physical offset of this record's header BEFORE we mutate the buffer.\n            let physical_record_offset = self\n                .end_of_buffer_offset_value()\n                .saturating_sub(buffer_size as u64);\n    \n            // If this record starts before the initial offset, skip it without returning payload.\n            if physical_record_offset < self.initial_log_offset() {\n                unsafe {\n                    (*result).clear();\n                }\n                trace!(\n                    \"LogReader::read_physical_record: skipping record at {} before initial_offset {}\",\n                    physical_record_offset,\n                    self.initial_log_offset()\n                );\n                self.buffer_slice_mut()\n                    .remove_prefix(header_size + length as usize);\n                return ExtendedRecordTypes::BadRecord.bits() as u32;\n            }\n    \n            // Expose the payload for this record.\n            unsafe {\n                *result = Slice::from_ptr_len(\n                    header_ptr.add(header_size),\n                    length as usize,\n                );\n            }\n    \n            // Now advance the buffer past this physical record.\n            self.buffer_slice_mut()\n                .remove_prefix(header_size + length as usize);\n    \n            trace!(\n                \"LogReader::read_physical_record: read record type={} length={}\",\n                record_type_byte,\n                length\n            );\n    \n            return record_type_byte as u32;\n        }\n    }\n}\n\n\nThis crate has the following authors:\n[\n    \"YourName <you@example.com>\",\n]\nThis crate has the following rust edition:\n2024\nThis crate has the following license:\nMIT\n","role":"user"}],"model":"gpt-5.1"}}
