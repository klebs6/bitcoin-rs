{"custom_id":"bitcoin-random-ai-readme-request","method":"POST","url":"/v1/chat/completions","body":{"max_completion_tokens":8192,"messages":[{"content":"We would like you to write a README.md for a rust crate with maximally helpful content.\nAdditionally, we would like you to generate several fields from the Cargo.toml package section for this crate.\nWe will show you the strict json output format we would like you to provide.\nPlease tailor your response for an apex consumer of maximal intelligence.\nA reader of this README.md should have a good sense of what the crate does and how to use it after reading what you write.\nPlease include a messaage indicating that this README.md file was generated by an AI model and may not be 100% accurate however it should be pretty good.\n\n\nCarefully read and parse the information we sent you.\n\nWe want to generate all information items requested by our schema:\n\n- Read the description of each field. Use it, along with the content we sent you to generate an appropriate value for each.\n- Ensure that each generated field optimally serves its desired purpose.\n- Each item should be deep, detailed, and specific. Use optimally descriptive and useful language. Do not be too verbose.\n- Do not add any extra keys to the generated object. Generate precisely what we ask of you: no more, no less.\n- For fields which are of an enum type: pick *exactly one variant*. Do not generate any information for the unselected variants.\n- For array fields: supply a JSON list [ ... ] of items, each conforming to the correct item type.\n- For map fields: Provide a JSON object { \"key_as_string\": <value>, ... }. Keys must be valid JSON strings. Keys and Values must conform to the key and value schemas we provide to you. Fill all subfields of each and every map value.\n- If we ask you to justify a map field, provide your justification/confidence score for the entire map. Do not attempt to separately justify each map element.\n- Likewise, if we ask you to justify an array field, provide your justification/confidence score for the entire array. Do not attempt to separately justify each element.\n- Provide numeric fields as real JSON numbers (not strings).\n- For string fields: provide the value precisely as a string literal. If optional, you may set it to null.\n- If we ask you to justify or provide confidence scores for the fields of a structure, please do so carefully. For each field we ask you to justify, please explain or justify your choice clearly. Confidence scores must be specified as real JSON numbers in the range `[0..1]`.\n- If we ask to you generate an optional field, either fill it or set it to null.\n- If we ask you to justify your choice of an enum variant, please explain your choice clearly. Do not justify your decision to leave a variant unselected. Provide your justification as a JSON string. Provide your confidence score as a real JSON number in the range `[0..1]`.\n- In terms of the JSON format we would like you to provide, the object itself will be deserialied based on the following schema. \n- We begin exactly at the top: the data should not be nested under a parent named 'fields' or anything similar.\n\n## Here is the schema to use:\n\nObject {\n    \"fields\": Object {\n        \"crate_name\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be the name of our crate, *verbatim*, trimmed for use as a toml field\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"full_readme_markdown\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be valid markdown representing the readme for this crate.\\\\n\\\\n Please make it terse, useful, and designed for an industrious commercially savy consumer of industrious nature. \\\\n\\\\n Advanced vocabulary and concepts are welcome. \\\\n\\\\n If this crate involves concepts from mathematics and/or physics, please describe and document them here.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_categories\": Object {\n            \"generation_instructions\": String(\"\\\"These categories will be used in our Cargo.toml file.\\\\n\\\\n It is important that users of this crate can find it on crates.io.\\\\n\\\\n Choose maximum 5 categories and make sure they are actually meaningful for our crate.\\\\n\\\\n These categories should only be chosen from among the legal crates.io categories. \\\\n\\\\n The ONLY legal categoreis are:\\\\n\\\\n``\\\\n accessibility\\\\n aerospace\\\\n algorithms\\\\n api-bindings\\\\n asynchronous\\\\n authentication\\\\n caching\\\\n command-line-interface\\\\n command-line-utilities\\\\n compilers\\\\n compression\\\\n computer-vision\\\\n concurrency\\\\n config\\\\n cryptography\\\\n data-structures\\\\n database\\\\n database-implementations\\\\n date-and-time\\\\n development-tools\\\\n email\\\\n embedded\\\\n emulators\\\\n encoding\\\\n external-ffi-bindings\\\\n filesystem\\\\n finance\\\\n game-development\\\\n game-engines\\\\n games\\\\n graphics\\\\n gui\\\\n hardware-support\\\\n internationalization\\\\n localization\\\\n mathematics\\\\n memory-management\\\\n multimedia\\\\n network-programming\\\\n no-std\\\\n os\\\\n parser-implementations\\\\n parsing\\\\n rendering\\\\n rust-patterns\\\\n science\\\\n simulation\\\\n template-engine\\\\n text-editors\\\\n text-processing\\\\n value-formatting\\\\n virtualization\\\\n visualization\\\\n wasm\\\\n web-programming\\\\n ``\\\\n\\\\n DO NOT specify a category that is not on this list.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n        \"package_description\": Object {\n            \"generation_instructions\": String(\"\\\"We will place this package_description in our Cargo.toml file so the users of this crate\\\\n know what it does.\\\\n\\\\n The discription should be technical, clear, and useful. It should be terse.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_keywords\": Object {\n            \"generation_instructions\": String(\"\\\"These keywords will be used in our Cargo.toml file so the users of this crate can easily\\\\n find it in the package system. Keywords should be useful, varied, meaningful, and reflect\\\\n the contents of our crate. They should make it easy to find. Please provide exactly five.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n    },\n    \"struct_docs\": String(\" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\"),\n    \"struct_name\": String(\"AiReadmeWriterDesiredOutput\"),\n    \"type\": String(\"struct\"),\n}\n\n## Please keep the following in mind:\n\nEntry phrasing:\n\n- Ensure entries are concise and focused.\n- Ensure there are no vague phrases (e.g., avoid starting with \"Illustrates the something something...\"; instead, use direct details like \"The hanging green vines on the garden wall\").\n\nEnsure Clarity and Consistency:\n\n- Use clear, grammatically correct sentences.\n- Maintain a consistent tone and style throughout.\n\nEnrich Content:\n\n- Communicate additional information where appropriate to enhance depth and value.\n- If a mathematical, physical, or otherwise technical background is helpful, please provide it.\n- Ensure that the content is comprehensive and meets all requirements.\n\nFocus Language:\n\n- Use deliberate and precise language.\n- Avoid vague verbs and keep the maximally intelligent and detail oriented audience in mind. \n- We explicitly ban the words `hush`, `subtle`, `mythic`, and `aura`. Please keep your vocabulary choices rigorous.\n- Do not use modern cultural references \n- Do not generically reference ideas which do not belong in the global environment.\n\n## Present the Final JSON:\n\n- Format the output as a JSON object.\n- Include all fields properly named from the schema we sent you.\n\nEnsure Proper Formatting:\n\n- Use proper JSON syntax with keys and arrays.\n- Ensure that all entries are correctly placed in their respective places.\n\nProvide only the final JSON data as specified by the schema. \n\nDo not include any schema generation instructions, meta-fields, nested-templates (unless directly asked) or extraneous information. \n\nProvide only the populated fields and their values, conforming exactly to the required structure without embedding any intermediate annotations or placeholders.\n\nYour output should only consist of the JSON object. do *not* include a preamble or postamble to your response. \n\nWe seek to successfully parse your response directly as JSON.\n\nFor the final answer, output the result only as a valid JSON object, wrapped inside triple backticks with json immediately after the opening backticks.\nThe JSON must be parser-ready:\n– No additional text before or after the block\n– No markdown or prose outside the fenced block\n– No HTML escaping of quotes, slashes, or other characters\n– Indentation and whitespace preserved exactly as written\n– The content between the fences must be valid JSON as per RFC 8259","role":"system"},{"content":"We have a rust crate named 'bitcoin-random' 0.1.21. \n\nHere is the interface:\npub enum RNGLevel {\n    FAST,\n    SLOW,\n    PERIODIC,\n}\n\npub trait RandRange {\n    fn randrange(&mut self, range: u64) -> u64;\n}\n\n/**\n| Compatibility with the C++11\n| \n| UniformRandomBitGenerator concept\n|\n*/\npub type FastRandomContextResultType = u64;\n\nlazy_static!{\n    pub static ref GET_RAND_MICROS: Duration = get_random_duration(Duration::microseconds(i64::MAX));\n    pub static ref GET_RAND_MILLIS: Duration = get_random_duration(Duration::milliseconds(i64::MAX));\n}\n\n\n#[cfg(have_getcpuid)]\n#[cfg(have_getcpuid)]\nlazy_static!{\n    pub static ref G_RDRAND_SUPPORTED: AtomicBool = AtomicBool::new(false);\n    pub static ref G_RDSEED_SUPPORTED: AtomicBool = AtomicBool::new(false);\n}\n\n\n#[cfg(have_getcpuid)]\n#[cfg(bit_RDRND)]\n#[cfg(have_getcpuid)]\n#[cfg(bit_RDRND)]\nconst_assert!{\n    CPUID_F1_ECX_RDRAND == bit_RDRND, \n    \"Unexpected value for bit_RDRND\"\n}\n\n\n#[cfg(have_getcpuid)]\n#[cfg(bit_RDSEED)]\n#[cfg(have_getcpuid)]\n#[cfg(bit_RDSEED)]\nconst_assert!{\n    CPUID_F7_EBX_RDSEED == bit_RDSEED, \n    \"Unexpected value for bit_RDSEED\"\n}\n\n\nlazy_static!{\n    pub static ref G_MOCK_DETERMINISTIC_TESTS: AtomicBool = AtomicBool::new(false);\n}\n\n\nlazy_static!{\n    pub static ref G_RNG: Arc<Mutex<Box<RNGState,SecureAllocator>>> \n        = Arc::new(\n            Mutex::new(\n                Box::new_in(RNGState::default(), SECURE_ALLOCATOR.clone())\n            )\n        );\n}\n\n/**\n| Fast randomness source. This is seeded\n| once with secure random data, but is\n| completely deterministic and does\n| not gather more entropy after that.\n| \n| This class is not thread-safe.\n|\n*/\n#[derive(Debug,Getters,MutGetters,Setters)]\n#[getset(get=\"pub\",set=\"pub\",get_mut=\"pub\")]\npub struct FastRandomContext  {\n    requires_seed: bool,\n    rng:           ChaCha20,\n    bytebuf:       [u8; 64],\n    bytebuf_size:  i32,\n    bitbuf:        u64,\n    bitbuf_size:   i32,\n}\n\n\n/**\n| The RNG state consists of 256 bits of\n| entropy, taken from the output of one\n| operation's SHA512 output, and fed\n| as input to the next one.\n| \n| Carrying 256 bits of entropy should\n| be sufficient to guarantee unpredictability\n| as long as any entropy source was ever\n| unpredictable to an attacker. To protect\n| against situations where an attacker\n| might observe the RNG's state, fresh\n| entropy is always mixed when\n| GetStrongRandBytes is called.\n|\n*/\npub struct RNGInnerState  {\n    state:           [u8; 32],\n    counter:         u64,\n    strongly_seeded: bool,\n}\n\n\n#[derive(Default)]\npub struct RNGStateEvents  {\n    hasher: Sha256,\n}\n\n\npub struct RNGState  {\n    inner:  Mutex<RNGInnerState>,\n    events: Mutex<RNGStateEvents>,\n}\n\n/**\n| Gather non-cryptographic environment\n| data that changes over time.\n|\n*/\npub fn rand_add_dynamic_env(hasher: &mut Sha512) {\n    // --- Windows: perfmon + FILETIME ---------------------------------------\n    #[cfg(WIN32)]\n    {\n        // Matches: RandAddSeedPerfmon(hasher);\n        rand_add_seed_perfmon(hasher);\n\n        // Matches: FILETIME ftime; GetSystemTimeAsFileTime(&ftime); hasher << ftime;\n        unsafe {\n            use winapi::um::{minwinbase::FILETIME, sysinfoapi::GetSystemTimeAsFileTime};\n            let mut ftime: FILETIME = core::mem::zeroed();\n            GetSystemTimeAsFileTime(&mut ftime);\n            hasher.write(\n                &ftime as *const _ as *const u8,\n                core::mem::size_of::<FILETIME>(),\n            );\n        }\n    }\n\n    // --- POSIX: various clocks (timespecs) + gettimeofday -------------------\n    #[cfg(not(WIN32))]\n    unsafe {\n        // struct timespec ts = {};\n        let mut ts: libc::timespec = core::mem::zeroed();\n\n        // #ifdef CLOCK_MONOTONIC\n        #[cfg(any(\n            target_os = \"linux\", target_os = \"android\",\n            target_os = \"freebsd\", target_os = \"netbsd\",\n            target_os = \"openbsd\", target_os = \"dragonfly\", target_os = \"macos\"\n        ))]\n        if libc::clock_gettime(libc::CLOCK_MONOTONIC, &mut ts) == 0 {\n            hasher.write(\n                &ts as *const _ as *const u8,\n                core::mem::size_of::<libc::timespec>(),\n            );\n        }\n\n        // #ifdef CLOCK_REALTIME\n        if libc::clock_gettime(libc::CLOCK_REALTIME, &mut ts) == 0 {\n            hasher.write(\n                &ts as *const _ as *const u8,\n                core::mem::size_of::<libc::timespec>(),\n            );\n        }\n\n        // #ifdef CLOCK_BOOTTIME  (Linux/Android)\n        #[cfg(any(target_os = \"linux\", target_os = \"android\"))]\n        if libc::clock_gettime(libc::CLOCK_BOOTTIME, &mut ts) == 0 {\n            hasher.write(\n                &ts as *const _ as *const u8,\n                core::mem::size_of::<libc::timespec>(),\n            );\n        }\n\n        // // gettimeofday is available on all UNIX systems (microsecond precision).\n        let mut tv: libc::timeval = core::mem::zeroed();\n        libc::gettimeofday(&mut tv, core::ptr::null_mut());\n        hasher.write(\n            &tv as *const _ as *const u8,\n            core::mem::size_of::<libc::timeval>(),\n        );\n    }\n\n    // --- C++11 clocks: write three counts in this exact order ---------------\n    //     system_clock::now().time_since_epoch().count();\n    //     steady_clock::now().time_since_epoch().count();\n    //     high_resolution_clock::now().time_since_epoch().count();\n    //\n    // We serialize them as 64-bit integers, matching the common libstdc++\n    // representation (nanoseconds) on POSIX. On Windows, we derive a\n    // monotonic-ish counter similarly to Core’s intent (value content is\n    // entropy; exact epoch isn’t relied upon).\n    {\n        // system_clock count\n        let sys_ns: i64 = {\n            #[cfg(not(WIN32))]\n            unsafe {\n                let mut ts: libc::timespec = core::mem::zeroed();\n                if libc::clock_gettime(libc::CLOCK_REALTIME, &mut ts) == 0 {\n                    let ns = (ts.tv_sec as i128) * 1_000_000_000i128 + (ts.tv_nsec as i128);\n                    ns as i64\n                } else {\n                    0\n                }\n            }\n            #[cfg(WIN32)]\n            {\n                // FILETIME is in 100ns ticks since 1601-01-01; multiply to ns.\n                use winapi::um::{minwinbase::FILETIME, sysinfoapi::GetSystemTimeAsFileTime};\n                let mut ft: FILETIME = unsafe { core::mem::zeroed() };\n                unsafe { GetSystemTimeAsFileTime(&mut ft) };\n                let v100ns = ((ft.dwHighDateTime as u64) << 32) | (ft.dwLowDateTime as u64);\n                (v100ns.saturating_mul(100) as i64)\n            }\n        };\n        hasher.write(&sys_ns as *const _ as *const u8, core::mem::size_of_val(&sys_ns));\n\n        // steady_clock count\n        #[cfg(not(WIN32))]\n        let steady_ns: i64 = unsafe {\n            let mut ts: libc::timespec = core::mem::zeroed();\n            if libc::clock_gettime(libc::CLOCK_MONOTONIC, &mut ts) == 0 {\n                let ns = (ts.tv_sec as i128) * 1_000_000_000i128 + (ts.tv_nsec as i128);\n                ns as i64\n            } else {\n                0\n            }\n        };\n        #[cfg(WIN32)]\n        let steady_ns: i64 = {\n            // Use our high‑res counter fallback; Core treats this as entropy.\n            get_performance_counter() as i64\n        };\n        hasher.write(\n            &steady_ns as *const _ as *const u8,\n            core::mem::size_of_val(&steady_ns),\n        );\n\n        // high_resolution_clock count (libstdc++ typically aliases steady_clock)\n        let high_ns: i64 = steady_ns;\n        hasher.write(\n            &high_ns as *const _ as *const u8,\n            core::mem::size_of_val(&high_ns),\n        );\n    }\n\n    // --- POSIX: current resource usage --------------------------------------\n    #[cfg(not(WIN32))]\n    unsafe {\n        let mut usage: libc::rusage = core::mem::zeroed();\n        if libc::getrusage(libc::RUSAGE_SELF, &mut usage) == 0 {\n            hasher.write(\n                &usage as *const _ as *const u8,\n                core::mem::size_of::<libc::rusage>(),\n            );\n        }\n    }\n\n    // --- Linux: a handful of small /proc snapshots --------------------------\n    #[cfg(all(not(WIN32), target_os = \"linux\"))]\n    {\n        use std::ffi::CString;\n        for p in [\n            \"/proc/diskstats\",\n            \"/proc/vmstat\",\n            \"/proc/schedstat\",\n            \"/proc/zoneinfo\",\n            \"/proc/meminfo\",\n            \"/proc/softirqs\",\n            \"/proc/stat\",\n            \"/proc/self/schedstat\",\n            \"/proc/self/status\",\n        ] {\n            if let Ok(cstr) = CString::new(p) {\n                add_file(hasher, cstr.as_ptr());\n            }\n        }\n    }\n\n    // --- Stack and heap location (exact pointer bytes, order preserved) -----\n    unsafe {\n        // c_void* addr = malloc(4097);\n        let addr = libc::malloc(4097);\n\n        // hasher << &addr\n        let pptr = &addr as *const *mut libc::c_void;\n        hasher.write(pptr as *const u8, core::mem::size_of::<*mut libc::c_void>());\n\n        // hasher << addr\n        hasher.write(\n            &addr as *const _ as *const u8,\n            core::mem::size_of::<*mut libc::c_void>(),\n        );\n\n        // free(addr);\n        libc::free(addr);\n    }\n}\n\n\n/**\n| Generate a uniform random duration\n| in the range [0..max). Precondition:\n| max.count() > 0\n|\n*/\npub fn get_random_duration(max: Duration) -> Duration {\n    let ticks = max.as_seconds_f64();\n    let rand  = get_rand(ticks as u64) as f64;\n\n    Duration::seconds_f64(rand)\n}\n\n\npub fn rand_failure() {\n    log_printf!(\"Failed to read randomness, aborting\\n\");\n    std::intrinsics::abort();\n}\n\n\npub fn proc_rand(out: &mut [u8], num: i32, level: RNGLevel) {\n    // Make sure the RNG is initialized first\n    // (as all Seed* function possibly need\n    // hwrand to be available).\n    let rng: &mut RNGState = &mut G_RNG.lock();\n\n    assert!{num <= 32};\n\n    let mut hasher = Sha512::default();\n\n    match level {\n        RNGLevel::FAST => {\n            seed_fast(&mut hasher);\n        }\n        RNGLevel::SLOW => {\n            seed_slow(&mut hasher, rng);\n        }\n        RNGLevel::PERIODIC => {\n            seed_periodic(&mut hasher, rng);\n        }\n    }\n\n    let num: usize = num.try_into().unwrap();\n\n    // Combine with and update state\n    if !rng.mix_extract(out, num, hasher, false) {\n\n        // On the first invocation, also seed with SeedStartup().\n        let mut startup_hasher = Sha512::default();\n        seed_startup(&mut startup_hasher, rng);\n        rng.mix_extract(out, num, startup_hasher, true);\n    }\n}\n\n\n/**\n| Seed with the entire set of perfmon data\n|\n*/\n#[cfg(WIN32)]\npub fn rand_add_seed_perfmon(hasher: &mut Sha512) {\n    /*\n       | This can take up to 2 seconds, so only\n       | do it every 10 minutes.\n       |\n       | Initialize last_perfmon to 0 seconds,\n       | we don't skip the first call.\n       */\n    lazy_static!{\n        static ref last_perfmon: Mutex<Duration> = Mutex::new(Duration::ZERO);\n    }\n\n    let last_time:    Duration = *last_perfmon.lock().unwrap();\n    let current_time: Duration = get_time();\n\n    if current_time < last_time + TEN_MINUTES {\n        return;\n    }\n\n    *last_perfmon.lock().unwrap() = current_time;\n\n    //-------------------------------------\n\n    let v_data: Vec<u8> = Vec::with_capacity(250000);\n\n    let ret:    i64 = 0;\n    let n_size: u64 = 0;\n\n    // Bail out at more than 10MB of performance data\n    const n_max_size: usize = 10000000; \n\n    while true {\n\n        n_size = v_data.len().try_into().unwrap();\n\n        ret = winreg::RegQueryValueExA(\n            winreg::HKEY_PERFORMANCE_DATA, \n            \"Global\", \n            null_mut(), \n            null_mut(), \n            v_data.as_mut_ptr(), \n            &n_size);\n\n        if ret != winerror::ERROR_MORE_DATA || v_data.len() >= n_max_size {\n            break;\n        }\n\n        // Grow size of buffer exponentially\n        v_data.resize(\n            std::cmp::min((v_data.len() * 3) / 2, n_max_size),\n            0\n        ); \n    }\n\n    winreg::RegCloseKey(winreg::HKEY_PERFORMANCE_DATA);\n\n    if ret == winerror::ERROR_SUCCESS {\n\n        hasher.write(\n            v_data.as_mut_ptr(), \n            n_size.try_into().unwrap());\n\n        memory_cleanse(\n            v_data.as_mut_ptr() as *mut c_void, \n            n_size.try_into().unwrap());\n\n    } else {\n\n        // Performance data is only a best-effort\n        // attempt at improving the situation when\n        // the OS randomness (and other sources)\n        // aren't adequate. As a result, failure\n        // to read it is isn't considered\n        // critical, so we don't call\n        // RandFailure().\n        //\n        // TODO: Add logging when the logger is\n        // made functional before global\n        // constructors have been invoked.\n    }\n}\n\n\n#[cfg(not(WIN32))]\npub fn add_sockaddr(hasher: &mut Sha512, addr: *const nix::sys::socket::sockaddr) {\n    if addr == null_mut() {\n        return;\n    }\n\n    unsafe {\n        match (*addr).sa_family {\n\n            AF_INET  => hasher.write(\n                addr as *mut u8, size_of::<libc::sockaddr_in>()\n            ),\n\n            AF_INET6 => hasher.write(\n                addr as *mut u8, size_of::<libc::sockaddr_in6>()\n            ),\n\n            _ => hasher.write(\n                &(*addr).sa_family as *const _ as *const u8, \n                size_of_val(&(*addr).sa_family)\n            )\n        };\n    }\n}\n\n\n#[cfg(not(WIN32))]\npub fn add_file(hasher: &mut Sha512, path: *const i8) {\n    let mut sb: libc::stat = unsafe { std::mem::zeroed() };\n\n    let f: i32 = unsafe { libc::open(path, libc::O_RDONLY) };\n\n    let mut total: usize = 0;\n\n    if f != -1 {\n\n        let mut fbuf: [u8; 4096] = [0; 4096];\n\n        let mut n: i32 = 0;\n\n        hasher.write(\n            &f as *const _ as *const u8, \n            size_of_val(&f)\n        );\n\n        if unsafe { libc::fstat(f, &mut sb) } == 0 {\n            hasher.feed_data_in(&mut sb);\n        }\n\n        loop {\n\n            n = unsafe {\n\n                libc::read(\n                    f, \n                    fbuf.as_mut_ptr() as *mut c_void, \n                    size_of_val(&fbuf)\n                ).try_into().unwrap()\n            };\n\n            if n > 0 {\n                hasher.write(fbuf.as_mut_ptr(), n.try_into().unwrap());\n            }\n\n            let offset: usize = n.try_into().unwrap();\n\n            total += offset;\n\n            /* not bothering with EINTR handling. */\n\n            /*\n              | Read only the first 1 Mbyte\n              |\n              */\n            let onward: bool = \n                n == (size_of_val(&fbuf) as i32) && total < 1048576; \n\n            if !onward {\n                break;\n            }\n        } \n\n        unsafe {\n            libc::close(f);\n        }\n    }\n}\n\n\n#[cfg(not(WIN32))]\npub fn add_path(hasher: &mut Sha512, path: *const u8) {\n    let mut sb: libc::stat = unsafe { std::mem::zeroed() };\n\n    if unsafe { libc::stat(path as *const i8, &mut sb) } == 0 {\n\n        let len = unsafe { libc::strlen(path as *const i8) + 1 };\n\n        hasher.write(path, len);\n        hasher.feed_data_in(&mut sb);\n    }\n}\n\n\n#[cfg(HAVE_SYSCTL)]\npub fn add_sysctl<const S: i32>(hasher: &mut Sha512) {\n    todo!();\n    /*\n    int CTL[sizeof...(S)] = {S...};\n    unsigned char buffer[65536];\n    size_t siz = 65536;\n    int ret = sysctl(CTL, sizeof...(S), buffer, &siz, nullptr, 0);\n    if (ret == 0 || (ret == -1 && errno == ENOMEM)) {\n        hasher << sizeof(CTL);\n        hasher.Write((const unsigned char*)CTL, sizeof(CTL));\n        if (siz > sizeof(buffer)) siz = sizeof(buffer);\n        hasher << siz;\n        hasher.Write(buffer, siz);\n    }\n    */\n}\n\n\n#[cfg(have_getcpuid)]\n#[inline]\npub fn addcpuid(\n    hasher:  &mut Sha512,\n    leaf:    u32,\n    subleaf: u32,\n    ax:      &mut u32,\n    bx:      &mut u32,\n    cx:      &mut u32,\n    dx:      &mut u32\n)\n{\n    getcpuid(leaf, subleaf, ax, bx, cx, dx);\n\n    *hasher <<= leaf;\n    *hasher <<= subleaf;\n    *hasher <<= ax;\n    *hasher <<= bx;\n    *hasher <<= cx;\n    *hasher <<= dx;\n}\n\n\n#[cfg(have_getcpuid)]\npub fn add_allcpuid(hasher: &mut Sha512) {\n    let mut ax: u32 = 0;\n    let mut bx: u32 = 0;\n    let mut cx: u32 = 0;\n    let mut dx: u32 = 0;\n\n    /*\n      | Iterate over all standard leaves\n      |\n      */\n    addcpuid(\n        hasher, \n        0, \n        0, \n        &mut ax, \n        &mut bx, \n        &mut cx, \n        &mut dx\n    ); // Returns max leaf in ax\n\n    let max: u32 = ax;\n\n    for leaf in 1..=std::cmp::min(max,0xFF) {\n\n        let mut maxsub: u32 = 0;\n\n        for subleaf in 0..=0xFF {\n\n            addcpuid(\n                hasher, \n                leaf, \n                subleaf, \n                &mut ax, \n                &mut bx, \n                &mut cx, \n                &mut dx);\n\n            /*\n              | Iterate subleafs for leaf values 4,\n              | 7, 11, 13\n              |\n              */\n            if leaf == 4 {\n\n                if (ax & 0x1f) == 0 {\n                    break;\n                }\n\n            } else if leaf == 7 {\n\n                if subleaf == 0 {\n                    maxsub = ax;\n                }\n\n                if subleaf == maxsub {\n                    break;\n                }\n\n            } else if leaf == 11 {\n\n                if (cx & 0xff00) == 0 {\n                    break;\n                }\n\n            } else if leaf == 13 {\n\n                if ax == 0 \n                    && bx == 0 \n                    && cx == 0 \n                    && dx == 0 \n                {\n                    break;\n                }\n\n            } else {\n\n                /*\n                  | For any other leaf, stop after subleaf\n                  | 0.\n                  |\n                  */\n                break;\n            }\n        }\n    }\n\n    // Iterate over all extended leaves\n    addcpuid(hasher, \n        0x80000000, \n        0, \n        &mut ax, \n        &mut bx, \n        &mut cx, \n        &mut dx); // Returns max extended leaf in ax\n\n    let ext_max: u32 = ax;\n\n    for leaf in 0x80000001..=std::cmp::min(ext_max,0x800000FF) {\n\n        addcpuid(\n            hasher, \n            leaf, \n            0, \n            &mut ax, \n            &mut bx, \n            &mut cx, \n            &mut dx\n        );\n    }\n}\n\n\npub fn rand_add_static_env(hasher: &mut Sha512) {\n    // --- Some compile-time static properties (exact order) ------------------\n    // hasher << (CHAR_MIN < 0)\n    // Serialize as a single byte 0/1 to match C++ bool layout.\n    let char_signed: u8 = if ((-1i8) as libc::c_char) < (0 as libc::c_char) { 1 } else { 0 };\n    unsafe { hasher.write(&char_signed as *const u8, core::mem::size_of::<u8>()) };\n\n    // hasher << sizeof(c_void*) << sizeof(long) << sizeof(int)\n    let sz_ptr  = core::mem::size_of::<*const libc::c_void>() as usize;\n    let sz_long = core::mem::size_of::<libc::c_long>() as usize;\n    let sz_int  = core::mem::size_of::<libc::c_int>() as usize;\n    unsafe {\n        hasher.write(&sz_ptr  as *const _ as *const u8, core::mem::size_of::<usize>());\n        hasher.write(&sz_long as *const _ as *const u8, core::mem::size_of::<usize>());\n        hasher.write(&sz_int  as *const _ as *const u8, core::mem::size_of::<usize>());\n    }\n\n    // --- Compiler/version macros block (skipped unless you expose them) -----\n    // C++ does:\n    //   hasher << __GNUC__ << __GNUC_MINOR__ << __GNUC_PATCHLEVEL__;\n    //   hasher << _MSC_VER; hasher << __cplusplus; hasher << _XOPEN_VERSION;\n    //   Write __VERSION__ string + NUL.\n    // If you surface equivalents via your cfg layer, write them here exactly\n    // as integers and a NUL-terminated string, in this order.\n    /*\n    #if defined(__GNUC__) && defined(__GNUC_MINOR__) && defined(__GNUC_PATCHLEVEL__)\n        hasher << __GNUC__ << __GNUC_MINOR__ << __GNUC_PATCHLEVEL__;\n    #endif\n    #ifdef _MSC_VER\n        hasher << _MSC_VER;\n    #endif\n        hasher << __cplusplus;\n    #ifdef _XOPEN_VERSION\n        hasher << _XOPEN_VERSION;\n    #endif\n    #ifdef __VERSION__\n        const char* COMPILER_VERSION = __VERSION__;\n        hasher.Write((const unsigned char*)COMPILER_VERSION, strlen(COMPILER_VERSION) + 1);\n    #endif\n    */\n\n    // --- Bitcoin client version (optional, only if you export it) -----------\n    // If your cfg layer exposes CLIENT_VERSION (i32/u32), write it raw here.\n    #[cfg(any())] // replace with an appropriate cfg if you export the symbol\n    unsafe {\n        extern \"C\" { static CLIENT_VERSION: libc::c_int; }\n        let v: libc::c_int = CLIENT_VERSION;\n        hasher.write(&v as *const _ as *const u8, core::mem::size_of::<libc::c_int>());\n    }\n\n    // --- getauxval() (Linux/Android) ---------------------------------------\n    #[cfg(any(target_os = \"linux\", target_os = \"android\"))]\n    unsafe {\n        // AT_HWCAP / AT_HWCAP2\n        let hwcap  = libc::getauxval(libc::AT_HWCAP);\n        let hwcap2 = libc::getauxval(libc::AT_HWCAP2);\n        hasher.write(&hwcap  as *const _ as *const u8, core::mem::size_of::<libc::c_ulong>());\n        hasher.write(&hwcap2 as *const _ as *const u8, core::mem::size_of::<libc::c_ulong>());\n\n        // AT_RANDOM (16 bytes)\n        let random_aux = libc::getauxval(libc::AT_RANDOM) as *const u8;\n        if !random_aux.is_null() {\n            hasher.write(random_aux, 16);\n        }\n\n        // AT_PLATFORM (string + NUL)\n        let platform = libc::getauxval(libc::AT_PLATFORM) as *const libc::c_char;\n        if !platform.is_null() {\n            let n = libc::strlen(platform) + 1;\n            hasher.write(platform as *const u8, n);\n        }\n\n        // AT_EXECFN (string + NUL)\n        let execfn = libc::getauxval(libc::AT_EXECFN) as *const libc::c_char;\n        if !execfn.is_null() {\n            let n = libc::strlen(execfn) + 1;\n            hasher.write(execfn as *const u8, n);\n        }\n    }\n\n    // --- CPU features via CPUID (exactly like Core) -------------------------\n    #[cfg(have_getcpuid)]\n    {\n        add_allcpuid(hasher);\n    }\n\n    // --- Memory locations (order preserved) ---------------------------------\n    unsafe {\n        // &hasher\n        let p_hasher = hasher as *mut Sha512;\n        hasher.write(\n            &p_hasher as *const _ as *const u8,\n            core::mem::size_of::<*mut Sha512>(),\n        );\n\n        // &RandAddStaticEnv (function pointer)\n        let fptr = rand_add_static_env as usize;\n        hasher.write(&fptr as *const _ as *const u8, core::mem::size_of::<usize>());\n\n        // &malloc (function pointer)\n        let mptr = libc::malloc as usize;\n        hasher.write(&mptr as *const _ as *const u8, core::mem::size_of::<usize>());\n\n        // &errno  -> pointer to the TLS errno int\n        #[cfg(any(target_os = \"linux\", target_os = \"android\"))]\n        {\n            let errno_ptr = libc::__errno_location();\n            hasher.write(\n                &errno_ptr as *const _ as *const u8,\n                core::mem::size_of::<*mut libc::c_int>(),\n            );\n        }\n        #[cfg(any(\n            target_os = \"macos\",\n            target_os = \"ios\",\n            target_os = \"freebsd\",\n            target_os = \"openbsd\",\n            target_os = \"netbsd\",\n            target_os = \"dragonfly\"\n        ))]\n        {\n            let errno_ptr = libc::__error();\n            hasher.write(\n                &errno_ptr as *const _ as *const u8,\n                core::mem::size_of::<*mut libc::c_int>(),\n            );\n        }\n\n        // &environ (address of the global)\n        #[cfg(not(WIN32))]\n        {\n            extern \"C\" {\n                static mut environ: *mut *mut libc::c_char;\n            }\n            let penv = &environ as *const _; // address of the global symbol\n            hasher.write(\n                &penv as *const _ as *const u8,\n                core::mem::size_of::<*const *mut *mut libc::c_char>(),\n            );\n        }\n    }\n\n    // --- Hostname -----------------------------------------------------------\n    #[cfg(not(WIN32))]\n    unsafe {\n        let mut hname = [0u8; 256];\n        if libc::gethostname(hname.as_mut_ptr() as *mut libc::c_char, hname.len()) == 0 {\n            // strnlen up to 256\n            let n = libc::strnlen(hname.as_ptr() as *const libc::c_char, hname.len());\n            hasher.write(hname.as_ptr(), n);\n        }\n    }\n\n    // --- Network interfaces -------------------------------------------------\n    #[cfg(any(\n        target_os = \"linux\",\n        target_os = \"android\",\n        target_os = \"macos\",\n        target_os = \"freebsd\",\n        target_os = \"openbsd\",\n        target_os = \"netbsd\",\n        target_os = \"dragonfly\"\n    ))]\n    unsafe {\n        let mut ifap: *mut libc::ifaddrs = core::ptr::null_mut();\n        if libc::getifaddrs(&mut ifap) == 0 {\n            let mut it = ifap;\n            while !it.is_null() {\n                // hasher.Write((unsigned char*)&ifit, sizeof(ifit))  -> pointer value\n                hasher.write(\n                    &it as *const _ as *const u8,\n                    core::mem::size_of::<*mut libc::ifaddrs>(),\n                );\n\n                // hasher.Write(ifa_name, strlen+1)\n                let name = (*it).ifa_name;\n                if !name.is_null() {\n                    let n = libc::strlen(name) + 1;\n                    hasher.write(name as *const u8, n);\n                }\n\n                // hasher.Write(&ifa_flags, sizeof(ifa_flags))\n                hasher.write(\n                    &(*it).ifa_flags as *const _ as *const u8,\n                    core::mem::size_of_val(&(*it).ifa_flags),\n                );\n\n                // AddSockaddr(hasher, ifa_addr/netmask/dstaddr)\n                add_sockaddr(hasher, (*it).ifa_addr as *const nix::sys::socket::sockaddr);\n                add_sockaddr(hasher, (*it).ifa_netmask as *const nix::sys::socket::sockaddr);\n                add_sockaddr(hasher, (*it).ifa_dstaddr as *const nix::sys::socket::sockaddr);\n\n                it = (*it).ifa_next;\n            }\n            libc::freeifaddrs(ifap);\n        }\n    }\n\n    // --- UNIX kernel information + basic paths/files ------------------------\n    #[cfg(not(WIN32))]\n    unsafe {\n        // uname\n        let mut name: libc::utsname = core::mem::zeroed();\n        if libc::uname(&mut name) != -1 {\n            // Write strings + NUL, in exact order\n            let sysname  = name.sysname.as_ptr();\n            let nodename = name.nodename.as_ptr();\n            let release  = name.release.as_ptr();\n            let version  = name.version.as_ptr();\n            let machine  = name.machine.as_ptr();\n\n            let mut w = |p: *const libc::c_char| {\n                if !p.is_null() {\n                    let n = libc::strlen(p) + 1;\n                    hasher.write(p as *const u8, n);\n                }\n            };\n            w(sysname);\n            w(nodename);\n            w(release);\n            w(version);\n            w(machine);\n        }\n\n        // AddPath (exact list/order)\n        add_path(hasher, b\"/\\0\".as_ptr());\n        add_path(hasher, b\".\\0\".as_ptr());\n        add_path(hasher, b\"/tmp\\0\".as_ptr());\n        add_path(hasher, b\"/home\\0\".as_ptr());\n        add_path(hasher, b\"/proc\\0\".as_ptr());\n\n        // Linux /proc files (exact list/order)\n        #[cfg(any(target_os = \"linux\", target_os = \"android\"))]\n        {\n            use std::ffi::CString;\n            for p in [\"/proc/cmdline\", \"/proc/cpuinfo\", \"/proc/version\"] {\n                if let Ok(cs) = CString::new(p) {\n                    add_file(hasher, cs.as_ptr());\n                }\n            }\n        }\n\n        // Common/etc files (exact list/order)\n        {\n            use std::ffi::CString;\n            for p in [\n                \"/etc/passwd\",\n                \"/etc/group\",\n                \"/etc/hosts\",\n                \"/etc/resolv.conf\",\n                \"/etc/timezone\",\n                \"/etc/localtime\",\n            ] {\n                if let Ok(cs) = CString::new(p) {\n                    add_file(hasher, cs.as_ptr());\n                }\n            }\n        }\n    }\n\n    // --- sysctl blocks for Mac/BSDs (defer until add_sysctl is implemented) -\n    // See your commented C++ list. Add them back here once add_sysctl<T...>()\n    // is available, preserving the same order.\n    /*\n    // For MacOS/BSDs, gather data through sysctl instead of /proc. Not all of these\n    // will exist on every system.\n    #if HAVE_SYSCTL\n    #  ifdef CTL_HW\n    #    ifdef HW_MACHINE\n        AddSysctl<CTL_HW, HW_MACHINE>(hasher);\n    #    endif\n    #    ifdef HW_MODEL\n        AddSysctl<CTL_HW, HW_MODEL>(hasher);\n    #    endif\n    #    ifdef HW_NCPU\n        AddSysctl<CTL_HW, HW_NCPU>(hasher);\n    #    endif\n    #    ifdef HW_PHYSMEM\n        AddSysctl<CTL_HW, HW_PHYSMEM>(hasher);\n    #    endif\n    #    ifdef HW_USERMEM\n        AddSysctl<CTL_HW, HW_USERMEM>(hasher);\n    #    endif\n    #    ifdef HW_MACHINE_ARCH\n        AddSysctl<CTL_HW, HW_MACHINE_ARCH>(hasher);\n    #    endif\n    #    ifdef HW_REALMEM\n        AddSysctl<CTL_HW, HW_REALMEM>(hasher);\n    #    endif\n    #    ifdef HW_CPU_FREQ\n        AddSysctl<CTL_HW, HW_CPU_FREQ>(hasher);\n    #    endif\n    #    ifdef HW_BUS_FREQ\n        AddSysctl<CTL_HW, HW_BUS_FREQ>(hasher);\n    #    endif\n    #    ifdef HW_CACHELINE\n        AddSysctl<CTL_HW, HW_CACHELINE>(hasher);\n    #    endif\n    #  endif\n    #  ifdef CTL_KERN\n    #    ifdef KERN_BOOTFILE\n         AddSysctl<CTL_KERN, KERN_BOOTFILE>(hasher);\n    #    endif\n    #    ifdef KERN_BOOTTIME\n         AddSysctl<CTL_KERN, KERN_BOOTTIME>(hasher);\n    #    endif\n    #    ifdef KERN_CLOCKRATE\n         AddSysctl<CTL_KERN, KERN_CLOCKRATE>(hasher);\n    #    endif\n    #    ifdef KERN_HOSTID\n         AddSysctl<CTL_KERN, KERN_HOSTID>(hasher);\n    #    endif\n    #    ifdef KERN_HOSTUUID\n         AddSysctl<CTL_KERN, KERN_HOSTUUID>(hasher);\n    #    endif\n    #    ifdef KERN_HOSTNAME\n         AddSysctl<CTL_KERN, KERN_HOSTNAME>(hasher);\n    #    endif\n    #    ifdef KERN_OSRELDATE\n         AddSysctl<CTL_KERN, KERN_OSRELDATE>(hasher);\n    #    endif\n    #    ifdef KERN_OSRELEASE\n         AddSysctl<CTL_KERN, KERN_OSRELEASE>(hasher);\n    #    endif\n    #    ifdef KERN_OSREV\n         AddSysctl<CTL_KERN, KERN_OSREV>(hasher);\n    #    endif\n    #    ifdef KERN_OSTYPE\n         AddSysctl<CTL_KERN, KERN_OSTYPE>(hasher);\n    #    endif\n    #    ifdef KERN_POSIX1\n         AddSysctl<CTL_KERN, KERN_OSREV>(hasher);\n    #    endif\n    #    ifdef KERN_VERSION\n         AddSysctl<CTL_KERN, KERN_VERSION>(hasher);\n    #    endif\n    #  endif\n    #endif\n    */\n\n    // --- Environment variables ---------------------------------------------\n    #[cfg(not(WIN32))]\n    unsafe {\n        extern \"C\" { static mut environ: *mut *mut libc::c_char; }\n        let mut p = environ;\n        if !p.is_null() {\n            loop {\n                let q = *p;\n                if q.is_null() { break; }\n                let n = libc::strlen(q);\n                hasher.write(q as *const u8, n); // no terminating NUL (matches Core)\n                p = p.add(1);\n            }\n        }\n    }\n\n    // --- Process/thread/user/session/group IDs ------------------------------\n    #[cfg(WIN32)]\n    unsafe {\n        use winapi::um::processthreadsapi::{GetCurrentProcessId, GetCurrentThreadId};\n        let pid = GetCurrentProcessId();\n        let tid = GetCurrentThreadId();\n        hasher.write(&pid as *const _ as *const u8, core::mem::size_of_val(&pid));\n        hasher.write(&tid as *const _ as *const u8, core::mem::size_of_val(&tid));\n    }\n\n    #[cfg(not(WIN32))]\n    unsafe {\n        // getpid, getppid, getsid(0), getpgid(0), getuid, geteuid, getgid, getegid\n        let pid   = libc::getpid();\n        let ppid  = libc::getppid();\n        let sid   = libc::getsid(0);\n        let pgid  = libc::getpgid(0);\n        let uid   = libc::getuid();\n        let euid  = libc::geteuid();\n        let gid   = libc::getgid();\n        let egid  = libc::getegid();\n        hasher.write(&pid  as *const _ as *const u8, core::mem::size_of_val(&pid));\n        hasher.write(&ppid as *const _ as *const u8, core::mem::size_of_val(&ppid));\n        hasher.write(&sid  as *const _ as *const u8, core::mem::size_of_val(&sid));\n        hasher.write(&pgid as *const _ as *const u8, core::mem::size_of_val(&pgid));\n        hasher.write(&uid  as *const _ as *const u8, core::mem::size_of_val(&uid));\n        hasher.write(&euid as *const _ as *const u8, core::mem::size_of_val(&euid));\n        hasher.write(&gid  as *const _ as *const u8, core::mem::size_of_val(&gid));\n        hasher.write(&egid as *const _ as *const u8, core::mem::size_of_val(&egid));\n\n        // std::this_thread::get_id() equivalent: pthread_self() raw bytes\n        let tid = libc::pthread_self();\n        hasher.write(&tid as *const _ as *const u8, core::mem::size_of_val(&tid));\n    }\n}\n\n\n/**\n| Generate random data via the internal\n| PRNG.\n| \n| These functions are designed to be fast\n| (sub microsecond), but do not necessarily\n| meaningfully add entropy to the PRNG\n| state.\n| Thread-safe.\n|\n*/\npub fn get_rand_bytes(buf: &mut [u8], num: i32) {\n    proc_rand(buf, num, RNGLevel::FAST);\n}\n\n\n/**\n| Gather entropy from various sources,\n| feed it into the internal PRNG, and generate\n| random data using it.\n| \n| This function will cause failure whenever\n| the OS RNG fails.\n| Thread-safe.\n|\n*/\npub fn get_strong_rand_bytes(buf: &mut [u8], num: i32) {\n    proc_rand(buf, num, RNGLevel::SLOW);\n}\n\n\n/**\n| Generate a uniform random integer in\n| the range [0..range). Precondition:\n| range > 0\n|\n*/\npub fn get_rand(n_max: u64) -> u64 {\n    let deterministic = G_MOCK_DETERMINISTIC_TESTS.load(atomic::Ordering::Relaxed);\n    let mut ctx = FastRandomContext::new(deterministic);\n    ctx.randrange(n_max)\n}\n\n\npub fn get_rand_int(n_max: i32) -> i32 {\n    let n_max: u64 = n_max.try_into().unwrap();\n\n    let result: u64 = get_rand(n_max);\n\n    result.try_into().unwrap()\n}\n\n\npub fn get_rand_hash() -> u256 {\n    let mut hash: u256 = u256::default();\n\n    get_rand_bytes(\n        unsafe { std::slice::from_raw_parts_mut(&mut hash as *mut _ as *mut u8, hash.byte_len()) }, \n        size_of_val(&hash).try_into().unwrap()\n    );\n\n    hash\n}\n\n\n/**\n| Add 64 bits of entropy gathered from\n| hardware to hasher. Do nothing if not\n| supported.\n|\n*/\n#[cfg(any(target_arch = \"x86\", target_arch = \"x86_64\"))]\npub fn seed_hardware_fast(hasher: &mut Sha512) {\n    if G_RDRAND_SUPPORTED.load(atomic::Ordering::Relaxed) {\n\n        let out: u64 = get_rd_rand();\n\n        hasher.write(&out as *const _ as *const u8, size_of_val(&out));\n    }\n}\n\n\n#[cfg(not(any(target_arch = \"x86\", target_arch = \"x86_64\")))]\npub fn seed_hardware_fast(_hasher: &mut Sha512) {}\n\n\n/**\n| Add 256 bits of entropy gathered from\n| hardware to hasher. Do nothing if not\n| supported.\n|\n*/\n#[cfg(any(target_arch = \"x86\", target_arch = \"x86_64\"))]\npub fn seed_hardware_slow(hasher: &mut Sha512) {\n    // When we want 256 bits of entropy,\n    // prefer RdSeed over RdRand, as it's\n    // guaranteed to produce independent\n    // randomness on every call.\n    //\n    if G_RDSEED_SUPPORTED.load(atomic::Ordering::Relaxed) {\n\n        for i in 0..4 {\n\n            let out: u64 = get_rd_seed();\n\n            hasher.write(&out as *const _ as *const u8, size_of_val(&out));\n        }\n\n        return;\n    }\n\n    // When falling back to RdRand, XOR the\n    // result of 1024 results.\n    //\n    // This guarantees a reseeding occurs\n    // between each.\n    if G_RDRAND_SUPPORTED.load(atomic::Ordering::Relaxed) {\n\n        for i in 0..4 {\n\n            let mut out: u64 = 0;\n\n            for j in 0..1024 {\n                out ^= get_rd_rand();\n            }\n\n            hasher.write(&out as *const _ as *const u8, size_of_val(&out));\n        }\n\n        return;\n    }\n}\n\n\n#[cfg(not(any(target_arch = \"x86\", target_arch = \"x86_64\")))]\npub fn seed_hardware_slow(_hasher: &mut Sha512) {}\n\n\n/**\n| Check that OS randomness is available\n| and returning the requested number\n| of bytes.\n|\n*/\npub fn random_sanity_check() -> bool {\n    let mut start: u64 = get_performance_counter().try_into().unwrap();\n\n    /*\n      | This does not measure the quality of\n      | randomness, but it does test that\n      | GetOSRand() overwrites all 32 bytes\n      | of the output given a maximum number\n      | of tries.\n      |\n      */\n    const MAX_TRIES: i32 = 1024;\n\n    let mut data: [u8; NUM_OS_RANDOM_BYTES as usize] = [0; NUM_OS_RANDOM_BYTES as usize];\n\n    /*\n      | Tracks which bytes have been overwritten\n      | at least once\n      |\n      */\n    let mut overwritten: [bool; NUM_OS_RANDOM_BYTES as usize] = [false; NUM_OS_RANDOM_BYTES as usize];\n\n    let mut num_overwritten: i32 = 0;\n    let mut tries:           i32 = 0;\n\n    /*\n      | Loop until all bytes have been overwritten\n      | at least once, or max number tries reached\n      |\n      */\n    loop {\n\n        unsafe {\n            libc::memset(\n                data.as_mut_ptr() as *mut c_void, \n                0, \n                NUM_OS_RANDOM_BYTES as usize\n            );\n        }\n\n        get_os_rand(data.as_mut_ptr());\n\n        for x in 0..NUM_OS_RANDOM_BYTES {\n            overwritten[x as usize] |= data[x as usize] != 0;\n        }\n\n        num_overwritten = 0;\n\n        for x in 0..NUM_OS_RANDOM_BYTES {\n            if overwritten[x as usize] {\n                num_overwritten += 1;\n            }\n        }\n\n        tries += 1;\n\n        let onward: bool = num_overwritten < NUM_OS_RANDOM_BYTES && tries < MAX_TRIES;\n\n        if !onward {\n            break;\n        }\n    } \n\n    if num_overwritten != NUM_OS_RANDOM_BYTES {\n\n        /*\n          | If this failed, bailed out after too\n          | many tries\n          |\n          */\n        return false; \n    }\n\n    /*\n      | Check that GetPerformanceCounter\n      | increases at least during a GetOSRand()\n      | call + 1ms sleep.\n      |\n      */\n    std::thread::sleep(ONE_MILLISECOND.try_into().unwrap());\n\n    let mut stop: u64 = get_performance_counter().try_into().unwrap();\n\n    if stop == start {\n        return false;\n    }\n\n    /*\n      | We called GetPerformanceCounter.\n      | Use it as entropy.\n      |\n      */\n    let mut to_add: Sha512 = Sha512::default();\n\n    to_add.write(&mut start as *mut u64 as *mut u8, size_of_val(&start));\n    to_add.write(&mut stop  as *mut u64 as *mut u8, size_of_val(&stop));\n\n    G_RNG.lock().mix_extract(&mut [], 0, to_add, false);\n\n    true\n}\n\n\n/**\n| Fallback: get 32 bytes of system entropy\n| from /dev/urandom. The most compatible\n| way to get cryptographic randomness\n| on UNIX-ish platforms.\n|\n*/\n#[cfg(not_windows)]\npub fn get_dev_urandom(ent32: *mut u8) {\n    let f: i32 = unsafe {\n        libc::open(b\"/dev/urandom\\0\".as_ptr() as *const i8, libc::O_RDONLY)\n    };\n\n    if f == -1 {\n        rand_failure();\n    }\n\n    let mut have: isize = 0;\n\n    loop {\n\n        let count: usize = \n            (NUM_OS_RANDOM_BYTES - (have as i32)).try_into().unwrap();\n\n        let n: isize = unsafe {\n            libc::read(\n                f, \n                ent32.offset(have) as *mut c_void, \n                count)\n        };\n\n        if n <= 0 || (n + have) as i32 > NUM_OS_RANDOM_BYTES {\n\n            unsafe {\n                libc::close(f);\n            }\n\n            rand_failure();\n        }\n\n        have += n;\n\n        if have as i32 >= NUM_OS_RANDOM_BYTES {\n            break;\n        }\n    }\n\n    unsafe {\n        libc::close(f);\n    }\n}\n\n\n/**\n| Get 32 bytes of system entropy. Do not\n| use this in application code: use\n| \n| GetStrongRandBytes instead.\n|\n*/\n#[cfg(windows)]\npub fn get_os_rand(ent32: *mut u8) {\n    let hProvider = winapi::wincrypt::HCRYPTPROV::default();\n\n    let ret: i32 = CryptAcquireContextW(\n        &hProvider, \n        null_mut(), \n        null_mut(), \n        PROV_RSA_FULL, \n        CRYPT_VERIFYCONTEXT\n    );\n\n    if ret == 0 {\n        rand_failure();\n    }\n\n    ret = crypt_gen_random(\n        hProvider, \n        NUM_OS_RANDOM_BYTES, \n        ent32);\n\n    if ret == 0 {\n        rand_failure();\n    }\n\n    crypt_release_context(hProvider, 0);\n}\n\n\n#[cfg(HAVE_SYS_GETRANDOM)]\npub fn get_os_rand(ent32: *mut u8) {\n    /*\n       | Linux. From the getrandom(2) man page:\n       | \"If the urandom source has been initialized,\n       | reads of up to 256 bytes will always return\n       | as many bytes as requested and will not\n       | be interrupted by signals.\"\n       |\n       */\n    let rv: i32 = libc::syscall(\n        libc::SYS_getrandom, \n        ent32, \n        NUM_OS_RANDOM_BYTES, \n        0);\n\n    if rv != NUM_OS_RANDOM_BYTES {\n\n        if rv < 0 && libc::errno() == libc::ENOSYS {\n\n            /*\n              | Fallback for kernel <3.17: the return\n              | value will be -1 and errno ENOSYS if the\n              | syscall is not available, in that case\n              | fall back to /dev/urandom.\n              |\n              */\n            get_dev_urandom(ent32);\n\n        } else {\n            rand_failure();\n        }\n    }\n}\n\n\n#[cfg(all(HAVE_GETENTROPY,openbsd))]\npub fn get_os_rand(ent32: *mut u8) {\n    /*\n      | On OpenBSD this can return up to 256 bytes\n      | of entropy, will return an error if more\n      | are requested.\n      | \n      | The call cannot return less than the\n      | requested number of bytes. getentropy\n      | is explicitly limited to openbsd here,\n      | as a similar (but not the same) function\n      | may exist on other platforms via glibc.\n      |\n      */\n    if libc::getentropy(ent32, NUM_OS_RANDOM_BYTES) != 0 {\n        rand_failure();\n    }\n}\n\n\n#[cfg(all(HAVE_GETENTROPY_RAND,MAC_OSX))]\npub fn get_os_rand(ent32: *mut u8) {\n    /*\n      | libc::getentropy() is available on macOS\n      | 10.12 and later.\n      |\n      */\n    if libc::getentropy(ent32, NUM_OS_RANDOM_BYTES) != 0 {\n        rand_failure();\n    }\n}\n\n\n#[cfg(HAVE_SYSCTL_ARND)]\npub fn get_os_rand(ent32: *mut u8) {\n    /*\n      | FreeBSD, NetBSD and similar. It is possible\n      | for the call to return less bytes than\n      | requested, so need to read in a loop.\n      |\n      */\n    static mut name: [i32; 2] = [libc::CTL_KERN, libc::KERN_ARND];\n\n    let have: i32 = 0;\n\n    loop {\n\n        let len: usize = (NUM_OS_RANDOM_BYTES - have).try_into().unwrap();\n\n        if libc::sysctl(\n            name.as_mut_ptr(), \n            std::size(name), \n            ent32.offset(have.try_into().unwrap()), \n            &mut len, \n            null_mut(), \n            0) != 0 \n        {\n            rand_failure();\n        }\n\n        have += len as i32;\n\n        if have >= NUM_OS_RANDOM_BYTES {\n            break;\n        }\n    }\n}\n\n\n/**\n| Fall back to /dev/urandom if there is\n| no specific method implemented to get\n| system entropy for this OS.\n|\n*/\n#[cfg(not(any(    windows,    HAVE_SYS_GETRANDOM,    all(HAVE_GETENTROPY,openbsd),    all(HAVE_GETENTROPY_RAND,MAC_OSX),    HAVE_SYSCTL_ARND,)))]\npub fn get_os_rand(ent32: *mut u8) {\n    get_dev_urandom(ent32);\n}\n\n\n///--------------------------------------\n#[cfg(have_getcpuid)]\npub fn init_hardware_rand() {\n    let mut eax: u32 = 0;\n    let mut ebx: u32 = 0;\n    let mut ecx: u32 = 0;\n    let mut edx: u32 = 0;\n\n    getcpuid(1, 0, &mut eax, &mut ebx, &mut ecx, &mut edx);\n\n    if (ecx & CPUID_F1_ECX_RDRAND) != 0 {\n        G_RDRAND_SUPPORTED.store(true, atomic::Ordering::Relaxed);\n    }\n\n    getcpuid(7, 0, &mut eax, &mut ebx, &mut ecx, &mut edx);\n\n    if (ebx & CPUID_F7_EBX_RDSEED) != 0 {\n        G_RDSEED_SUPPORTED.store(true, atomic::Ordering::Relaxed);\n    }\n}\n\n\n/**\n| Access to other hardware random number\n| generators could be added here later,\n| assuming it is sufficiently fast (in\n| the order of a few hundred CPU cycles).\n| \n| Slower sources should probably be invoked\n| separately, and/or only from\n| RandAddPeriodic (which is called once\n| a minute).\n|\n*/\n#[cfg(not(have_getcpuid))]\npub fn init_hardware_rand() {\n}\n\n\n///-----------------------------------\n#[cfg(have_getcpuid)]\npub fn report_hardware_rand() {\n    /*\n      | This must be done in a separate function,\n      | as InitHardwareRand() may be indirectly\n      | called from global constructors, before\n      | logging is initialized.\n      */\n    if G_RDSEED_SUPPORTED.load(atomic::Ordering::Relaxed) {\n        log_printf!(\"Using RdSeed as additional entropy source\\n\");\n    }\n\n    if G_RDRAND_SUPPORTED.load(atomic::Ordering::Relaxed) {\n        log_printf!(\"Using RdRand as an additional entropy source\\n\");\n    }\n}\n\n\n#[cfg(not(have_getcpuid))]\npub fn report_hardware_rand() {\n}\n\n\n/**\n| Initialize global RNG state and log\n| any CPU features that are used.\n| \n| Calling this function is optional.\n| RNG state will be initialized when first\n| needed if it is not called.\n|\n*/\npub fn random_init() {\n    /*\n      | Invoke RNG code to trigger initialization\n      | (if not already performed)\n      |\n      */\n    proc_rand(&mut [], 0, RNGLevel::FAST);\n\n    report_hardware_rand();\n}\n\n\n/**\n| Gather entropy from various expensive\n| sources, and feed them to the PRNG state.\n| \n| Thread-safe.\n|\n*/\npub fn rand_add_periodic() {\n    proc_rand(&mut [], 0, RNGLevel::PERIODIC);\n}\n\n\n/**\n| Gathers entropy from the low bits of\n| the time at which events occur. Should\n| be called with a uint32_t describing\n| the event at the time an event occurs.\n| \n| Thread-safe.\n|\n*/\npub fn rand_add_event(event_info: u32) {\n    G_RNG.lock().add_event(event_info);\n}\n\n\n/**\n| Read 64 bits of entropy using rdseed.\n| \n| Must only be called when RdSeed is supported.\n|\n*/\n#[cfg(have_getcpuid)]\npub fn get_rd_seed() -> u64 {\n    // RdSeed may fail when the HW RNG is overloaded. \n    //\n    // Loop indefinitely until enough entropy is gathered, but pause after every failure.\n    //\n    #[cfg(target_arch = \"x86\")]\n    {\n        let mut ok: u8 = 0;\n        let mut r1: u32 = 0;\n        let mut r2: u32 = 0;\n\n        loop {\n            unsafe {\n                // rdseed %eax\n                core::arch::asm!(\".byte 0x0f, 0xc7, 0xf8; setc {ok}\",\n                                  out(\"eax\") r1, ok = out(reg_byte) ok,\n                                  options(nostack, preserves_flags));\n            }\n            if ok != 0 { break; }\n            unsafe { core::arch::asm!(\"pause\", options(nomem, nostack, preserves_flags)); }\n        }\n        loop {\n            unsafe {\n                // rdseed %eax\n                core::arch::asm!(\".byte 0x0f, 0xc7, 0xf8; setc {ok}\",\n                                  out(\"eax\") r2, ok = out(reg_byte) ok,\n                                  options(nostack, preserves_flags));\n            }\n            if ok != 0 { break; }\n            unsafe { core::arch::asm!(\"pause\", options(nomem, nostack, preserves_flags)); }\n        }\n        return ((r2 as u64) << 32) | (r1 as u64);\n    }\n\n    #[cfg(target_arch = \"x86_64\")]\n    {\n        let mut ok: u8 = 0;\n        let mut r: u64 = 0;\n        loop {\n            unsafe {\n                // rdseed %rax\n                core::arch::asm!(\".byte 0x48, 0x0f, 0xc7, 0xf8; setc {ok}\",\n                                  out(\"rax\") r, ok = out(reg_byte) ok,\n                                  options(nostack, preserves_flags));\n            }\n            if ok != 0 { break; }\n            unsafe { core::arch::asm!(\"pause\", options(nomem, nostack, preserves_flags)); }\n        }\n        return r;\n    }\n\n    #[cfg(not(any(target_arch = \"x86\", target_arch = \"x86_64\")))]\n    {\n        panic!(\"RdSeed is only supported on x86 and x86_64\");\n    }\n}\n\n\n/**\n| Read 64 bits of entropy using rdrand.\n| \n| Must only be called when RdRand is supported.\n|\n*/\n#[cfg(have_getcpuid)]\npub fn get_rd_rand() -> u64 {\n    // 32-bit x86: stitch two 32-bit RDRAND results\n    #[cfg(target_arch = \"x86\")]\n    {\n        // RdRand may very rarely fail. Invoke it\n        // up to 10 times in a loop to reduce this\n        // risk.\n        let mut ok: u8 = 0;\n\n        // Initialize to 0 to silence a compiler\n        // warning that r1 or r2 may be used\n        // uninitialized. Even if rdrand fails\n        // (!ok) it will set the output to 0, but\n        // there is no way that the compiler could\n        // know that.\n        let mut r1: u32 = 0;\n        let mut r2: u32 = 0;\n\n        for _ in 0..10 {\n            unsafe {\n                // rdrand %eax\n                core::arch::asm!(\".byte 0x0f, 0xc7, 0xf0; setc {ok}\",\n                                  out(\"eax\") r1, ok = out(reg_byte) ok,\n                                  options(nostack, preserves_flags));\n            }\n            if ok != 0 { break; }\n        }\n        for _ in 0..10 {\n            unsafe {\n                // rdrand %eax\n                core::arch::asm!(\".byte 0x0f, 0xc7, 0xf0; setc {ok}\",\n                                  out(\"eax\") r2, ok = out(reg_byte) ok,\n                                  options(nostack, preserves_flags));\n            }\n            if ok != 0 { break; }\n        }\n        return ((r2 as u64) << 32) | (r1 as u64);\n    }\n\n    // 64-bit x86_64: single 64-bit RDRAND\n    #[cfg(target_arch = \"x86_64\")]\n    {\n        let mut ok: u8 = 0;\n\n        // See above why we initialize to 0.\n        let mut r: u64 = 0;\n\n        for _ in 0..10 {\n\n            unsafe {\n\n                // rdrand %rax\n                core::arch::asm!(\".byte 0x48, 0x0f, 0xc7, 0xf0; setc {ok}\",\n                                  out(\"rax\") r, ok = out(reg_byte) ok,\n                                  options(nostack, preserves_flags));\n            }\n\n            if ok != 0 { break; }\n        }\n        return r;\n    }\n\n    #[cfg(not(any(target_arch = \"x86\", target_arch = \"x86_64\")))]\n    {\n        panic!(\"RdRand is only supported on x86 and x86_64\");\n    }\n}\n\n\n/**\n| Use repeated SHA512 to strengthen the\n| randomness in seed32, and feed into\n| hasher.\n|\n*/\npub fn strengthen(seed: &[u8; 32], microseconds: i32, hasher: &mut Sha512) {\n    let microseconds: i64 = microseconds.into();\n\n    let mut inner_hasher: Sha512 = Sha512::default();\n\n    inner_hasher.write(seed.as_ptr() as *const u8, size_of_val(seed));\n\n    // Hash loop\n    let mut buffer: [u8; 64] = unsafe { std::mem::zeroed() };\n\n    let stop: Instant = get_time() + Duration::microseconds(microseconds);\n\n    loop {\n\n        for i in 0..1000 {\n            inner_hasher.finalize(&mut buffer);\n            inner_hasher.reset();\n            inner_hasher.write(buffer.as_mut_ptr() as *const u8, size_of_val(&buffer));\n        }\n\n        // Benchmark operation and feed it into outer hasher.\n        let perf: i64 = get_performance_counter();\n\n        hasher.write(&perf as *const _ as *const u8, size_of_val(&perf));\n\n        if get_time() >= stop {\n            break;\n        }\n    }\n\n    // Produce output from inner state and feed it to outer hasher.\n    inner_hasher.finalize(&mut buffer);\n    hasher.write(buffer.as_mut_ptr() as *const u8, size_of_val(&buffer));\n\n    // Try to clean up.\n    inner_hasher.reset();\n\n    memory_cleanse(buffer.as_mut_ptr() as *mut c_void, size_of_val(&buffer));\n}\n\n\n/**\n| More efficient than using std::shuffle\n| on a FastRandomContext.\n| \n| This is more efficient as std::shuffle\n| will consume entropy in groups of 64\n| bits at the time and throw away most.\n| This also works around a bug in libstdc++\n| std::shuffle that may cause type::operator=(type&&)\n| to be invoked on itself, which the library's\n| debug mode detects and panics on. This\n| is a known issue, see https://stackoverflow.com/questions/22915325/avoiding-self-assignment-in-stdshuffle\n|\n*/\npub fn shuffle<'a, I: 'a, R>(mut first: I, last: I, mut rng: R) where \nI: PartialEq,\nI: Copy,\nI: Sub<Output = I>,\nI: AddAssign<u64>,\nI: Add<Output = u64>,\nI: Deref<Target = &'a mut I>,\nI: DerefMut<Target = &'a mut I>,\nI: Add<u64>,\nR: RandRange,\n<I as Add<u64>>::Output: Deref<Target = &'a mut I>,\n<I as Add<u64>>::Output: DerefMut<Target = &'a mut I>,\n\nu64: From<I>\n{\n    while first != last {\n\n        let j: u64 = rng.randrange(u64::from(last - first));\n\n        if j != 0 {\n            let mut offset = first + j;\n            std::mem::swap(*first, *(offset));\n        }\n\n        first += 1;\n    }\n}\n\n\npub fn shuffle_all<'a, I: 'a, R>(mut first: I, mut rng: R) where \nI: PartialEq,\nI: Copy,\nI: Sub<Output = I>,\nI: AddAssign<u64>,\nI: Add<Output = u64>,\nI: Deref<Target = &'a mut I>,\nI: DerefMut<Target = &'a mut I>,\nI: Add<u64>,\nR: RandRange,\n<I as Add<u64>>::Output: Deref<Target = &'a mut I>,\n<I as Add<u64>>::Output: DerefMut<Target = &'a mut I>,\n\nu64: From<I>\n{\n    todo!();\n}\n\n\n/**\n| Read the hardware time stamp counter when\n| available.\n|\n| See\n| https://en.wikipedia.org/wiki/Time_Stamp_Counter\n| for more information.\n*/\n#[cfg(all(windows, any(target_arch = \"x86\", target_arch = \"x86_64\")))]\n#[inline]\npub fn get_performance_counter() -> i64 {\n    // Use RDTSC where available\n    unsafe {\n        #[cfg(target_arch = \"x86\")]   { core::arch::x86::_rdtsc() as i64 }\n        #[cfg(target_arch = \"x86_64\")]{ core::arch::x86_64::_rdtsc() as i64 }\n    }\n}\n\n\n#[cfg(all(not(windows), target_arch = \"x86\"))]\n#[inline]\npub fn get_performance_counter() -> i64 {\n    let mut r_lo: u32 = 0;\n    let mut r_hi: u32 = 0;\n    unsafe {\n        core::arch::asm!(\"rdtsc\", out(\"eax\") r_lo, out(\"edx\") r_hi, options(nostack, preserves_flags));\n    }\n    ((r_hi as u64) << 32 | (r_lo as u64)) as i64\n}\n\n\n#[cfg(all(not(windows), target_arch = \"x86_64\"))]\n#[inline]\npub fn get_performance_counter() -> i64 {\n    let mut r: u64 = 0;\n    unsafe {\n        // Constrain r1 to rax and r2 to rdx.\n        core::arch::asm!(\"rdtsc\", out(\"rax\") r, out(\"rdx\") _, options(nostack, preserves_flags));\n    }\n    r as i64\n}\n\n\n/**\n| Fall back to using C++11 clock (usually\n| microsecond or nanosecond precision)\n|\n*/\n#[cfg(not(any(    all(windows, any(target_arch = \"x86\", target_arch = \"x86_64\")),    all(not(windows), target_arch = \"x86\"),    all(not(windows), target_arch = \"x86_64\"),)))]\n#[inline]\npub fn get_performance_counter() -> i64 {\n    quanta::Instant::now().as_u64() as i64\n}\n\n\n/**\n| A note on the use of in the seeding functions\n| below:\n| \n| None of the RNG code should ever throw\n| any exception.\n|\n*/\npub fn seed_timestamp(hasher: &mut Sha512) {\n    let mut perfcounter: i64 = get_performance_counter();\n\n    hasher.write(\n        &mut perfcounter as *mut _ as *mut u8,  \n        size_of_val(&perfcounter)\n    );\n}\n\n\npub fn seed_fast(hasher: &mut Sha512) {\n    let mut buffer: [u8; 32] = [0; 32];\n\n    /*\n      | Stack pointer to indirectly commit\n      | to thread/callstack\n      |\n      */\n    let mut ptr = buffer.as_mut_ptr();\n\n    hasher.write(ptr as *mut u8, size_of_val(&ptr));\n\n    /*\n      | Hardware randomness is very fast when\n      | available; use it always.\n      |\n      */\n    seed_hardware_fast(hasher);\n\n    /*\n      | High-precision timestamp\n      |\n      */\n    seed_timestamp(hasher);\n}\n\n\npub fn seed_slow(hasher: &mut Sha512, rng: &mut RNGState) {\n    let mut buffer: [u8; 32] = [0; 32];\n\n    /*\n      | Everything that the 'fast' seeder includes\n      |\n      */\n    seed_fast(hasher);\n\n    /*\n      | OS randomness\n      |\n      */\n    get_os_rand(buffer.as_mut_ptr());\n\n    hasher.write(\n        buffer.as_mut_ptr(), \n        size_of_val(&buffer)\n    );\n\n    /*\n      | Add the events hasher into the mix\n      |\n      */\n    rng.seed_events(hasher);\n\n    /*\n      | High-precision timestamp.\n      |\n      | Note that we also commit to a timestamp in\n      | the Fast seeder, so we indirectly commit to\n      | a benchmark of all the entropy gathering\n      | sources in this function).\n      */\n    seed_timestamp(hasher);\n}\n\n\n/**\n| Extract entropy from rng, strengthen\n| it, and feed it into hasher.\n|\n*/\npub fn seed_strengthen(hasher: &mut Sha512, rng: &mut RNGState, microseconds: i32) {\n    /*\n      | Generate 32 bytes of entropy from the\n      | RNG, and a copy of the entropy already\n      | in hasher.\n      |\n      */\n    let mut strengthen_seed: [u8; 32] = [0; 32];\n\n    let len = strengthen_seed.len();\n\n    rng.mix_extract(\n        unsafe { std::slice::from_raw_parts_mut(strengthen_seed.as_mut_ptr(), len) }, \n        size_of_val(&strengthen_seed), \n        hasher.clone(), \n        false\n    );\n\n    /*\n      | Strengthen the seed, and feed it into\n      | hasher.\n      |\n      */\n    strengthen(&strengthen_seed, microseconds, hasher);\n}\n\n\npub fn seed_periodic(hasher: &mut Sha512, rng: &mut RNGState) {\n    /*\n      | Everything that the 'fast' seeder includes\n      |\n      */\n    seed_fast(hasher);\n\n    /*\n      | High-precision timestamp\n      |\n      */\n    seed_timestamp(hasher);\n\n    /*\n      | Add the events hasher into the mix\n      |\n      */\n    rng.seed_events(hasher);\n\n    /*\n      | Dynamic environment data (performance\n      | monitoring, ...)\n      |\n      */\n    let old_size = hasher.size();\n\n    rand_add_dynamic_env(hasher);\n\n    log_print!{\n        LogFlags::RAND, \n        \"Feeding {} bytes of dynamic environment data into RNG\\n\", \n        hasher.size() - old_size\n    };\n\n    /*\n      | Strengthen for 10 ms\n      |\n      */\n    seed_strengthen(hasher, rng, 10000);\n}\n\n\npub fn seed_startup(hasher: &mut Sha512, rng: &mut RNGState) {\n    /*\n      | Gather 256 bits of hardware randomness,\n      | if available\n      |\n      */\n    seed_hardware_slow(hasher);\n\n    /*\n      | Everything that the 'slow' seeder includes.\n      |\n      */\n    seed_slow(hasher, rng);\n\n    /*\n      | Dynamic environment data (performance\n      | monitoring, ...)\n      |\n      */\n    let old_size = hasher.size();\n\n    rand_add_dynamic_env(hasher);\n\n    // Static environment data\n    rand_add_static_env(hasher);\n\n    log_print!{\n        LogFlags::RAND, \n        \"Feeding {} bytes of environment data into RNG\\n\", \n        hasher.size() - old_size\n    };\n\n    /*\n      | Strengthen for 100 ms\n      |\n      */\n    seed_strengthen(hasher, rng, 100000);\n}\n\nimpl Default for FastRandomContext {\n\n    fn default() -> Self {\n        let deterministic = false;\n    \n        Self::new(deterministic)\n    }\n}\n\nimpl RandRange for FastRandomContext {\n\n    /**\n    | Generate a random integer in the range\n    | [0..range).\n    | \n    | Precondition: range > 0.\n    |\n    */\n    fn randrange(&mut self, mut range: u64) -> u64 {\n        assert!(range != 0);\n    \n        range -= 1;\n    \n        let bits: u64 = count_bits(range);\n    \n        while true {\n    \n            let ret: u64 = self.randbits(bits.try_into().unwrap());\n    \n            if ret <= range {\n                return ret;\n            }\n        }\n    \n        unreachable!();\n    }\n}\n\nimpl From<&u256> for FastRandomContext {\n\n    /**\n    | Initialize with explicit seed (only\n    | for testing)\n    |\n    */\n    fn from(seed: &u256) -> Self {\n        let mut rng = ChaCha20::default();\n        rng.set_key(seed.blob().begin(), 32);\n    \n        Self {\n            requires_seed: false,\n            rng:           rng,\n            bytebuf:       [0; 64],\n            bytebuf_size:  0,\n            bitbuf:        0,\n            bitbuf_size:   0,\n        }\n    }\n}\n\nimpl RngCore for FastRandomContext {\n\n    fn next_u32(&mut self) -> u32 {\n        self.randbits(32).try_into().unwrap()\n    }\n\n    fn next_u64(&mut self) -> u64 {\n        if self.bytebuf_size < 8 {\n            self.fill_byte_buffer();\n        }\n    \n        let offset: isize = self.bytebuf_size.try_into().unwrap();\n    \n        let ret: u64 = readle64(\n            unsafe {\n                self.bytebuf.as_mut_ptr().add(64).offset(-offset)\n            }\n        );\n    \n        self.bytebuf_size -= 8;\n    \n        ret\n    }\n\n    fn fill_bytes(&mut self, dest: &mut [u8]) {\n        if self.requires_seed {\n            self.random_seed();\n        }\n    \n        self.rng.keystream(dest.as_mut_ptr(), dest.len());\n    }\n\n    fn try_fill_bytes(&mut self, dest: &mut [u8]) -> Result<(), rand::Error> {\n        Ok(self.fill_bytes(dest))\n    }\n}\n\nimpl From<&mut FastRandomContext> for FastRandomContext {\n\n    /**\n    | Move a FastRandomContext. If the original\n    | one is used again, it will be reseeded.\n    |\n    */\n    fn from(other: &mut FastRandomContext) -> Self {\n        let mut x: Self = unsafe { std::mem::zeroed() };\n    \n        x.requires_seed = other.requires_seed;\n        x.rng           = other.rng.clone();\n        x.bytebuf       = other.bytebuf;\n    \n        x.bytebuf_size  = other.bytebuf_size;\n        x.bitbuf        = other.bitbuf;\n        x.bitbuf_size   = other.bitbuf_size;\n    \n        other.requires_seed = true;\n        other.bytebuf_size  = 0;\n        other.bitbuf_size   = 0;\n    \n        x\n    }\n}\n\nimpl FastRandomContext {\n\n    pub fn new(deterministic: bool) -> Self {\n        let mut x = Self {\n            requires_seed: !deterministic,\n            rng:           ChaCha20::default(),\n            bytebuf:       [0; 64],\n            bytebuf_size:  0,\n            bitbuf:        0,\n            bitbuf_size:   0,\n        };\n    \n        if !deterministic {\n            return x;\n        }\n    \n        let mut seed: u256 = u256::default();\n    \n        x.rng.set_key(seed.blob().begin(), 32);\n    \n        x\n    }\n\n    pub fn fill_byte_buffer(&mut self) {\n        //I call this unsafe because we are\n        //essentially borrowing self as mut twice\n        //\n        //the below trick, converting to ptr and\n        //back is a hack to silence the borrow\n        //czecher\n        unsafe {\n            let ptr = self.bytebuf.as_mut_slice().as_mut_ptr();\n            let len = self.bytebuf.len();\n    \n            let slice = unsafe { std::slice::from_raw_parts_mut(ptr,len) };\n    \n            self.fill_bytes(slice);\n    \n            self.bytebuf_size = size_of_val(&self.bytebuf).try_into().unwrap();\n        }\n    }\n\n    pub fn fill_bit_buffer(&mut self) {\n        self.bitbuf = self.rand64();\n        self.bitbuf_size = 64;\n    }\n\n    /**\n    | Generate a random 64-bit integer.\n    |\n    */\n    pub fn rand64(&mut self) -> u64 {\n        self.next_u64()\n    }\n\n    /**\n    | Generate a random (bits)-bit integer.\n    |\n    */\n    pub fn randbits(&mut self, bits: i32) -> u64 {\n        if bits == 0 {\n            0\n    \n        } else if bits > 32 {\n            self.rand64() >> (64 - bits)\n    \n        } else {\n    \n            if self.bitbuf_size < bits {\n                self.fill_bit_buffer();\n            }\n    \n            let ret: u64 = self.bitbuf & (!0 >> (64 - bits));\n            self.bitbuf >>= bits;\n            self.bitbuf_size -= bits;\n            ret\n        }\n    }\n\n    /**\n    | Generate a random 32-bit integer.\n    |\n    */\n    pub fn rand32(&mut self) -> u32 {\n        self.next_u32()\n    }\n\n    /**\n    | Generate a random boolean.\n    |\n    */\n    pub fn randbool(&mut self) -> bool {\n        self.randbits(1) != 0\n    }\n\n    pub fn min() -> u64 {\n        0\n    }\n\n    pub fn max() -> u64 {\n        u64::MAX\n    }\n\n    #[inline]\n    pub fn invoke(&mut self) -> u64 {\n        self.rand64()\n    }\n\n    pub fn random_seed(&mut self) {\n        let seed: u256 = get_rand_hash();\n        self.rng.set_key(seed.blob().begin(), 32);\n        self.requires_seed = false;\n    }\n\n    /**\n    | generate a random uint256.\n    |\n    */\n    pub fn rand256(&mut self) -> u256 {\n        if self.bytebuf_size < 32 {\n            self.fill_byte_buffer();\n        }\n    \n        let mut ret = u256::default();\n    \n        let offset: isize = self.bytebuf_size.try_into().unwrap();\n    \n        unsafe {\n            let dst = ret.blob().begin() as *mut c_void;\n            let src = self.bytebuf.as_mut_ptr().add(64).offset(-offset);\n    \n            libc::memcpy(\n                dst, \n                src as *const c_void, \n                32\n            );\n        }\n    \n        self.bytebuf_size -= 32;\n    \n        ret\n    }\n\n    /**\n    | Generate random bytes.\n    |\n    */\n    pub fn randbytes(&mut self, len: usize) -> Vec<u8> {\n        if self.requires_seed {\n            self.random_seed();\n        }\n        let mut ret = Vec::<u8>::with_capacity(len);\n        if len > 0 {\n            unsafe {\n                self.rng.keystream(ret.as_mut_ptr(), len);\n                ret.set_len(len); // <-- add this\n            }\n        }\n        ret\n    }\n}\n\nimpl Default for RNGInnerState {\n\n    fn default() -> Self {\n        Self {\n            state:           [0; 32],\n            counter:         0,\n            strongly_seeded: false,\n        }\n    }\n}\n\nimpl Default for RNGState {\n\n    fn default() -> Self {\n        init_hardware_rand();\n    \n        Self {\n            inner:  Mutex::new(RNGInnerState::default()),\n            events: Mutex::new(RNGStateEvents::default()),\n        }\n    }\n}\n\nimpl RNGState {\n\n    pub fn add_event(&mut self, event_info: u32) {\n        let mut events = self.events.lock();\n    \n        events.hasher.write_ptr(\n            &event_info as *const _ as *const u8, \n            size_of_val(&event_info)\n        );\n    \n        /*\n        | Get the low four bytes of the\n        | performance counter. This\n        | translates to roughly the subsecond\n        | part.\n        */\n        let perfcounter: u32 \n        = (get_performance_counter() & 0xffffffff).try_into().unwrap();\n    \n        events.hasher.write_ptr(\n            &perfcounter as *const _ as *const u8, \n            size_of_val(&perfcounter)\n        );\n    }\n\n    /**\n    | Feed (the hash of) all events added through\n    | AddEvent() to hasher.\n    |\n    */\n    pub fn seed_events(&mut self, hasher: &mut Sha512) {\n        /*\n          | We use only Sha256 for the events\n          | hashing to get the ASM speedups we have\n          | for Sha256, since we want it to be fast\n          | as network peers may be able to trigger\n          | it repeatedly.\n          */\n        let mut events = self.events.lock();\n    \n        let mut events_hash: [u8; 32] = [0; 32];\n    \n        events.hasher.finalize(&mut events_hash);\n        events.hasher.write_ptr(events_hash.as_mut_ptr(), 32);\n    \n        // Re-initialize the hasher with the finalized state to use later.\n        events.hasher.reset();\n        events.hasher.write_ptr(events_hash.as_mut_ptr(), 32);\n    }\n\n    /**\n    | Extract up to 32 bytes of entropy from\n    | the RNG state, mixing in new entropy\n    | from hasher.\n    | \n    | If this function has never been called\n    | with strong_seed = true, false is returned.\n    |\n    */\n    pub fn mix_extract(\n        &mut self,\n        out:         &mut [u8],\n        num:         usize,\n        mut hasher:  Sha512,\n        strong_seed: bool\n    ) -> bool\n    {\n        assert!{num <= 32};\n    \n        type BufType = [u8; 64];\n    \n        //Buffer needs to have hasher's output size\n        const_assert!{\n            size_of::<BufType>() == SHA512_OUTPUT_SIZE, \n        };\n    \n        let mut buf: BufType = [0; 64];\n    \n        let mut ret: bool = false;\n    \n        {\n            let mut inner = self.inner.lock();\n    \n            inner.strongly_seeded |= strong_seed;\n    \n            ret = inner.strongly_seeded;\n    \n            /*\n              | Write the current state of the RNG into\n              | the hasher\n              |\n              */\n            hasher.write(\n                inner.state.as_ptr(), \n                32\n            );\n    \n            /*\n              | Write a new counter number into the state\n              |\n              */\n            hasher.write(\n                &mut inner.counter as *mut u64 as *mut u8, \n                size_of_val(&inner.counter)\n            );\n    \n            inner.counter += 1;\n    \n            // Finalize the hasher\n            hasher.finalize(&mut buf);\n    \n            unsafe {\n    \n                /*\n                  | Store the last 32 bytes of the\n                  | hash output as new RNG state.\n                  |\n                  */\n                libc::memcpy(\n                    inner.state.as_mut_ptr() as *mut c_void, \n                    buf.as_ptr().add(32) as *const c_void, \n                    32\n                );\n            }\n        }\n    \n        /*\n           | If desired, copy (up to) the first 32\n           | bytes of the hash output as output.\n           |\n           */\n        if num != 0 { \n    \n            assert!{out.len() != 0};\n    \n            unsafe {\n                libc::memcpy(\n                    out.as_mut_ptr() as *mut c_void, \n                    buf.as_ptr() as *const c_void, \n                    num\n                );\n            }\n        }\n    \n        // Best effort cleanup of internal state\n        hasher.reset();\n    \n        memory_cleanse(buf.as_mut_ptr() as *mut c_void, 64);\n    \n        ret\n    }\n}\n\n\nThis crate has the following authors:\n[\n    \"klebs <none>\",\n]\nThis crate has the following rust edition:\n2021\nThis crate has the following license:\nMIT\nThis crate is held in the following repository:\nhttps://github.com/klebs6/bitcoin-rs\n","role":"user"}],"model":"gpt-5.1"}}
