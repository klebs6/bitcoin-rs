{"custom_id":"bitcoinleveldb-lru-ai-readme-request","method":"POST","url":"/v1/chat/completions","body":{"max_completion_tokens":8192,"messages":[{"content":"We would like you to write a README.md for a rust crate with maximally helpful content.\nAdditionally, we would like you to generate several fields from the Cargo.toml package section for this crate.\nWe will show you the strict json output format we would like you to provide.\nPlease tailor your response for an apex consumer of maximal intelligence.\nA reader of this README.md should have a good sense of what the crate does and how to use it after reading what you write.\nPlease include a messaage indicating that this README.md file was generated by an AI model and may not be 100% accurate however it should be pretty good.\n\n\nCarefully read and parse the information we sent you.\n\nWe want to generate all information items requested by our schema:\n\n- Read the description of each field. Use it, along with the content we sent you to generate an appropriate value for each.\n- Ensure that each generated field optimally serves its desired purpose.\n- Each item should be deep, detailed, and specific. Use optimally descriptive and useful language. Do not be too verbose.\n- Do not add any extra keys to the generated object. Generate precisely what we ask of you: no more, no less.\n- For fields which are of an enum type: pick *exactly one variant*. Do not generate any information for the unselected variants.\n- For array fields: supply a JSON list [ ... ] of items, each conforming to the correct item type.\n- For map fields: Provide a JSON object { \"key_as_string\": <value>, ... }. Keys must be valid JSON strings. Keys and Values must conform to the key and value schemas we provide to you. Fill all subfields of each and every map value.\n- If we ask you to justify a map field, provide your justification/confidence score for the entire map. Do not attempt to separately justify each map element.\n- Likewise, if we ask you to justify an array field, provide your justification/confidence score for the entire array. Do not attempt to separately justify each element.\n- Provide numeric fields as real JSON numbers (not strings).\n- For string fields: provide the value precisely as a string literal. If optional, you may set it to null.\n- If we ask you to justify or provide confidence scores for the fields of a structure, please do so carefully. For each field we ask you to justify, please explain or justify your choice clearly. Confidence scores must be specified as real JSON numbers in the range `[0..1]`.\n- If we ask to you generate an optional field, either fill it or set it to null.\n- If we ask you to justify your choice of an enum variant, please explain your choice clearly. Do not justify your decision to leave a variant unselected. Provide your justification as a JSON string. Provide your confidence score as a real JSON number in the range `[0..1]`.\n- In terms of the JSON format we would like you to provide, the object itself will be deserialied based on the following schema. \n- We begin exactly at the top: the data should not be nested under a parent named 'fields' or anything similar.\n\n## Here is the schema to use:\n\nObject {\n    \"fields\": Object {\n        \"crate_name\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be the name of our crate, *verbatim*, trimmed for use as a toml field\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"full_readme_markdown\": Object {\n            \"generation_instructions\": String(\"\\\"This string should be valid markdown representing the readme for this crate.\\\\n\\\\n Please make it terse, useful, and designed for an industrious commercially savy consumer of industrious nature. \\\\n\\\\n Advanced vocabulary and concepts are welcome. \\\\n\\\\n If this crate involves concepts from mathematics and/or physics, please describe and document them here.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_categories\": Object {\n            \"generation_instructions\": String(\"\\\"These categories will be used in our Cargo.toml file.\\\\n\\\\n It is important that users of this crate can find it on crates.io.\\\\n\\\\n Choose maximum 5 categories and make sure they are actually meaningful for our crate.\\\\n\\\\n These categories should only be chosen from among the legal crates.io categories. \\\\n\\\\n The ONLY legal categoreis are:\\\\n\\\\n``\\\\n accessibility\\\\n aerospace\\\\n algorithms\\\\n api-bindings\\\\n asynchronous\\\\n authentication\\\\n caching\\\\n command-line-interface\\\\n command-line-utilities\\\\n compilers\\\\n compression\\\\n computer-vision\\\\n concurrency\\\\n config\\\\n cryptography\\\\n data-structures\\\\n database\\\\n database-implementations\\\\n date-and-time\\\\n development-tools\\\\n email\\\\n embedded\\\\n emulators\\\\n encoding\\\\n external-ffi-bindings\\\\n filesystem\\\\n finance\\\\n game-development\\\\n game-engines\\\\n games\\\\n graphics\\\\n gui\\\\n hardware-support\\\\n internationalization\\\\n localization\\\\n mathematics\\\\n memory-management\\\\n multimedia\\\\n network-programming\\\\n no-std\\\\n os\\\\n parser-implementations\\\\n parsing\\\\n rendering\\\\n rust-patterns\\\\n science\\\\n simulation\\\\n template-engine\\\\n text-editors\\\\n text-processing\\\\n value-formatting\\\\n virtualization\\\\n visualization\\\\n wasm\\\\n web-programming\\\\n ``\\\\n\\\\n DO NOT specify a category that is not on this list.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n        \"package_description\": Object {\n            \"generation_instructions\": String(\"\\\"We will place this package_description in our Cargo.toml file so the users of this crate\\\\n know what it does.\\\\n\\\\n The discription should be technical, clear, and useful. It should be terse.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"string\"),\n        },\n        \"package_keywords\": Object {\n            \"generation_instructions\": String(\"\\\"These keywords will be used in our Cargo.toml file so the users of this crate can easily\\\\n find it in the package system. Keywords should be useful, varied, meaningful, and reflect\\\\n the contents of our crate. They should make it easy to find. Please provide exactly five.\\\"\"),\n            \"required\": Bool(true),\n            \"type\": String(\"array_of_strings\"),\n        },\n    },\n    \"struct_docs\": String(\" This structure is a directive indicating which fields we need you to provide for us in your response to our query.\\n\"),\n    \"struct_name\": String(\"AiReadmeWriterDesiredOutput\"),\n    \"type\": String(\"struct\"),\n}\n\n## Please keep the following in mind:\n\nEntry phrasing:\n\n- Ensure entries are concise and focused.\n- Ensure there are no vague phrases (e.g., avoid starting with \"Illustrates the something something...\"; instead, use direct details like \"The hanging green vines on the garden wall\").\n\nEnsure Clarity and Consistency:\n\n- Use clear, grammatically correct sentences.\n- Maintain a consistent tone and style throughout.\n\nEnrich Content:\n\n- Communicate additional information where appropriate to enhance depth and value.\n- If a mathematical, physical, or otherwise technical background is helpful, please provide it.\n- Ensure that the content is comprehensive and meets all requirements.\n\nFocus Language:\n\n- Use deliberate and precise language.\n- Avoid vague verbs and keep the maximally intelligent and detail oriented audience in mind. \n- We explicitly ban the words `hush`, `subtle`, `mythic`, and `aura`. Please keep your vocabulary choices rigorous.\n- Do not use modern cultural references \n- Do not generically reference ideas which do not belong in the global environment.\n\n## Present the Final JSON:\n\n- Format the output as a JSON object.\n- Include all fields properly named from the schema we sent you.\n\nEnsure Proper Formatting:\n\n- Use proper JSON syntax with keys and arrays.\n- Ensure that all entries are correctly placed in their respective places.\n\nProvide only the final JSON data as specified by the schema. \n\nDo not include any schema generation instructions, meta-fields, nested-templates (unless directly asked) or extraneous information. \n\nProvide only the populated fields and their values, conforming exactly to the required structure without embedding any intermediate annotations or placeholders.\n\nYour output should only consist of the JSON object. do *not* include a preamble or postamble to your response. \n\nWe seek to successfully parse your response directly as JSON.\n\nFor the final answer, output the result only as a valid JSON object, wrapped inside triple backticks with json immediately after the opening backticks.\nThe JSON must be parser-ready:\n– No additional text before or after the block\n– No markdown or prose outside the fenced block\n– No HTML escaping of quotes, slashes, or other characters\n– Indentation and whitespace preserved exactly as written\n– The content between the fences must be valid JSON as per RFC 8259","role":"system"},{"content":"We have a rust crate named 'bitcoinleveldb-lru' 0.1.19. \n\nHere is the interface:\n/// A single shard of sharded cache.\n#[derive(Getters,Setters)]\n#[getset(get=\"pub\",set=\"pub\")]\npub struct LRUCache  {\n    capacity: usize,\n    mutex:    RefCell<Mutex<LRUCacheInner>>,\n}\n\n\npub struct ShardedLRUCache  {\n    base:     Cache,\n    shard:    [LRUCache; NUM_SHARDS],\n    id_mutex: parking_lot::RawMutex,\n    last_id:  u64,\n}\n\n\npub struct LRUCacheInner  {\n    usage:  usize,\n    lru:    Box<LRUHandle>,\n    in_use: Box<LRUHandle>,\n    table:  HandleTable,\n}\n\n\n/// An entry is a variable length heap-allocated structure. \n///\n/// Entries are kept in a circular doubly linked list ordered by access time.\n/// \n#[repr(C)]\npub struct LRUHandle  {\n    value:      *mut c_void,\n    deleter:    fn(_0: &Slice, value: *mut c_void) -> c_void,\n    next_hash:  *mut LRUHandle,\n    next:       *mut LRUHandle,\n    prev:       *mut LRUHandle,\n    charge:     usize,\n    key_length: usize,\n    in_cache:   bool,\n    refs:       u32,\n    hash:       u32,\n    key_data:   [u8; 1],\n}\n\n\n/// We provide our own simple hash table since it removes a whole bunch of\n/// porting hacks and is also faster than some of the built-in hash table\n/// implementations in some of the compiler/runtime combinations we have tested.\n///\n/// E.g., readrandom speeds up by ~5% over the g++\n/// 4.4.3's builtin hashtable.\n#[derive(Getters,Setters)]\n#[getset(get=\"pub\",set=\"pub\")]\npub struct HandleTable  {\n    length: u32,\n    elems:  u32,\n    list:   *mut *mut LRUHandle,\n}\n\npub fn lru_debug_verify_inner(inner: &mut LRUCacheInner) {\n    trace!(\n        \"lru_debug_verify_inner: usage={}, lru_head={:p}, in_use_head={:p}\",\n        inner.usage(),\n        inner.lru_head_mut(),\n        inner.in_use_head_mut()\n    );\n\n    let lru_head: *mut LRUHandle    = inner.lru_head_mut();\n    let in_use_head: *mut LRUHandle = inner.in_use_head_mut();\n\n    lru_debug_verify_list(lru_head, \"lru_\");\n    lru_debug_verify_list(in_use_head, \"in_use_\");\n}\n\n\npub fn lru_debug_verify_list(head: *mut LRUHandle, list_name: &str) {\n    trace!(\n        \"lru_debug_verify_list: list_name={}, head={:p}\",\n        list_name,\n        head\n    );\n\n    assert!(\n        !head.is_null(),\n        \"lru_debug_verify_list({}): list head pointer is null\",\n        list_name\n    );\n\n    let align = core::mem::align_of::<LRUHandle>();\n    let head_addr = head as usize;\n\n    assert!(\n        head_addr % align == 0,\n        \"lru_debug_verify_list({}): head pointer {:p} is misaligned (align={})\",\n        list_name,\n        head,\n        align\n    );\n\n    let mut node: *mut LRUHandle = (*head).next_ptr();\n    let mut steps: usize         = 0;\n\n    while !core::ptr::eq(node, head) {\n        let addr = node as usize;\n\n        assert!(\n            addr % align == 0,\n            \"lru_debug_verify_list({}): node pointer {:p} is misaligned (align={}) at step {}\",\n            list_name,\n            node,\n            align,\n            steps\n        );\n\n        let next = (*node).next_ptr();\n        let prev = (*node).prev_ptr();\n\n        assert!(\n            !next.is_null() && !prev.is_null(),\n            \"lru_debug_verify_list({}): node {:p} has null next/prev (next={:p}, prev={:p})\",\n            list_name,\n            node,\n            next,\n            prev\n        );\n\n        assert!(\n            core::ptr::eq((*next).prev_ptr(), node),\n            \"lru_debug_verify_list({}): next.prev inconsistency at node {:p} (next={:p}, next.prev={:p})\",\n            list_name,\n            node,\n            next,\n            (*next).prev_ptr()\n        );\n\n        assert!(\n            core::ptr::eq((*prev).next_ptr(), node),\n            \"lru_debug_verify_list({}): prev.next inconsistency at node {:p} (prev={:p}, prev.next={:p})\",\n            list_name,\n            node,\n            prev,\n            (*prev).next_ptr()\n        );\n\n        steps = steps.wrapping_add(1);\n\n        assert!(\n            steps < 1_000_000,\n            \"lru_debug_verify_list({}): list appears excessively long; possible cycle without head\",\n            list_name\n        );\n\n        node = next;\n    }\n\n    debug!(\n        \"lru_debug_verify_list: list_name={}, head={:p}, nodes_traversed={}\",\n        list_name,\n        head,\n        steps\n    );\n}\n\n\npub fn lru_remove_node(e: *mut LRUHandle) {\n    trace!(\"lru_remove_node: e={:p}\", e);\n\n    let next = (*e).next_ptr();\n    let prev = (*e).prev_ptr();\n\n    (*next).set_prev_ptr(prev);\n    (*prev).set_next_ptr(next);\n}\n\n\npub fn unref_inner(inner: &mut LRUCacheInner, e: *mut LRUHandle) {\n    let refs_before = (*e).refs();\n    let in_cache    = (*e).is_in_cache();\n    let hash        = (*e).hash_value();\n    let charge      = (*e).charge_value();\n\n    trace!(\n        \"unref_inner: e={:p}, refs_before={}, in_cache={}, hash=0x{:x}, charge={}\",\n        e,\n        refs_before,\n        in_cache,\n        hash,\n        charge\n    );\n\n    assert!(\n        refs_before > 0,\n        \"unref_inner: attempt to decrement reference count below zero\"\n    );\n\n    (*e).decrement_refs();\n    let refs_after     = (*e).refs();\n    let in_cache_after = (*e).is_in_cache();\n\n    trace!(\n        \"unref_inner: e={:p}, refs_after_decrement={}, in_cache={}\",\n        e,\n        refs_after,\n        in_cache_after\n    );\n\n    if refs_after == 0 {\n        // Exact LevelDB semantics: an entry is only destroyed when its refcount\n        // drops to zero and it is no longer in the cache.\n        assert!(\n            !in_cache_after,\n            \"unref_inner: entry with refs==0 must not be marked in_cache\"\n        );\n\n        let deleter = (*e).deleter_fn();\n        let key     = (*e).key();\n        let value   = (*e).value_ptr();\n\n        trace!(\n            \"unref_inner: deallocating e={:p}, hash=0x{:x}, charge={}\",\n            e,\n            hash,\n            charge\n        );\n\n        deleter(&key, value);\n        libc::free(e as *mut libc::c_void);\n    } else if in_cache_after && refs_after == 1 {\n        // Exact LevelDB semantics: when the cache still owns the only remaining\n        // reference, move the entry from the in_use_ list back to the lru_ list.\n        trace!(\n            \"unref_inner: moving e={:p} from in_use_ list to lru_ list (refs now 1)\",\n            e\n        );\n\n        lru_remove_node(e);\n        let lru_head: *mut LRUHandle = inner.lru_head_mut();\n        lru_append_node(lru_head, e);\n    } else {\n        // Multiple references remain, or the entry is not in the cache.\n        trace!(\n            \"unref_inner: e={:p} remains with refs={} (in_cache={}), no list movement performed\",\n            e,\n            refs_after,\n            in_cache_after\n        );\n    }\n}\n\n\npub fn finish_erase_inner(inner: &mut LRUCacheInner, e: *mut LRUHandle) -> bool {\n    trace!(\"finish_erase_inner: e={:p}\", e);\n\n    if !e.is_null() {\n        assert!((*e).is_in_cache());\n        lru_remove_node(e);\n        (*e).set_in_cache(false);\n\n        let charge = (*e).charge_value();\n        inner.sub_usage(charge);\n\n        unref_inner(inner, e);\n        true\n    } else {\n        false\n    }\n}\n\n\npub fn lru_noop_deleter(_key: &Slice, _value: *mut c_void) -> c_void {\n    trace!(\"lru_noop_deleter invoked\");\n    unsafe { core::mem::zeroed() }\n}\n\n\npub fn new_lru_cache(capacity: usize) -> *mut Cache {\n    trace!(\"new_lru_cache: capacity={}\", capacity);\n    let cache = ShardedLRUCache::new(capacity);\n    let boxed = Box::new(cache);\n    Box::into_raw(boxed) as *mut Cache\n}\n\n\npub fn mock_test_deleter(_: &Slice, _: *mut c_void) -> c_void {\n    unsafe { core::mem::zeroed() }\n}\n\n\npub fn mock_make_heap_sentinel_node() -> *mut LRUHandle {\n    let node = libc::malloc(core::mem::size_of::<LRUHandle>()) as *mut LRUHandle;\n    assert!(!node.is_null(), \"mock_make_heap_sentinel_node: alloc failed\");\n\n    (*node).set_value_ptr(core::ptr::null_mut());\n    (*node).set_deleter_fn(mock_test_deleter);\n    (*node).set_charge_value(0);\n    (*node).set_key_length(0);\n    (*node).set_hash_value(0);\n    (*node).set_in_cache(false);\n    (*node).set_refs(0);\n    (*node).set_next_hash_ptr(core::ptr::null_mut());\n    (*node).set_next_ptr(node);\n    (*node).set_prev_ptr(node);\n\n    node\n}\n\n\npub fn mock_make_list_node() -> *mut LRUHandle {\n    let node = libc::malloc(core::mem::size_of::<LRUHandle>()) as *mut LRUHandle;\n    assert!(!node.is_null(), \"mock_make_list_node: allocation failed\");\n\n    (*node).set_value_ptr(core::ptr::null_mut());\n    (*node).set_deleter_fn(mock_test_deleter);\n    (*node).set_charge_value(0);\n    (*node).set_key_length(0);\n    (*node).set_hash_value(0);\n    (*node).set_in_cache(false);\n    (*node).set_refs(1);\n    (*node).set_next_hash_ptr(core::ptr::null_mut());\n    (*node).set_next_ptr(core::ptr::null_mut());\n    (*node).set_prev_ptr(core::ptr::null_mut());\n\n    node\n}\n\n\npub fn handle_table_keys_equal(entry: *const LRUHandle, key_: &Slice) -> bool {\n    trace!(\"handle_table_keys_equal: entry={:p}\", entry);\n\n    unsafe {\n        let entry_len = (*entry).key_len();\n        let key_len   = *key_.size();\n\n        if entry_len != key_len {\n            trace!(\n                \"handle_table_keys_equal: length mismatch (entry_len={}, key_len={})\",\n                entry_len,\n                key_len\n            );\n            return false;\n        }\n\n        if entry_len == 0 {\n            trace!(\"handle_table_keys_equal: both keys empty, treating as equal\");\n            return true;\n        }\n\n        let entry_ptr = (*entry).key_data_ptr();\n        let key_ptr   = *key_.data();\n\n        if entry_ptr.is_null() || key_ptr.is_null() {\n            debug!(\n                \"handle_table_keys_equal: null pointer(s) encountered (entry_ptr={:p}, key_ptr={:p})\",\n                entry_ptr,\n                key_ptr\n            );\n            return false;\n        }\n\n        let entry_slice = core::slice::from_raw_parts(entry_ptr, entry_len);\n        let key_slice   = core::slice::from_raw_parts(key_ptr,   key_len);\n\n        let equal = entry_slice == key_slice;\n\n        trace!(\n            \"handle_table_keys_equal: compared {} bytes, equal={}\",\n            entry_len,\n            equal\n        );\n\n        equal\n    }\n}\n\n\npub fn unref_inner_list_contains(head: *mut LRUHandle, target: *mut LRUHandle, list_name: &str) -> bool {\n    trace!(\n        \"unref_inner_list_contains: list_name={}, head={:p}, target={:p}\",\n        list_name,\n        head,\n        target\n    );\n\n    if head.is_null() || target.is_null() {\n        debug!(\n            \"unref_inner_list_contains: list_name={} has null head/target (head={:p}, target={:p})\",\n            list_name,\n            head,\n            target\n        );\n        return false;\n    }\n\n    let align = core::mem::align_of::<LRUHandle>();\n    let mut node: *mut LRUHandle = (*head).next_ptr();\n    let mut steps: usize         = 0;\n\n    while !core::ptr::eq(node, head) {\n        let addr = node as usize;\n\n        if addr % align != 0 {\n            warn!(\n                \"unref_inner_list_contains({}): node pointer {:p} is misaligned (align={}), aborting search\",\n                list_name,\n                node,\n                align\n            );\n            return false;\n        }\n\n        if core::ptr::eq(node, target) {\n            trace!(\n                \"unref_inner_list_contains: list_name={} contains target={:p} after {} steps\",\n                list_name,\n                target,\n                steps\n            );\n            return true;\n        }\n\n        node = (*node).next_ptr();\n        steps = steps.wrapping_add(1);\n\n        if steps > 1_000_000 {\n            warn!(\n                \"unref_inner_list_contains({}): aborting search after {} steps; possible cycle without head\",\n                list_name,\n                steps\n            );\n            return false;\n        }\n    }\n\n    trace!(\n        \"unref_inner_list_contains: list_name={} does not contain target={:p}\",\n        list_name,\n        target\n    );\n    false\n}\n\n\npub fn lru_append_node(list: *mut LRUHandle, e: *mut LRUHandle) {\n    trace!(\"lru_append_node: list={:p}, e={:p}\", list, e);\n\n    // Make \"e\" newest entry by inserting just before *list\n    let last = (*list).prev_ptr();\n\n    (*e).set_next_ptr(list);\n    (*e).set_prev_ptr(last);\n    (*last).set_next_ptr(e);\n    (*list).set_prev_ptr(e);\n}\n\n\npub fn ref_inner(inner: &mut LRUCacheInner, e: *mut LRUHandle) {\n    trace!(\"ref_inner: e={:p}, refs={}\", e, (*e).refs());\n\n    if (*e).refs() == 1 && (*e).is_in_cache() {\n        // If on lru_ list, move to in_use_ list.\n        lru_remove_node(e);\n        let in_use_head: *mut LRUHandle = inner.in_use_head_mut();\n        lru_append_node(in_use_head, e);\n    }\n\n    (*e).increment_refs();\n}\n\nimpl HandleTable {\n\n    pub fn remove(&mut self, key_: &Slice, hash_: u32) -> *mut LRUHandle {\n        trace!(\"HandleTable::remove: hash={}\", hash_);\n        unsafe {\n            let ptr    = self.find_pointer(key_, hash_);\n            let result = *ptr;\n            if !result.is_null() {\n                let next_hash = (*result).next_hash_ptr();\n                *ptr = next_hash;\n                self.set_elems(self.elems().wrapping_sub(1));\n            }\n            result\n        }\n    }\n}\n\nimpl LRUCache {\n\n    pub fn prune(&mut self) {\n        trace!(\"LRUCache::prune\");\n    \n        let mut guard = self.mutex().borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            let lru_head: *mut LRUHandle = inner.lru_head_mut();\n            let align                     = core::mem::align_of::<LRUHandle>();\n    \n            while !core::ptr::eq((*lru_head).next_ptr(), lru_head) {\n                let e: *mut LRUHandle = (*lru_head).next_ptr();\n                let addr              = e as usize;\n    \n                if addr % align != 0 {\n                    error!(\n                        \"LRUCache::prune: encountered misaligned LRU node pointer {:p} \\\n                         (align={}); aborting prune\",\n                        e,\n                        align\n                    );\n                    break;\n                }\n    \n                let refs     = (*e).refs();\n                let in_cache = (*e).is_in_cache();\n                let hash     = (*e).hash_value();\n    \n                if refs != 1 {\n                    error!(\n                        \"LRUCache::prune: LRU entry {:p} has refs={} (expected 1), \\\n                         in_cache={}, hash={:#x}\",\n                        e,\n                        refs,\n                        in_cache,\n                        hash\n                    );\n                }\n    \n                assert!(\n                    refs == 1,\n                    \"LRUCache::prune: LRU entry has refs != 1\"\n                );\n    \n                let removed = inner\n                    .table_mut()\n                    .remove(&(*e).key(), (*e).hash_value());\n    \n                let erased = finish_erase_inner(&mut inner, removed);\n                if !erased {\n                    debug!(\n                        \"LRUCache::prune: FinishErase returned false while pruning \\\n                         (entry={:p}, hash={:#x})\",\n                        e,\n                        hash\n                    );\n                    assert!(erased);\n                }\n            }\n        }\n    }\n}\n\nimpl Drop for LRUCache {\n\n    fn drop(&mut self) {\n        trace!(\"LRUCache::drop: starting destruction\");\n    \n        let mut guard = self.mutex().borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            // Best-effort check of in_use_ list; do not panic here since Drop\n            // is often run during unwinding and we want to avoid aborting.\n            let in_use_head: *mut LRUHandle = inner.in_use_head_mut();\n            let in_use_next                 = (*in_use_head).next_ptr();\n            if !core::ptr::eq(in_use_next, in_use_head) {\n                warn!(\n                    \"LRUCache::drop: in_use_ list not empty at drop time; \\\n                     some handles may still be in use\"\n                );\n            }\n    \n            // Walk the lru_ list and try to remove each node through the\n            // HandleTable and FinishErase. This mirrors LevelDB's destructor\n            // but adds additional safety checks to avoid infinite loops and\n            // misaligned pointer dereferences.\n            let lru_head: *mut LRUHandle = inner.lru_head_mut();\n            let mut e: *mut LRUHandle    = (*lru_head).next_ptr();\n    \n            let align = core::mem::align_of::<LRUHandle>();\n            let mut steps: usize = 0;\n    \n            while !e.is_null() && !core::ptr::eq(e, lru_head) {\n                let addr = e as usize;\n                if addr % align != 0 {\n                    error!(\n                        \"LRUCache::drop: encountered misaligned LRUHandle pointer {:p}; \\\n                         aborting cleanup loop to avoid undefined behavior\",\n                        e\n                    );\n                    break;\n                }\n    \n                // Capture next before we mutate or potentially free `e`.\n                let next: *mut LRUHandle = (*e).next_ptr();\n                let refs: u32            = (*e).refs();\n                let in_cache: bool       = (*e).is_in_cache();\n                let hash: u32            = (*e).hash_value();\n                let charge: usize        = (*e).charge_value();\n    \n                debug!(\n                    \"LRUCache::drop: cleaning node {:p}, refs={}, in_cache={}, hash=0x{:x}, charge={}\",\n                    e,\n                    refs,\n                    in_cache,\n                    hash,\n                    charge\n                );\n    \n                if in_cache {\n                    // Normal path: entry is still in the cache and should be present\n                    // in the HandleTable. Remove it there and run FinishErase to\n                    // drop the cache's reference and free as appropriate.\n                    let removed = inner\n                        .table_mut()\n                        .remove(&(*e).key(), (*e).hash_value());\n    \n                    if !removed.is_null() {\n                        let erased = finish_erase_inner(&mut inner, removed);\n                        if !erased {\n                            warn!(\n                                \"LRUCache::drop: FinishErase returned false for {:p} (hash=0x{:x})\",\n                                removed,\n                                hash\n                            );\n                        }\n                    } else {\n                        // Inconsistent state: the node is on the lru_ list but\n                        // not in the table. Clear the in_cache flag and drop\n                        // a single reference so we at least move the refcount\n                        // toward zero, then continue.\n                        warn!(\n                            \"LRUCache::drop: node {:p} (hash=0x{:x}) not found in HandleTable; \\\n                             clearing in_cache flag and dropping one reference\",\n                            e,\n                            hash\n                        );\n                        (*e).set_in_cache(false);\n                        if refs > 0 {\n                            unref_inner(&mut inner, e);\n                        }\n                    }\n                } else {\n                    // Strongly inconsistent state: an entry reachable from lru_\n                    // but marked as not in_cache. To avoid spinning forever on\n                    // a corrupted list, drop a single reference (if any) and\n                    // then stop iterating.\n                    if refs > 0 {\n                        warn!(\n                            \"LRUCache::drop: LRU node {:p} has in_cache=false and refs={}; \\\n                             dropping a single reference and stopping cleanup loop\",\n                            e,\n                            refs\n                        );\n                        unref_inner(&mut inner, e);\n                    } else {\n                        warn!(\n                            \"LRUCache::drop: LRU node {:p} has in_cache=false and refs=0; \\\n                             stopping cleanup loop\",\n                            e\n                        );\n                    }\n                    break;\n                }\n    \n                e = next;\n                steps = steps.wrapping_add(1);\n    \n                if steps > 1_000_000 {\n                    error!(\n                        \"LRUCache::drop: aborting cleanup after {} iterations to avoid infinite loop\",\n                        steps\n                    );\n                    break;\n                }\n            }\n        }\n    \n        trace!(\"LRUCache::drop: completed destruction\");\n    }\n}\n\nimpl LRUCache {\n\n    pub fn new() -> Self {\n        trace!(\"LRUCache::new\");\n    \n        let inner = LRUCacheInner::new();\n    \n        LRUCache {\n            capacity: 0,\n            mutex:    RefCell::new(Mutex::new(inner)),\n        }\n    }\n\n    pub fn total_charge(&self) -> usize {\n        trace!(\"LRUCache::total_charge\");\n    \n        let mut guard = self.mutex.borrow_mut();\n        let inner = guard.lock();\n        inner.usage()\n    }\n\n    pub fn ref_(&mut self, e: *mut LRUHandle) {\n        trace!(\"LRUCache::ref_: e={:p}\", e);\n    \n        let mut guard = self.mutex.borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            ref_inner(&mut inner, e);\n        }\n    }\n\n    pub fn unref(&mut self, e: *mut LRUHandle) {\n        trace!(\"LRUCache::unref: e={:p}\", e);\n    \n        let mut guard = self.mutex.borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            unref_inner(&mut inner, e);\n        }\n    }\n\n    pub fn lru_remove(&mut self, e: *mut LRUHandle) {\n        trace!(\"LRUCache::lru_remove: e={:p}\", e);\n        unsafe { lru_remove_node(e); }\n    }\n\n    pub fn lru_append(&mut self, list: *mut LRUHandle, e: *mut LRUHandle) {\n        trace!(\"LRUCache::lru_append: list={:p}, e={:p}\", list, e);\n        unsafe { lru_append_node(list, e); }\n    }\n\n    pub fn lookup(&mut self, key_: &Slice, hash_: u32) -> *mut CacheHandle {\n        trace!(\"LRUCache::lookup: hash={}\", hash_);\n    \n        let mut guard = self.mutex.borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            let e = inner.table_mut().lookup(key_, hash_);\n            if !e.is_null() {\n                ref_inner(&mut inner, e);\n            }\n            e as *mut CacheHandle\n        }\n    }\n\n    pub fn release(&mut self, handle: *mut CacheHandle) {\n        trace!(\"LRUCache::release: handle={:p}\", handle);\n    \n        let mut guard = self.mutex.borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            let e = handle as *mut LRUHandle;\n            unref_inner(&mut inner, e);\n        }\n    }\n}\n\nimpl LRUCache {\n\n    /// If e != nullptr, finish removing *e from the cache; \n    ///\n    /// it has already been removed from the hash table. Return whether e !=\n    /// nullptr.\n    #[EXCLUSIVE_LOCKS_REQUIRED(mutex_)]\n    pub fn finish_erase(&mut self, e: *mut LRUHandle) -> bool {\n        trace!(\"LRUCache::finish_erase: e={:p}\", e);\n    \n        let mut guard = self.mutex().borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe { finish_erase_inner(&mut inner, e) }\n    }\n\n    pub fn erase(&mut self, key_: &Slice, hash_: u32) {\n        trace!(\"LRUCache::erase: hash={}\", hash_);\n    \n        let mut guard = self.mutex().borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            let e = inner.table_mut().remove(key_, hash_);\n            let _ = finish_erase_inner(&mut inner, e);\n        }\n    }\n}\n\nimpl ShardedLRUCache {\n\n    pub fn new(capacity: usize) -> Self {\n        trace!(\n            \"ShardedLRUCache::new: capacity={}, shards={}\",\n            capacity,\n            NUM_SHARDS\n        );\n    \n        let per_shard = if NUM_SHARDS == 0 {\n            0\n        } else {\n            (capacity + (NUM_SHARDS - 1)) / NUM_SHARDS\n        };\n    \n        let mut shards: [LRUCache; NUM_SHARDS] =\n            core::array::from_fn(|_| LRUCache::new());\n    \n        for shard in &mut shards {\n            shard.set_capacity(per_shard);\n        }\n    \n        ShardedLRUCache {\n            base:     Cache::default(),\n            shard:    shards,\n            id_mutex: parking_lot::RawMutex::INIT,\n            last_id:  0,\n        }\n    }\n\n    #[inline]\n    pub fn hash_slice(s: &Slice) -> u32 {\n        trace!(\"ShardedLRUCache::hash_slice\");\n        unsafe { leveldb_hash(*s.data(), *s.size(), 0) }\n    }\n\n    pub fn shard(hash_: u32) -> u32 {\n        hash_ >> (32 - NUM_SHARD_BITS as u32)\n    }\n\n    pub fn insert(\n        &mut self,\n        key_:      &Slice,\n        value:     *mut c_void,\n        charge:    usize,\n        deleter:   fn(key_: &Slice, value: *mut c_void) -> c_void\n    ) -> *mut CacheHandle\n    {\n        let hash = ShardedLRUCache::hash_slice(key_);\n        let idx  = ShardedLRUCache::shard(hash) as usize;\n    \n        trace!(\n            \"ShardedLRUCache::insert: hash={}, shard={}, charge={}\",\n            hash,\n            idx,\n            charge\n        );\n    \n        self.shard[idx].insert(key_, hash, value, charge, deleter)\n    }\n\n    pub fn lookup(&mut self, key_: &Slice) -> *mut CacheHandle {\n        let hash = ShardedLRUCache::hash_slice(key_);\n        let idx  = ShardedLRUCache::shard(hash) as usize;\n    \n        trace!(\n            \"ShardedLRUCache::lookup: hash={}, shard={}\",\n            hash,\n            idx\n        );\n    \n        self.shard[idx].lookup(key_, hash)\n    }\n\n    pub fn release(&mut self, handle: *mut CacheHandle) {\n        trace!(\"ShardedLRUCache::release: handle={:p}\", handle);\n    \n        unsafe {\n            let h   = handle as *mut LRUHandle;\n            let idx = ShardedLRUCache::shard((*h).hash_value()) as usize;\n            self.shard[idx].release(handle);\n        }\n    }\n\n    pub fn erase(&mut self, key_: &Slice) {\n        let hash = ShardedLRUCache::hash_slice(key_);\n        let idx  = ShardedLRUCache::shard(hash) as usize;\n    \n        trace!(\n            \"ShardedLRUCache::erase: hash={}, shard={}\",\n            hash,\n            idx\n        );\n    \n        self.shard[idx].erase(key_, hash);\n    }\n\n    pub fn value(&mut self, handle: *mut CacheHandle) -> *mut c_void {\n        trace!(\"ShardedLRUCache::value: handle={:p}\", handle);\n    \n        unsafe {\n            let h = handle as *mut LRUHandle;\n            if h.is_null() {\n                core::ptr::null_mut()\n            } else {\n                (*h).value_ptr()\n            }\n        }\n    }\n\n    pub fn new_id(&mut self) -> u64 {\n        trace!(\"ShardedLRUCache::new_id\");\n    \n        unsafe {\n            self.id_mutex.lock();\n            self.last_id = self.last_id.wrapping_add(1);\n            let id = self.last_id;\n            self.id_mutex.unlock();\n            id\n        }\n    }\n\n    pub fn prune(&mut self) {\n        trace!(\"ShardedLRUCache::prune\");\n    \n        for (i, shard) in self.shard.iter_mut().enumerate() {\n            trace!(\"ShardedLRUCache::prune: shard={}\", i);\n            shard.prune();\n        }\n    }\n\n    pub fn total_charge(&self) -> usize {\n        trace!(\"ShardedLRUCache::total_charge\");\n    \n        let mut total = 0usize;\n        for shard in &self.shard {\n            total = total.wrapping_add(shard.total_charge());\n        }\n        total\n    }\n}\n\nimpl Default for LRUCacheInner {\n\n    fn default() -> Self {\n        LRUCacheInner::new()\n    }\n}\n\nimpl LRUCacheInner {\n\n    /// Construct an inner state with properly initialized\n    /// LRU and in-use sentinel nodes.\n    pub fn new_with_sentinels() -> Self {\n        trace!(\"LRUCacheInner::new_with_sentinels: initializing inner state\");\n        LRUCacheInner::new()\n    }\n\n    pub fn new() -> Self {\n        trace!(\"LRUCacheInner::new: constructing inner state\");\n    \n        let mut lru_sentinel    = Box::new(LRUHandle::make_sentinel());\n        let mut in_use_sentinel = Box::new(LRUHandle::make_sentinel());\n    \n        unsafe {\n            let lru_head: *mut LRUHandle     = lru_sentinel.as_mut();\n            let in_use_head: *mut LRUHandle  = in_use_sentinel.as_mut();\n    \n            (*lru_head).set_next_ptr(lru_head);\n            (*lru_head).set_prev_ptr(lru_head);\n    \n            (*in_use_head).set_next_ptr(in_use_head);\n            (*in_use_head).set_prev_ptr(in_use_head);\n    \n            debug!(\n                \"LRUCacheInner::new: initialized sentinels lru_head={:p}, in_use_head={:p}\",\n                lru_head,\n                in_use_head\n            );\n        }\n    \n        LRUCacheInner {\n            usage: 0,\n            lru:   lru_sentinel,\n            in_use: in_use_sentinel,\n            table: HandleTable::default(),\n        }\n    }\n\n    pub fn usage(&self) -> usize {\n        self.usage\n    }\n\n    pub fn set_usage(&mut self, usage: usize) {\n        self.usage = usage;\n    }\n\n    pub fn add_usage(&mut self, delta: usize) {\n        self.usage = self.usage.wrapping_add(delta);\n    }\n\n    pub fn sub_usage(&mut self, delta: usize) {\n        self.usage = self.usage.wrapping_sub(delta);\n    }\n\n    pub fn lru_head_mut(&mut self) -> *mut LRUHandle {\n        &mut *self.lru\n    }\n\n    pub fn in_use_head_mut(&mut self) -> *mut LRUHandle {\n        &mut *self.in_use\n    }\n\n    pub fn table(&self) -> &HandleTable {\n        &self.table\n    }\n\n    pub fn table_mut(&mut self) -> &mut HandleTable {\n        &mut self.table\n    }\n}\n\nimpl HandleTable {\n\n    pub fn resize(&mut self) {\n        trace!(\n            \"HandleTable::resize: current length={}, elems={}\",\n            self.length(),\n            self.elems()\n        );\n    \n        unsafe {\n            let mut new_length: u32 = 4;\n            while new_length < *self.elems() {\n                new_length = new_length.wrapping_mul(2);\n            }\n    \n            let bytes =\n                (new_length as usize) * core::mem::size_of::<*mut LRUHandle>();\n    \n            let new_list: *mut *mut LRUHandle = if bytes == 0 {\n                core::ptr::null_mut()\n            } else {\n                let ptr = libc::malloc(bytes) as *mut *mut LRUHandle;\n                if !ptr.is_null() {\n                    core::ptr::write_bytes(ptr as *mut u8, 0u8, bytes);\n                }\n                ptr\n            };\n    \n            if new_list.is_null() {\n                error!(\n                    \"HandleTable::resize: allocation failed for {} bytes; leaving table unchanged\",\n                    bytes\n                );\n                return;\n            }\n    \n            let old_length: u32               = *self.length();\n            let old_list: *mut *mut LRUHandle = *self.list();\n            let mut count: u32                = 0;\n    \n            for i in 0..old_length {\n                let mut h: *mut LRUHandle = *old_list.add(i as usize);\n                while !h.is_null() {\n                    let next: *mut LRUHandle = (*h).next_hash_ptr();\n                    let hash: u32            = (*h).hash_value();\n                    let bucket: usize        =\n                        (hash & (new_length - 1)) as usize;\n                    let slot: *mut *mut LRUHandle = new_list.add(bucket);\n                    let prev_head: *mut LRUHandle = *slot;\n    \n                    (*h).set_next_hash_ptr(prev_head);\n                    *slot = h;\n                    h = next;\n                    count = count.wrapping_add(1);\n                }\n            }\n    \n            assert_eq!(\n                *self.elems(),\n                count,\n                \"HandleTable::resize: element count mismatch (elems_={}, counted={})\",\n                self.elems(),\n                count\n            );\n    \n            if !old_list.is_null() {\n                libc::free(old_list as *mut libc::c_void);\n            }\n    \n            self.set_list(new_list);\n            self.set_length(new_length);\n    \n            debug!(\n                \"HandleTable::resize: new length={}, elems={}\",\n                self.length(),\n                self.elems()\n            );\n        }\n    }\n}\n\nimpl LRUHandle {\n\n    pub fn make_sentinel() -> LRUHandle {\n        trace!(\"LRUHandle::make_sentinel: constructing LRUHandle sentinel\");\n    \n        LRUHandle {\n            value:      core::ptr::null_mut(),\n            deleter:    lru_noop_deleter,\n            next_hash:  core::ptr::null_mut(),\n            next:       core::ptr::null_mut(), // will be set to self in LRUCacheInner::new\n            prev:       core::ptr::null_mut(), // will be set to self in LRUCacheInner::new\n            charge:     0,\n            key_length: 0,\n            in_cache:   false,\n            refs:       0,\n            hash:       0,\n            key_data:   [0u8; 1],\n        }\n    }\n\n    pub fn key(&self) -> Slice {\n        trace!(\"LRUHandle::key: key_length={}\", self.key_length);\n    \n        unsafe {\n            // next is only equal to this if the LRU handle is the list head of an\n            // empty list. List heads never have meaningful keys.\n            let self_ptr = self as *const LRUHandle as *mut LRUHandle;\n            assert!(\n                self.next != self_ptr,\n                \"LRUHandle::key called on list head / sentinel\"\n            );\n    \n            Slice::from_ptr_len(self.key_data_ptr(), self.key_length)\n        }\n    }\n\n    /// Pointer to the beginning of the inlined key bytes.\n    pub fn key_data_ptr(&self) -> *const u8 {\n        self.key_data.as_ptr()\n    }\n\n    pub fn value_ptr(&self) -> *mut c_void {\n        self.value\n    }\n\n    pub fn set_value_ptr(&mut self, value: *mut c_void) {\n        self.value = value;\n    }\n\n    pub fn deleter_fn(&self) -> fn(&Slice, *mut c_void) -> c_void {\n        self.deleter\n    }\n\n    pub fn set_deleter_fn(&mut self, deleter: fn(&Slice, *mut c_void) -> c_void) {\n        self.deleter = deleter;\n    }\n\n    pub fn next_hash_ptr(&self) -> *mut LRUHandle {\n        self.next_hash\n    }\n\n    pub fn set_next_hash_ptr(&mut self, ptr: *mut LRUHandle) {\n        self.next_hash = ptr;\n    }\n\n    /// Mutable reference to the `next_hash` link, used by HandleTable::find_pointer.\n    pub fn next_hash_link(&mut self) -> &mut *mut LRUHandle {\n        &mut self.next_hash\n    }\n\n    pub fn next_ptr(&self) -> *mut LRUHandle {\n        self.next\n    }\n\n    pub fn set_next_ptr(&mut self, ptr: *mut LRUHandle) {\n        self.next = ptr;\n    }\n\n    pub fn prev_ptr(&self) -> *mut LRUHandle {\n        self.prev\n    }\n\n    pub fn set_prev_ptr(&mut self, ptr: *mut LRUHandle) {\n        self.prev = ptr;\n    }\n\n    pub fn charge_value(&self) -> usize {\n        self.charge\n    }\n\n    pub fn set_charge_value(&mut self, charge: usize) {\n        self.charge = charge;\n    }\n\n    pub fn key_len(&self) -> usize {\n        self.key_length\n    }\n\n    pub fn set_key_length(&mut self, len: usize) {\n        self.key_length = len;\n    }\n\n    pub fn is_in_cache(&self) -> bool {\n        self.in_cache\n    }\n\n    pub fn set_in_cache(&mut self, in_cache: bool) {\n        self.in_cache = in_cache;\n    }\n\n    pub fn refs(&self) -> u32 {\n        self.refs\n    }\n\n    pub fn set_refs(&mut self, refs: u32) {\n        self.refs = refs;\n    }\n\n    pub fn increment_refs(&mut self) {\n        self.refs = self.refs.wrapping_add(1);\n    }\n\n    pub fn decrement_refs(&mut self) {\n        self.refs = self.refs.wrapping_sub(1);\n    }\n\n    pub fn hash_value(&self) -> u32 {\n        self.hash\n    }\n\n    pub fn set_hash_value(&mut self, hash: u32) {\n        self.hash = hash;\n    }\n\n    /// Pointer to the beginning of the inlined key bytes (mutable).\n    pub fn key_data_mut(&mut self) -> *mut u8 {\n        self.key_data.as_mut_ptr()\n    }\n}\n\nimpl HandleTable {\n\n    pub fn lookup(&mut self, key_: &Slice, hash_: u32) -> *mut LRUHandle {\n        trace!(\"HandleTable::lookup: hash={}\", hash_);\n        unsafe { *self.find_pointer(key_, hash_) }\n    }\n\n    /// Return a pointer to slot that points to a cache entry that matches key/hash.  \n    ///\n    /// If there is no such cache entry, return a pointer to the trailing slot in the corresponding\n    /// linked list.\n    pub fn find_pointer(&mut self, key_: &Slice, hash_: u32) -> *mut *mut LRUHandle {\n        trace!(\"HandleTable::find_pointer: hash={}\", hash_);\n    \n        unsafe {\n            debug_assert!(\n                *self.length() != 0,\n                \"HandleTable::find_pointer: length is zero\"\n            );\n    \n            let mask = self.length().wrapping_sub(1);\n            let mut ptr: *mut *mut LRUHandle =\n                self.list().add((hash_ & mask) as usize);\n    \n            loop {\n                let entry = *ptr;\n                if entry.is_null() {\n                    trace!(\n                        \"HandleTable::find_pointer: reached end of chain without match (hash={})\",\n                        hash_\n                    );\n                    break;\n                }\n    \n                let entry_hash = (*entry).hash_value();\n                if entry_hash == hash_ {\n                    if handle_table_keys_equal(entry as *const LRUHandle, key_) {\n                        trace!(\n                            \"HandleTable::find_pointer: found matching entry {:p} for hash={}\",\n                            entry,\n                            hash_\n                        );\n                        break;\n                    } else {\n                        trace!(\n                            \"HandleTable::find_pointer: hash match but key mismatch for entry {:p}, hash={}\",\n                            entry,\n                            hash_\n                        );\n                    }\n                } else {\n                    trace!(\n                        \"HandleTable::find_pointer: skipping entry {:p} with non-matching hash={}\",\n                        entry,\n                        entry_hash\n                    );\n                }\n    \n                let next_hash_ref: &mut *mut LRUHandle = (*entry).next_hash_link();\n                ptr = next_hash_ref as *mut *mut LRUHandle;\n            }\n    \n            ptr\n        }\n    }\n}\n\nimpl Default for HandleTable {\n\n    fn default() -> Self {\n        trace!(\"HandleTable::default: initializing\");\n    \n        let mut table = HandleTable {\n            length: 0,\n            elems:  0,\n            list:   core::ptr::null_mut(),\n        };\n    \n        table.resize();\n        table\n    }\n}\n\nimpl Drop for HandleTable {\n\n    fn drop(&mut self) {\n        trace!(\n            \"HandleTable::drop: length={}, elems={}\",\n            self.length,\n            self.elems\n        );\n    \n        unsafe {\n            if !self.list.is_null() {\n                libc::free(self.list as *mut libc::c_void);\n                self.list = core::ptr::null_mut();\n            }\n        }\n    }\n}\n\nimpl LRUCache {\n\n    pub fn insert(\n        &mut self,\n        key_:      &Slice,\n        hash_:     u32,\n        value:     *mut c_void,\n        charge:    usize,\n        deleter:   fn(key_: &Slice, value: *mut c_void) -> c_void\n    ) -> *mut CacheHandle\n    {\n        trace!(\n            \"LRUCache::insert: hash={}, charge={}, capacity={}\",\n            hash_,\n            charge,\n            self.capacity()\n        );\n    \n        let mut guard = self.mutex().borrow_mut();\n        let mut inner = guard.lock();\n    \n        unsafe {\n            let key_size: usize    = *key_.size();\n            let key_ptr: *const u8 = *key_.data();\n    \n            // Allocate handle with inlined key bytes (flexible tail).\n            let alloc_size = core::mem::size_of::<LRUHandle>() - 1 + key_size;\n            let e = libc::malloc(alloc_size) as *mut LRUHandle;\n    \n            if e.is_null() {\n                error!(\"LRUCache::insert: malloc failed, size={}\", alloc_size);\n                return core::ptr::null_mut();\n            }\n    \n            // Initialize the new handle.\n            (*e).set_value_ptr(value);\n            (*e).set_deleter_fn(deleter);\n            (*e).set_charge_value(charge);\n            (*e).set_key_length(key_size);\n            (*e).set_hash_value(hash_);\n            (*e).set_in_cache(false);\n            (*e).set_refs(1); // client handle that we're going to return\n            (*e).set_next_hash_ptr(core::ptr::null_mut());\n            (*e).set_next_ptr(core::ptr::null_mut());\n            (*e).set_prev_ptr(core::ptr::null_mut());\n    \n            // Copy key bytes into inlined storage.\n            core::ptr::copy_nonoverlapping(key_ptr, (*e).key_data_mut(), key_size);\n    \n            if *self.capacity() > 0 {\n                // Cache enabled: take an extra reference for the cache itself\n                // and put the entry on the in_use_ list.\n                (*e).increment_refs(); // refs: 1 (client) + 1 (cache) = 2\n                (*e).set_in_cache(true);\n    \n                let in_use_head: *mut LRUHandle = inner.in_use_head_mut();\n                lru_append_node(in_use_head, e);\n    \n                inner.add_usage(charge);\n    \n                // Insert into hash table; if there was an old entry with same key/hash,\n                // FinishErase it (drops cache ref, adjusts usage, frees if needed).\n                let old = inner.table_mut().insert(e);\n                let _ = finish_erase_inner(&mut inner, old);\n            } else {\n                // capacity == 0 → caching disabled. We still return a handle, but\n                // the entry is not put in any LRU structure or table, and must\n                // NOT be considered in-cache.\n                (*e).set_in_cache(false);\n                (*e).set_next_ptr(core::ptr::null_mut());\n                (*e).set_prev_ptr(core::ptr::null_mut());\n                (*e).set_next_hash_ptr(core::ptr::null_mut());\n            }\n    \n            // Evict from LRU_ while over capacity.\n            while inner.usage() > *self.capacity() {\n                let lru_head: *mut LRUHandle = inner.lru_head_mut();\n                let old: *mut LRUHandle      = (*lru_head).next_ptr();\n    \n                // If the LRU list is empty but usage_ is still high, we can't\n                // evict anything else; log and break.\n                if core::ptr::eq(old, lru_head) {\n                    warn!(\n                        \"LRUCache::insert: usage={} > capacity={} but LRU list is empty; \\\n                         breaking eviction loop\",\n                        inner.usage(),\n                        self.capacity()\n                    );\n                    break;\n                }\n    \n                let old_refs = (*old).refs();\n                if old_refs != 1 {\n                    warn!(\n                        \"LRUCache::insert: LRU entry has refs={} during eviction \\\n                         (expected 1); entry={:p}, hash={:#x}\",\n                        old_refs,\n                        old,\n                        (*old).hash_value()\n                    );\n                }\n    \n                // Remove from the hash table and finish erasing the entry\n                // (drops cache's reference, updates usage, and frees when needed).\n                let removed = inner\n                    .table_mut()\n                    .remove(&(*old).key(), (*old).hash_value());\n    \n                let erased = finish_erase_inner(&mut inner, removed);\n                if !erased {\n                    debug!(\n                        \"LRUCache::insert: FinishErase returned false during eviction \\\n                         (entry may already have been erased)\"\n                    );\n                    debug_assert!(erased);\n                }\n            }\n    \n            e as *mut CacheHandle\n        }\n    }\n}\n\nimpl HandleTable {\n\n    pub fn insert(&mut self, h: *mut LRUHandle) -> *mut LRUHandle {\n        unsafe {\n            let hash = (*h).hash_value();\n            let key  = (*h).key();\n    \n            trace!(\n                \"HandleTable::insert: handle={:p}, hash={}, elems={}, length={}\",\n                h,\n                hash,\n                self.elems(),\n                self.length()\n            );\n    \n            let ptr = self.find_pointer(&key, hash);\n            let old = *ptr;\n    \n            if old.is_null() {\n                (*h).set_next_hash_ptr(core::ptr::null_mut());\n            } else {\n                let next_hash = (*old).next_hash_ptr();\n                (*h).set_next_hash_ptr(next_hash);\n            }\n    \n            *ptr = h;\n    \n            if old.is_null() {\n                self.set_elems(self.elems().wrapping_add(1));\n                if self.elems() > self.length() {\n                    debug!(\n                        \"HandleTable::insert: resizing (elems={}, length={})\",\n                        self.elems(),\n                        self.length()\n                    );\n                    self.resize();\n                }\n            }\n    \n            old\n        }\n    }\n}\n\n\nThis crate has the following authors:\n[\n    \"klebs <none>\",\n]\nThis crate has the following rust edition:\n2021\nThis crate has the following license:\nMIT\nThis crate is held in the following repository:\nhttps://github.com/klebs6/bitcoin-rs\n","role":"user"}],"model":"gpt-5.1"}}
