{
  "crate_name": "bitcoin-locked-pool",
  "full_readme_markdown": "# bitcoin-locked-pool\n\nHigh‑assurance, page‑locked memory pool for secret material, extracted from Bitcoin Core's secure allocator design and implemented in Rust.\n\n> **Note**: This README was generated by an AI model. It may contain minor inaccuracies, but it should be a strong and mostly correct approximation of the crate's intent and API.\n\n---\n\n## Overview\n\n`bitcoin-locked-pool` provides an arena‑based allocator for **locked (non‑swappable) memory**, intended for cryptographic secrets such as private keys, seeds, and authentication credentials.\n\nIt builds on a pluggable `LockedPageAllocator` implementation (e.g. `mlock`/`VirtualLock` via the companion `bitcoin_locked_page_allocator` crate) to:\n\n- Reserve contiguous regions of page‑locked memory (`LockedPageArena`).\n- Allocate and free variable‑sized chunks from these arenas (`LockedPool`).\n- Expose a process‑wide singleton manager (`LockedPoolManager`) suitable for use in custom allocators and higher‑level abstractions.\n- Provide rigorous destruction ordering so that locked pages are always freed correctly and without use‑after‑free.\n\nThe design mirrors the original C++ Bitcoin Core secure memory allocator: metadata is kept **out of** locked memory so that the limited locked quota is reserved for actual secrets.\n\nThis crate does **not** itself implement a `GlobalAlloc` or STL‑compatible allocator, but is intended as the building block for such facilities.\n\n---\n\n## Core Concepts\n\n### Locked vs. pageable memory\n\nOn most operating systems, virtual memory pages can be swapped to disk under memory pressure. This is unacceptable for long‑lived secrets, because it leaves residues on persistent storage.\n\nA locked page allocator requests that specific ranges of memory be **pinned** in RAM (e.g. via `mlock(2)` on POSIX or `VirtualLock` on Windows). Systems usually impose a strict limit on the total lockable memory per process or per user.\n\n`bitcoin-locked-pool` is built around this constraint:\n\n- The first arena is capped by the process limit (if non‑zero) to ensure it can be fully locked.\n- Subsequent arenas are allocated as needed, but still go through the underlying `LockedPageAllocator`, which may lock all or none of the pages.\n- A callback lets the application **fail hard** or **continue with a warning** when locking fails.\n\n### Arena‑based design\n\n`LockedPool` manages a `Vec<LockedPageArena>`. Each `LockedPageArena` owns:\n\n- A raw base pointer (`*mut c_void`).\n- A byte size (`usize`).\n- A pointer back to the underlying `LockedPageAllocator` used to free the memory.\n- An internal `Arena` that does the sub‑allocation bookkeeping.\n\nAllocation strategy:\n\n1. Short‑circuit if the requested `size == 0` or exceeds a maximum arena size constant.\n2. Try to allocate from existing arenas.\n3. If all are full, ask the allocator for a **new locked arena**, then retry allocation from it.\n\nFreeing locates the owning arena by address range and delegates to its `Arena` instance.\n\n---\n\n## Crate Structure\n\n### Types\n\n#### `type LockingFailed_Callback = fn() -> bool;`\n\nA callback invoked when an arena allocation succeeds but **locking that memory fails**.\n\n- Return `true` to proceed with the allocation (non‑locked, but at least resident for now).\n- Return `false` to abort: the pool will immediately free the just‑allocated pages and propagate failure to the caller.\n\nThis allows you to decide at runtime whether to:\n\n- Strictly require page‑locking for all secret allocations; or\n- Degrade gracefully in constrained environments, while emitting logs/metrics.\n\n---\n\n#### `struct LockedPool`\n\nA pool for locked memory chunks. Fields (readable via the generated getters):\n\n- `allocator: Box<dyn LockedPageAllocator + Send + Sync>` – Strategy for allocating and freeing locked pages.\n- `arenas: Vec<LockedPageArena>` – Current set of arenas.\n- `lf_cb: Option<LockingFailed_Callback>` – Optional callback for locking failures.\n- `cumulative_bytes_locked: usize` – Aggregate bytes successfully locked across all arenas.\n- `mutex: Mutex<()>` – Internal synchronization for statistics and other multi‑arena operations.\n\n##### Construction\n\n```rust\nuse bitcoin_locked_page_allocator::{\n    PosixLockedPageAllocator,\n    // or Win32LockedPageAllocator on Windows\n};\nuse bitcoin_locked_pool::LockedPool;\n\nfn locking_failed() -> bool {\n    // Log, increment a metric, etc. Return false to enforce hard failure.\n    eprintln!(\"WARNING: unable to lock newly allocated arena; proceeding unlocked\");\n    true\n}\n\nlet allocator: Box<dyn bitcoin_locked_page_allocator::LockedPageAllocator + Send + Sync> =\n    Box::new(PosixLockedPageAllocator::default());\n\nlet mut pool = LockedPool::new(allocator, Some(locking_failed));\n```\n\n##### Allocation & free\n\n```rust\nuse core::ffi::c_void;\n\nlet p: *mut c_void = pool.alloc(64); // allocate 64 bytes\nif p.is_null() {\n    // Allocation failed (either full, oversize, or system error)\n} else {\n    unsafe {\n        // Use `p` as raw memory. Wrap it in your own abstraction that zeroizes on drop.\n    }\n\n    // Free when done; `null` is a no‑op.\n    pool.free(p);\n}\n```\n\nGuarantees and behavior:\n\n- `alloc(size == 0)` returns `null`.\n- `alloc(size > LOCKED_POOL_ARENA_SIZE)` returns `null`.\n- On failure to create or lock a new arena (with a `false` callback result), `alloc` returns `null`.\n- `free(null)` is a no‑op.\n- Freeing a pointer not belonging to any arena causes a `panic!`, surfacing allocator misuse early.\n\n##### Statistics\n\n```rust\nuse bitcoin_locked_pool::LockedPoolStats;\n\nlet stats: LockedPoolStats = pool.stats();\nprintln!(\n    \"used={} free={} total={} locked={} chunks_used={} chunks_free={}\",\n    stats.used(),\n    stats.free(),\n    stats.total(),\n    stats.locked(),\n    stats.chunks_used(),\n    stats.chunks_free(),\n);\n```\n\nThe `stats` call holds a mutex, aggregates per‑arena statistics, and returns an owned `LockedPoolStats` value.\n\n---\n\n#### `struct LockedPoolManager`\n\nA lazily‑initialized, process‑wide singleton around a `LockedPool`, intended for use as the backing storage for global secure allocators.\n\nKey properties:\n\n- `Send + Sync` – safe to share and access from multiple threads.\n- Uses `once_cell::sync::OnceCell` for one‑time initialization.\n- On Unix, uses `PosixLockedPageAllocator::default()`.\n- On Windows, uses `Win32LockedPageAllocator::default()`.\n\n##### Accessing the global pool\n\n```rust\nuse bitcoin_locked_pool::LockedPoolManager;\nuse core::ffi::c_void;\n\nlet manager: &LockedPoolManager = LockedPoolManager::instance();\n\n// `LockedPoolManager` implements `Deref<Target = LockedPool>`\nlet p: *mut c_void = manager.alloc(32);\n```\n\nFor mutating operations that need unique access to the manager itself, you can obtain `&'static mut LockedPoolManager` only via interior mutability or custom patterns; in most cases, you should treat `instance()` as giving you a shared reference and rely on any internal synchronization designed into higher‑level abstractions.\n\n---\n\n#### `struct LockedPageArena`\n\nRepresents a single contiguous region of memory obtained from `LockedPageAllocator`.\n\nConstruction is `unsafe` because the caller must guarantee that `base_in .. base_in + size_in` is valid for the arena's lifetime:\n\n```rust\nuse core::ffi::c_void;\nuse bitcoin_locked_pool::LockedPageArena;\n\nunsafe {\n    let raw_ptr: *mut c_void = /* from some low-level allocator */;\n    let allocator: *mut dyn bitcoin_locked_page_allocator::LockedPageAllocator = /* ... */;\n\n    let arena = LockedPageArena::new(allocator, raw_ptr, 4096, 8);\n    let p = arena.alloc(128);\n    arena.free(p);\n}\n```\n\nOn `Drop`, the arena calls `free_locked(base, size)` on its stored allocator pointer, ensuring that pages are both unlocked and freed.\n\nHigh‑level users typically do **not** construct arenas directly; `LockedPool::new_arena` encapsulates this logic alongside page locking and accounting.\n\n---\n\n#### `struct LockedPoolStats`\n\nImmutable snapshot of pool‑wide memory accounting:\n\n- `used: usize` – total bytes currently allocated to clients across all arenas.\n- `free: usize` – total free bytes still available for allocation.\n- `total: usize` – `used + free`, the overall capacity of all arenas combined.\n- `locked: usize` – cumulative bytes successfully locked via the underlying allocator.\n- `chunks_used: usize` – number of active allocation chunks.\n- `chunks_free: usize` – number of free chunks in the arenas' internal data structures.\n\nGenerated via `LockedPool::stats()` using `LockedPoolStatsBuilder`.\n\n---\n\n## Drop Ordering & Safety Considerations\n\nTwo key invariants are enforced:\n\n1. **Arenas must be destroyed before the allocator**.\n   - `LockedPool` implements a custom `Drop` that first `take`s the `arenas` vec, drops it, and only then allows the `allocator` field to be dropped.\n   - This prevents arenas from calling `free_locked` on a freed allocator instance, which would otherwise be undefined behavior.\n\n2. **Arena deallocation must use the original allocator**.\n   - Each `LockedPageArena` stores a raw `*mut dyn LockedPageAllocator` pointer.\n   - This pointer is assumed to remain valid for the arena's entire lifetime; the `LockedPool` drop sequence guarantees this.\n\nWhen building higher‑level abstractions (e.g., `ZeroizingBox`, `SecureVec<T>`), you should:\n\n- Ensure that logical lifetimes of clients are bound by the lifetime of the underlying pool/manager.\n- Aggressively zero memory before freeing, if you store secrets in these chunks.\n- Treat `*mut c_void` pointers as unsafe; encapsulate them in strongly‑typed wrappers.\n\n---\n\n## Integration Patterns\n\n### Custom secret container\n\nA typical use‑case is to wrap the raw pool allocations in a safe secret type:\n\n```rust\nuse bitcoin_locked_pool::LockedPoolManager;\nuse core::{ffi::c_void, ptr};\n\npub struct SecretBox {\n    ptr: *mut u8,\n    len: usize,\n}\n\nimpl SecretBox {\n    pub fn new(len: usize) -> Option<Self> {\n        let pool = LockedPoolManager::instance();\n        let raw = pool.alloc(len) as *mut u8;\n        if raw.is_null() {\n            return None;\n        }\n        unsafe { ptr::write_bytes(raw, 0, len); }\n        Some(Self { ptr: raw, len })\n    }\n\n    pub fn as_mut_slice(&mut self) -> &mut [u8] {\n        assert!(!self.ptr.is_null());\n        unsafe { core::slice::from_raw_parts_mut(self.ptr, self.len) }\n    }\n}\n\nimpl Drop for SecretBox {\n    fn drop(&mut self) {\n        if self.ptr.is_null() { return; }\n        unsafe {\n            core::ptr::write_bytes(self.ptr, 0, self.len);\n        }\n        let pool = LockedPoolManager::instance();\n        pool.free(self.ptr as *mut c_void);\n    }\n}\n```\n\nThis pattern localizes all `unsafe` usage, ensures zeroization, and leverages the global locked pool.\n\n---\n\n## Platform Behavior\n\n`LockedPoolManager::instance()` selects a platform‑appropriate allocator:\n\n- **Unix**: `PosixLockedPageAllocator`\n  - Typically uses `mlock`/`munlock` and may also apply `mlockall` or `MADV_DONTDUMP` semantics.\n- **Windows**: `Win32LockedPageAllocator`\n  - Typically uses `VirtualLock`/`VirtualUnlock`.\n\nObserve OS‐specific limits (e.g., `RLIMIT_MEMLOCK` on Unix). If the first arena cannot be fully locked due to a strict limit, the `LockingFailed_Callback` is invoked; you can decide whether to:\n\n- Refuse to operate without locked memory.\n- Continue with degraded security while tracking the condition.\n\n---\n\n## Error Handling Philosophy\n\n- **Hard logic errors** (e.g., freeing a pointer that does not belong to any arena) result in a `panic!`. This is a programming bug and should be detected during testing.\n- **Resource exhaustion** (e.g., pool full, OS refusing a new locked region) is signaled via `null` from `alloc`.\n- **Locking failure after successful allocation** is under your control via `LockingFailed_Callback`.\n\nThis approach keeps the core logic simple while allowing applications to layer richer error handling and telemetry as needed.\n\n---\n\n## Safety & `unsafe` Code\n\nThis crate uses low‑level, `unsafe` constructs:\n\n- Raw pointers to arenas and allocators.\n- Assumptions about allocator lifetime and memory validity.\n\nThe public API attempts to:\n\n- Confine `unsafe` to boundary construction (e.g., `LockedPageArena::new`).\n- Provide safe high‑level operations (`LockedPool::alloc/free`, `LockedPool::stats`) once the invariants are established.\n\nWhen extending the crate or integrating deeply:\n\n- Treat all raw pointers as **unsafe contracts**.\n- Ensure that any types built on top of this pool handle zeroization and destruction in a principled manner.\n\n---\n\n## License\n\nThis crate is licensed under the **MIT** license.\n\nSee the `LICENSE` file for details.\n",
  "package_categories": [
    "cryptography",
    "memory-management",
    "concurrency",
    "science"
  ],
  "package_description": "Arena-based, page-locked memory pool and singleton manager for cryptographic secrets, modeled after Bitcoin Core’s secure allocator and backed by pluggable OS-specific locked-page allocators.",
  "package_keywords": [
    "bitcoin",
    "locked-memory",
    "secure-allocation",
    "crypto",
    "key-management"
  ]
}