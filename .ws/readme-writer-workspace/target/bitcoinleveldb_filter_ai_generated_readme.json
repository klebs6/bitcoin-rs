{
  "crate_name": "bitcoinleveldb-filter",
  "full_readme_markdown": "# bitcoinleveldb-filter\n\nA low-level implementation of LevelDB-compatible filter blocks (Bloom filters) for `bitcoin-rs`, written in Rust.\n\nThis crate models the filter subsystem used by LevelDB to accelerate point lookups in SSTables (sorted string tables). It provides the primitives necessary to build and query filter blocks, while delegating the concrete filter policy (e.g., Bloom filter configuration) to user-defined implementations.\n\n> Note: This README.md was generated by an AI model and may not be 100% accurate, but it is intended to be a close and technically useful description of the crate.\n\n---\n\n## Overview\n\nLevelDB uses *filter blocks* (typically Bloom filters) to quickly determine whether a key **cannot** be present in a particular data block, substantially reducing I/O for negative lookups. Each SSTable has:\n\n- A **filter block**, consisting of one filter region per data block range.\n- A small trailer describing where the filter block lives and how to interpret it.\n\nThis crate focuses on the **filter block** part:\n\n- **`FilterBlockBuilder`**: constructs an in-memory filter block given a user-supplied filter policy and a stream of keys grouped by block offsets.\n- **`FilterBlockReader`**: parses a filter block and uses the policy to answer `key_may_match` queries for a given block offset.\n\nThe design is intentionally low-level and close to the original C++ LevelDB layout, so that filter blocks are bitwise-compatible with LevelDB and can be used in interoperable storage formats (such as Bitcoin Core’s LevelDB-backed chainstate and index databases).\n\nYou are expected to plug in a concrete filter policy (generally a Bloom filter) via the `FilterPolicy`, `CreateFilter`, and `KeyMayMatch` traits.\n\n## Core Traits\n\n### `FilterPolicy`\n\n```rust\npub trait FilterPolicy {}\n```\n\n`FilterPolicy` is a marker trait representing a particular filter strategy.\n\nIn practice you will implement additional traits (`CreateFilter`, `KeyMayMatch`) for the same type, so that a single policy type both *creates* filters and *queries* them. The crate’s `FilterBlockBuilder` and `FilterBlockReader` store a `Box<dyn FilterPolicy>`; your type must be downcast or enriched to also implement the necessary behavior.\n\n### `CreateFilter`\n\n```rust\npub trait CreateFilter {\n    fn create_filter(\n        &self,\n        keys:  *const Slice,\n        n:     i32,\n        dst:   &mut Vec<u8>\n    );\n}\n```\n\nThis trait encapsulates the construction of a single filter region:\n\n- `keys`: pointer to an array of `Slice` values (each slice is a key).\n- `n`: number of keys in the array.\n- `dst`: output buffer where the concrete filter representation is appended.\n\nTypical implementation for a Bloom filter will:\n\n1. Hash each key multiple times.\n2. Set bits in a bitset sized according to `bits_per_key * n`.\n3. Serialize the bitset (and possibly some metadata, such as number of probes) into `dst`.\n\n### `KeyMayMatch`\n\n```rust\npub trait KeyMayMatch {\n    fn key_may_match(&self, key: &Slice, filter: &Slice) -> bool;\n}\n```\n\nThis trait encapsulates querying a single filter region:\n\n- `key`: the key to test.\n- `filter`: serialized filter region for some key range.\n\nReturn semantics:\n\n- `false`: the key is *definitely not* present in this region.\n- `true`: the key *may* be present (false positives allowed; false negatives are **not** allowed for a correct Bloom filter implementation).\n\nIn Bloom-filter terms, `key_may_match` checks that all probe bits corresponding to the key are set in the underlying bitset.\n\n## Filter Block Construction\n\n### `FilterBlockBuilder`\n\n```rust\n#[derive(Getters, Setters)]\npub struct FilterBlockBuilder  {\n    policy:         Box<dyn FilterPolicy>,\n    keys:           Vec<u8>,\n    start:          Vec<usize>,\n    result:         Vec<u8>,\n    tmp_keys:       Vec<Slice>,\n    filter_offsets: Vec<u32>,\n}\n```\n\nThe builder aggregates keys per logical data block and finally emits a single contiguous filter block buffer.\n\n#### Layout\n\nThe produced filter block has the same high-level layout as in LevelDB:\n\n```text\n[filter_0][filter_1]...[filter_(n-1)][offset_array][array_offset][base_lg]\n```\n\n- `filter_i`: opaque bytes produced by the `FilterPolicy` for range `i`.\n- `offset_array`: `n` little-endian `u32` values; `offset_array[i]` is the start offset of `filter_i` within the block.\n- `array_offset`: `u32` little-endian, byte offset where `offset_array` begins.\n- `base_lg`: `u8`, log2 of the filter base (block alignment in bytes) used to compute region indices.\n\nBecause the policy is policy-specific, the crate does not prescribe the inner layout of each `filter_i` region; it merely provides the surrounding indexing structure.\n\n#### Constructor\n\n```rust\nimpl FilterBlockBuilder {\n    pub fn new(policy: Box<dyn FilterPolicy>) -> Self { ... }\n}\n```\n\nCreates a new builder bound to a specific filter policy.\n\n#### `start_block`\n\n```rust\npub fn start_block(&mut self, block_offset: u64)\n```\n\nIndicates that keys subsequently added belong to the data block whose *file-level* offset is `block_offset`.\n\n- `FILTER_BASE` (constant, external to this snippet) defines the granularity at which filters are grouped. The filter index is computed as:\n\n  ```rust\n  let filter_index = (block_offset / FILTER_BASE as u64) as usize;\n  ```\n\n- If `filter_index` skips ahead relative to the current number of filters, empty filters are generated for intermediate indices via `generate_filter()`.\n\nThis mirrors LevelDB’s logic where multiple neighboring data blocks may share the same filter region.\n\n#### `add_key`\n\n```rust\npub fn add_key(&mut self, key_: &Slice)\n```\n\nAdds a key to the current, not-yet-flushed filter batch.\n\n- The raw key bytes are appended to `self.keys`.\n- `self.start` records the start index of each key within that buffer.\n\n`Slice` is assumed to be a lightweight pointer+length structure (from the surrounding codebase), and here it is reinterpreted as a byte slice using `from_raw_parts`.\n\n#### `finish`\n\n```rust\npub fn finish(&mut self) -> Slice\n```\n\nFinalizes all filters and returns a `Slice` view over the resulting buffer.\n\nProcess:\n\n1. If there are unflushed keys (tracked via `self.start`), the builder calls `generate_filter()` once more to produce the last `filter_i` region.\n2. Append all `filter_offsets` as little-endian `u32` values.\n3. Append `array_offset` (the index where `filter_offsets` begins) as `u32`.\n4. Append `FILTER_BASE_LG` as `u8`.\n\nThe returned `Slice` points into `self.result`. It is your responsibility to ensure `self.result` outlives this slice.\n\n### Low-level Utilities\n\n#### `put_fixed32`\n\n```rust\npub fn put_fixed32(dst: &mut Vec<u8>, value: u32)\n```\n\nAppends a 32-bit little-endian integer to `dst`. Used to encode offsets and `array_offset`.\n\n#### `decode_fixed32`\n\n```rust\npub fn decode_fixed32(src: &[u8]) -> u32\n```\n\nDecodes a 32-bit little-endian integer from the first 4 bytes of `src`. Caller must ensure at least 4 bytes are available.\n\n## Filter Block Querying\n\n### `FilterBlockReader`\n\n```rust\n#[derive(Getters, Setters)]\npub struct FilterBlockReader  {\n    policy:  Box<dyn FilterPolicy>,\n    data:    Arc<[u8]>,\n    offset:  usize,\n    num:     usize,\n    base_lg: usize,\n    valid:   bool,\n}\n```\n\nThe reader parses a serialized filter block and exposes `key_may_match` per block offset.\n\n- `policy`: same policy implementation used at build time (or at least compatible with the serialized format).\n- `data`: reference-counted backing storage for the filter block bytes.\n- `offset`: byte offset of the offsets array.\n- `num`: number of filter regions.\n- `base_lg`: log2 of the filter base (i.e., `FILTER_BASE = 1 << base_lg`).\n- `valid`: whether parsing was successful.\n\n#### Constructor\n\n```rust\nimpl FilterBlockReader {\n    pub fn new(policy: Box<dyn FilterPolicy>, contents: &Slice) -> Self { ... }\n}\n```\n\nParsing algorithm:\n\n1. Copy `contents` into an `Arc<[u8]>`.\n2. If total length `n < 5`, mark invalid (insufficient space for `array_offset + base_lg`).\n3. Read `base_lg` as the last byte (`data[n - 1]`).\n4. Read `last_word = decode_fixed32(&data[n - 5..n - 1])` as the start offset of the offsets array.\n5. Validate `last_word <= n - 5`; otherwise mark invalid.\n6. Let `offset = last_word` and `num = (n - 5 - last_word) / 4`.\n7. Mark as valid.\n\nThis reconstructs the same metadata the builder appended during `finish`.\n\n#### `key_may_match`\n\n```rust\npub fn key_may_match(&self, block_offset: u64, key: &Slice) -> bool\n```\n\nEvaluates whether `key` may be present in the filter associated with the given `block_offset`.\n\nProcess:\n\n1. If `self.valid` is `false`, conservatively return `true` (cannot safely refute membership).\n2. Compute the filter index:\n\n   ```rust\n   let index = (block_offset >> self.base_lg) as usize;\n   ```\n\n3. If `index >= self.num`, return `true` (out-of-range; treat as potential match).\n4. Read the start and limit offsets for this index from the offsets array:\n\n   ```rust\n   let idx1 = self.offset + index * 4;\n   let start = decode_fixed32(&self.data[idx1..idx1 + 4]) as usize;\n   let limit = decode_fixed32(&self.data[idx1 + 4..idx1 + 8]) as usize;\n   ```\n\n5. If `start == limit`, region is empty ⇒ return `false`.\n6. If `start <= limit && limit <= self.offset`, the filter region is `data[start..limit]`.\n7. Wrap this region in a `Slice` and delegate to `self.policy.key_may_match(key, &filter_slice)`.\n\nThe contract is: reader handles indexing and bounds; the policy handles the semantics of the filter.\n\n## Bloom Filter Policy Stub\n\n```rust\npub fn new_bloom_filter_policy(_bits_per_key_: i32) -> Box<dyn FilterPolicy> {\n    unimplemented!(\"new_bloom_filter_policy is not yet implemented\");\n}\n```\n\nThis function is a placeholder intended to produce a concrete Bloom filter policy implementation. In canonical LevelDB, the Bloom filter policy:\n\n- Uses `k ≈ ln(2) * bits_per_key` hash functions.\n- Achieves false positive probability approximately:\n\n  \\[\n  p \\approx (1 - e^{-k n / m})^{k}\n  \\]\n\n  where \\(n\\) is the number of keys, \\(m\\) is the total number of bits (\\(m = \\text{bits\\_per\\_key} \\times n\\)).\n\nTo make this function functional, you should:\n\n1. Define a type, e.g. `struct BloomFilterPolicy { bits_per_key: i32, k: u8, ... }`.\n2. Implement `FilterPolicy` for it (marker).\n3. Implement `CreateFilter` and `KeyMayMatch` using a robust hashing scheme (e.g., MurmurHash or a suitable 64-bit hash folded into multiple probes).\n4. Pack per-region Bloom filter bits and any parameters necessary to decode them.\n5. Return `Box::new(BloomFilterPolicy { ... })` from `new_bloom_filter_policy`.\n\nYou must ensure that the encoding/decoding conventions of your policy are self-consistent between builder and reader.\n\n## Example Usage\n\n> The following is conceptual and assumes you supply a concrete `BloomFilterPolicy` type implementing `FilterPolicy + CreateFilter + KeyMayMatch` and exposing a constructor used by `new_bloom_filter_policy`.\n\n### Building a Filter Block\n\n```rust\nuse bitcoinleveldb_filter::FilterBlockBuilder;\nuse bitcoinleveldb_filter::new_bloom_filter_policy;\n// use crate::Slice; // from the surrounding bitcoin-rs/leveldb implementation\n\nfn build_filter_block(blocks: Vec<(u64, Vec<Slice>)>) -> Slice {\n    let policy = new_bloom_filter_policy(10); // e.g., 10 bits per key\n    let mut builder = FilterBlockBuilder::new(policy);\n\n    for (block_offset, keys) in blocks {\n        builder.start_block(block_offset);\n        for key in &keys {\n            builder.add_key(key);\n        }\n    }\n\n    let filter_block = builder.finish();\n    filter_block\n}\n```\n\n### Querying a Filter Block\n\n```rust\nuse bitcoinleveldb_filter::FilterBlockReader;\nuse bitcoinleveldb_filter::new_bloom_filter_policy;\n\nfn probe_key(filter_block: &Slice, block_offset: u64, key: &Slice) -> bool {\n    let policy = new_bloom_filter_policy(10); // must match the writer\n    let reader = FilterBlockReader::new(policy, filter_block);\n\n    reader.key_may_match(block_offset, key)\n}\n```\n\nIn an integrated LevelDB-like system, you will:\n\n1. Build filter blocks while constructing SSTables.\n2. Persist filter blocks alongside data and index blocks.\n3. Reload the filter block with `FilterBlockReader` when opening the table.\n4. Use `key_may_match` as a fast pre-check before binary searching or performing disk I/O for the data block.\n\n## Safety and Invariants\n\n- `add_key` uses `unsafe` to reinterpret a `Slice` as `&[u8]`. The correctness of that operation depends on `Slice` containing a valid pointer and length for the lifetime of the builder.\n- `decode_fixed32` assumes that `src.len() >= 4`; misuse is undefined behavior.\n- `finish` returns a `Slice` borrowing from `self.result`; do not drop or mutate `FilterBlockBuilder` in ways that invalidate `self.result` while that `Slice` is in use.\n- `FilterBlockReader::new` copies the input bytes into an `Arc<[u8]>`, so the `Slice` passed to it does not need to outlive the reader.\n\n## Integration Context\n\nThis crate is part of `bitcoin-rs`:\n\n- Repository: <https://github.com/klebs6/bitcoin-rs>\n- Likely used by the LevelDB reimplementation or bindings inside that repository to support Bitcoin-compatible storage layouts.\n\nWhen integrating, keep the following aligned:\n\n- `FILTER_BASE` and `FILTER_BASE_LG` must match between the writer and reader.\n- The same `FilterPolicy` implementation (or a wire-compatible version) must be used to create and query filter regions.\n- Any on-disk format expectations (e.g., Bitcoin Core’s) must be reflected in the policy’s encoding strategy.\n\n## License and Metadata\n\n- **License**: MIT\n- **Edition**: Rust 2021\n- **Authors**: `klebs <none>`\n\nThis design emphasizes low-level control and compatibility with existing LevelDB-based ecosystems while remaining idiomatic enough to compose nicely with the rest of `bitcoin-rs`.\n",
  "package_categories": [
    "database",
    "database-implementations",
    "algorithms",
    "encoding",
    "science"
  ],
  "package_description": "LevelDB-compatible filter block (Bloom filter) builder and reader used by bitcoin-rs, exposing low-level traits and utilities for constructing and querying per-block filters over SSTable-style key ranges.",
  "package_keywords": [
    "leveldb",
    "bloom-filter",
    "bitcoin",
    "filter-block",
    "sstable"
  ]
}