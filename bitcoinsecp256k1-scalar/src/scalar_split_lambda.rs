// ---------------- [ File: bitcoinsecp256k1-scalar/src/scalar_split_lambda.rs ]
/*!
  | Find r1 and r2 such that r1+r2*lambda
  | = k, where r1 and r2 or their negations
  | are maximum 128 bits long (see ge_mul_lambda).
  |
  */

crate::ix!();

/**
 | Find r1 and r2 given k, such that 
 | r1 + r2 * lambda == k mod n; 
 |
 | unlike in the full case we don't bother making
 | r1 and r2 be small, we just want them to be
 | nontrivial to get full test coverage for the
 | exhaustive tests. 
 |
 | We therefore (arbitrarily) set 
 | r2 = k + 5 (mod n) and 
 | r1 = k - r2 * lambda (mod n).
 */
#[cfg(feature="exhaustive-test-order")]
pub fn scalar_split_lambda(r1: *mut Scalar, r2: *mut Scalar, k: *const Scalar) {
    unsafe {
        *r2 = ((*k).wrapping_add(5)) % EXHAUSTIVE_TEST_ORDER;
        *r1 = ((*k).wrapping_add((EXHAUSTIVE_TEST_ORDER - *r2).wrapping_mul(EXHAUSTIVE_TEST_LAMBDA)))
            % EXHAUSTIVE_TEST_ORDER;
    }
}

/**
 | Both lambda and beta are primitive cube roots
 | of unity.  
 |
 | That is lamba^3 == 1 mod n and beta^3 == 1 mod
 | p, where n is the curve order and p is the
 | field order.
 |
 | Futhermore, because (X^3 - 1) = (X - 1)(X^2
 | + X + 1), the primitive cube roots of unity are
 | roots of X^2 + X + 1.  
 |
 | Therefore lambda^2 + lamba == -1 mod n and
 | beta^2 + beta == -1 mod p.
 |
 | (The other primitive cube roots of unity are
 | lambda^2 and beta^2 respectively.)
 |
 | Let l = -1/2 + i*sqrt(3)/2, 
 | the complex root of X^2 + X + 1. 
 |
 | We can define a ring homomorphism phi : Z[l] ->
 | Z_n where phi(a + b*l) == a + b*lambda mod n. 
 |
 | The kernel of phi is a lattice over Z[l]
 | (considering Z[l] as a Z-module). 
 |
 | This lattice is generated by a reduced basis
 | {a1 + b1*l, a2 + b2*l} where
 |
 | - a1 =      {0x30,0x86,0xd2,0x21,0xa7,0xd4,0x6b,0xcd,0xe8,0x6c,0x90,0xe4,0x92,0x84,0xeb,0x15}
 | - b1 =     -{0xe4,0x43,0x7e,0xd6,0x01,0x0e,0x88,0x28,0x6f,0x54,0x7f,0xa9,0x0a,0xbf,0xe4,0xc3}
 | - a2 = {0x01,0x14,0xca,0x50,0xf7,0xa8,0xe2,0xf3,0xf6,0x57,0xc1,0x10,0x8d,0x9d,0x44,0xcf,0xd8}
 | - b2 =      {0x30,0x86,0xd2,0x21,0xa7,0xd4,0x6b,0xcd,0xe8,0x6c,0x90,0xe4,0x92,0x84,0xeb,0x15}
 |
 | "Guide to Elliptic Curve Cryptography"
 | (Hankerson, Menezes, Vanstone) gives an
 | algorithm (algorithm 3.74) to find k1 and k2
 | given k, such that k1 + k2 * lambda == k mod n,
 | and k1 and k2 are small in absolute value.
 |
 | The algorithm computes 
 | c1 = round(b2 * k / n) and 
 | c2 = round((-b1) * k / n), and gives
 | k1 = k - (c1*a1 + c2*a2) and 
 | k2 = -(c1*b1 + c2*b2). 
 |
 | Instead, we use modular arithmetic, and compute 
 | r2 = k2 mod n, and 
 | r1 = k1 mod n = (k - r2 * lambda) mod n, 
 | avoiding the need for the constants a1 and a2.
 |
 | g1, g2 are precomputed constants used to
 | replace division with a rounded multiplication
 | when decomposing the scalar for an
 | endomorphism-based point multiplication.
 |
 | The possibility of using precomputed estimates
 | is mentioned in "Guide to Elliptic Curve
 | Cryptography" (Hankerson, Menezes, Vanstone) in
 | section 3.5.
 |
 | The derivation is described in the paper
 | "Efficient Software Implementation of
 | Public-Key Cryptography on Sensor Networks
 | Using the MSP430X Microcontroller" (Gouvea,
 | Oliveira, Lopez), Section 4.3 (here we use
 | a somewhat higher-precision estimate):
 |
 | d = a1*b2 - b1*a2
 | g1 = round(2^384 * b2/d)
 | g2 = round(2^384 * (-b1)/d)
 |
 | (Note that d is also equal to the curve order,
 | n, here because [a1,b1] and [a2,b2] can be
 | found as outputs of the Extended Euclidean
 | Algorithm on inputs n and lambda).
 |
 | The function below splits k into r1 and r2, such that
 |
 | - r1 + lambda * r2 == k (mod n)
 | - either r1 < 2^128 or -r1 mod n < 2^128
 | - either r2 < 2^128 or -r2 mod n < 2^128
 |
 | See proof below.
 */
#[cfg(not(feature="exhaustive-test-order"))]
pub fn scalar_split_lambda(r1: *mut Scalar, r2: *mut Scalar, k: *mut Scalar) {
    unsafe {
        let mut c1: Scalar = Scalar::new();
        let mut c2: Scalar = Scalar::new();

        const minus_b1: Scalar = scalar_const!(
            0x00000000,
            0x00000000,
            0x00000000,
            0x00000000,
            0xE4437ED6,
            0x010E8828,
            0x6F547FA9,
            0x0ABFE4C3
        );
        const minus_b2: Scalar = scalar_const!(
            0xFFFFFFFF,
            0xFFFFFFFF,
            0xFFFFFFFF,
            0xFFFFFFFE,
            0x8A280AC5,
            0x0774346D,
            0xD765CDA8,
            0x3DB1562C
        );
        const g1: Scalar = scalar_const!(
            0x3086D221,
            0xA7D46BCD,
            0xE86C90E4,
            0x9284EB15,
            0x3DAA8A14,
            0x71E8CA7F,
            0xE893209A,
            0x45DBB031
        );
        const g2: Scalar = scalar_const!(
            0xE4437ED6,
            0x010E8828,
            0x6F547FA9,
            0x0ABFE4C4,
            0x221208AC,
            0x9DF506C6,
            0x1571B4AE,
            0x8AC47F71
        );

        verify_check!(r1 != k);
        verify_check!(r2 != k);

        /* these _var calls are constant time since the shift amount is constant */
        scalar_mul_shift_var(&mut c1, k, &g1, 384);
        scalar_mul_shift_var(&mut c2, k, &g2, 384);

        scalar_mul(&mut c1, &c1, &minus_b1);
        scalar_mul(&mut c2, &c2, &minus_b2);
        scalar_add(r2, &c1, &c2);

        scalar_mul(r1, r2, &*const_lambda);
        scalar_negate(r1, r1);
        scalar_add(r1, r1, k);

        #[cfg(feature="secp256k1-verify")]
        {
            scalar_split_lambda_verify(r1, r2, k);
        }
    }
}

#[cfg(test)]
#[cfg(not(feature = "exhaustive-test-order"))]
mod scalar_split_lambda_contracts {
    use super::*;
    use crate::scalar_test_support::*;
    use tracing::{debug, info, trace};

    fn is_lt_2_128(be: &[u8; 32]) -> bool {
        be[0..16].iter().all(|&b| b == 0)
    }

    #[traced_test]
    fn scalar_split_lambda_recombines_and_meets_128_bit_bounds() {
        info!("validating scalar_split_lambda recombination and 128-bit bounds");

        let lambda = &*const_lambda;

        let vectors: &[[u8; 32]] = &[
            SCALAR_ZERO_BE,
            SCALAR_ONE_BE,
            SCALAR_TWO_BE,
            SCALAR_THREE_BE,
            SCALAR_MAX_U32_BE,
            SECP256K1_ORDER_HALF_BE,
            SECP256K1_ORDER_MINUS_1_BE,
        ];

        for (i, k_be) in vectors.iter().enumerate() {
            let mut k = scalar_from_be_bytes(k_be);

            let mut r1 = scalar_zero_value();
            let mut r2 = scalar_zero_value();

            unsafe {
                scalar_split_lambda(
                    &mut r1 as *mut Scalar,
                    &mut r2 as *mut Scalar,
                    &mut k as *mut Scalar,
                );
            }

            let r1_be = scalar_to_be_bytes(&r1);
            let r2_be = scalar_to_be_bytes(&r2);

            // Check r1 + lambda*r2 == k (mod n).
            let mut s = scalar_zero_value();
            unsafe {
                scalar_mul(
                    &mut s as *mut Scalar,
                    lambda as *const Scalar,
                    &r2 as *const Scalar,
                );
                let _ov = scalar_add(
                    &mut s as *mut Scalar,
                    &s as *const Scalar,
                    &r1 as *const Scalar,
                );
                assert_ne!(scalar_eq(&s as *const Scalar, &k as *const Scalar), 0);
            }

            // Check bound: either r < 2^128 or -r mod n < 2^128 for both r1 and r2.
            let r1_neg = be_neg_mod_n(&r1_be);
            let r2_neg = be_neg_mod_n(&r2_be);

            trace!(i, ?k_be, ?r1_be, ?r2_be, "split_lambda outputs");
            debug!(i, r1_small = is_lt_2_128(&r1_be), r1_neg_small = is_lt_2_128(&r1_neg), "r1 bound");
            debug!(i, r2_small = is_lt_2_128(&r2_be), r2_neg_small = is_lt_2_128(&r2_neg), "r2 bound");

            assert!(is_lt_2_128(&r1_be) || is_lt_2_128(&r1_neg));
            assert!(is_lt_2_128(&r2_be) || is_lt_2_128(&r2_neg));
        }
    }
}
