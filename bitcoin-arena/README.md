# bitcoin-arena

High‑performance, pointer‑driven arena allocator for pre‑reserved memory regions, designed for latency‑sensitive systems such as Bitcoin node implementations, mempools, UTXO caches, and other blockchain infrastructure components.

> **Note**: This README was generated by an AI model. It may not be perfectly accurate in every detail, but it is intended to be a close and useful approximation of the crate's intent and interface.

---

## Overview

`bitcoin-arena` provides an explicit, low‑level arena allocator operating over a caller‑supplied contiguous memory region. The core idea is:

- You **own the backing memory** (for example, a large `Vec<u8>`, mmap region, or shared buffer),
- You construct an `Arena` over it via `Arena::new`,
- You obtain and release raw pointers into that region using `alloc` / `free`,
- The allocator performs **O(log n) best‑fit allocation** with **coalescing free blocks**, keeping fragmentation low while staying predictable.

This is targeted at sophisticated consumers who need to:

- Avoid the global allocator for performance isolation or determinism,
- Manage memory with tight control in a single region (e.g. cache‑friendly structures, memory pools, arenas for graph / transaction data),
- Integrate with FFI or existing C/C++ systems that operate on raw pointers.

The implementation emphasizes:

- **Strict safety contracts** enforced at the API boundary,
- **Logarithmic‑time operations** on insertion/removal of free chunks using `BTreeMap` + `BTreeSet`,
- **Left/right coalescing** to reduce fragmentation,
- **Instrumentation hooks** (`stats`, optional `walk`) to observe allocator state.

This crate does **not** implement the `GlobalAlloc` trait; instead, it is an explicit arena that you integrate where you need full control.

---

## Core types

```rust
pub type ArenaSizeToChunkSortedMap = multimap::MultiMap<usize, *mut u8>;

pub type ArenaSizeToChunkSortedMapIterator = Box<dyn Iterator<Item = (usize, *mut u8)>>;

pub type ArenaChunkToSizeMap = std::collections::HashMap<*mut u8, ArenaSizeToChunkSortedMapIterator>;
```

These alias types describe auxiliary data structures relating chunk sizes and addresses. They are useful if you want to build higher‑level allocators, statistics, or debugging tools on top of the raw arena.

### `ArenaStats`

```rust
#[derive(Getters, Builder, Default, Debug, Clone)]
#[builder(setter(into, strip_option), pattern = "owned")]
#[getset(get = "pub")]
pub struct ArenaStats  {
    used:        usize,
    free:        usize,
    total:       usize,
    chunks_used: usize,
    chunks_free: usize,
}
```

`ArenaStats` is a cheap snapshot of the allocator state:

- `used`: Total bytes currently allocated.
- `free`: Total bytes currently available.
- `total`: `used + free`, i.e. arena capacity tracked by the allocator.
- `chunks_used`: Number of allocated chunks.
- `chunks_free`: Number of free chunks (after coalescing).

Instances are created via `Arena::stats()` using the generated `ArenaStatsBuilder`.

### `Arena`

```rust
#[no_copy]
pub struct Arena  {
    free_by_offset: BTreeMap<usize /*offset*/, usize /*size*/>,
    free_by_size:   BTreeMap<usize /*size*/, BTreeSet<usize /*offset*/>>,
    used_chunks:    HashMap<usize /*offset*/, usize /*size*/>,
    base:           *mut u8,
    end:            *mut u8,
    alignment:      usize,
}
```

The arena is described purely in terms of **offsets** and **sizes** over a caller‑supplied `[base, base + size)` memory interval.

- `free_by_offset`: used for **coalescing** neighboring free blocks; ordered by offset.
- `free_by_size`: used for **best‑fit search** in `O(log n)` by size.
- `used_chunks`: mapping from allocation offset to size, used for validation and statistics.
- `base`, `end`: raw pointers bounding the arena.
- `alignment`: enforced alignment in bytes for all allocations.

The allocator maintains the class invariants:

- All free regions are **non‑overlapping** and **sorted by offset**.
- `free_by_offset` and `free_by_size` represent the same set of free blocks.
- `used_chunks` describes all current allocations; any `free` must correspond to an entry in `used_chunks`.

---

## Safety model

The API is intentionally low‑level and uses raw pointers extensively. You are responsible for maintaining several invariants.

### Construction

```rust
impl Arena {
    /// Safety contract: `base_in … base_in+size_in` must remain valid
    /// for the lifetime of the `Arena`.
    pub fn new(base_in: *mut c_void, size_in: usize, alignment_in: usize) -> Self { /* … */ }
}
```

**Safety contract for `Arena::new`:**

- The memory range `[base_in, base_in + size_in)` must be valid for the *entire* lifetime of the `Arena`.
- No other component may mutate or deallocate that region in a way that would invalidate pointers.
- Alignment must be a power of two appropriate for your target data layout. The allocator uses an `align_up` function; if `alignment_in` is inconsistent with the data you place into the arena, you may introduce undefined behavior at the consumer level.

If any part of this contract is violated, later dereferences of pointers returned by the arena may trigger undefined behavior.

### Allocation

```rust
pub fn alloc(&mut self, size: usize) -> *mut c_void
```

- `size` is rounded up with `align_up(size, self.alignment)`.
- If the aligned size is zero, a null pointer is returned (logged as such).
- The allocator performs a **best‑fit search** via `free_by_size.range(size..).next()`. This yields the smallest free chunk of size ≥ `size` in `O(log n)`.
- A chunk is carved from the **tail** of that free block:
  - The free range `[offset, offset + best_size)` becomes: `[offset, offset + remaining)` + `[offset + remaining, offset + best_size)`.
  - The remaining part (if non‑zero) is re‑inserted into the free structures.
  - The allocated part is registered into `used_chunks` keyed by its starting offset.

**Failure semantics:**

- If no free block is large enough, `alloc` returns `std::ptr::null_mut()`.
- The allocator does **not** panic on allocation failure; the caller must check for null.

### Deallocation

```rust
pub fn free(&mut self, ptr: *mut c_void)
```

- A null pointer is accepted and treated as a **no‑op**.
- A non‑null pointer must have been returned by a previous successful `alloc` call that has not yet been freed.
- The allocator validates:
  - `address_in_arena(ptr)` must hold; otherwise it panics with `"pointer outside arena"`.
  - The pointer must correspond to a known allocation (by offset) in `used_chunks`; otherwise it panics with `"Arena: invalid or double free"`.

Deallocation algorithm:

1. Compute `offset = (ptr as usize) - (self.base as usize)`.
2. Remove the chunk from `used_chunks`, obtaining its size.
3. **Coalesce left**:
   - Find the free block with maximal offset `< offset`.
   - If it ends exactly at `offset`, merge it into the reclaiming block by removing it and extending `size`, and updating `offset` to the left blockʼs start.
4. **Coalesce right**:
   - Find the free block with minimal offset `> offset`.
   - If it starts exactly at `offset + size`, merge symmetrically.
5. Insert the resulting merged free block into `free_by_offset` and `free_by_size`.

These steps ensure that free ranges remain maximally coalesced, minimizing fragmentation.

### Address checks

```rust
pub fn address_in_arena(&self, ptr: *mut c_void) -> bool
```

This helper checks whether a pointer is within `[base, end)`. It is used to validate `free` and can also be useful for external code that needs to determine whether a pointer originates from this arena or from somewhere else.

---

## Statistics and debugging

### `stats`

```rust
pub fn stats(&self) -> ArenaStats
```

Collects an `O(chunks)` view of allocator state:

- Computes `used` as the sum of all `used_chunks` sizes.
- Computes `free` as the sum of all `free_by_offset` sizes.
- Derives `total = used + free`.
- Counts `chunks_used` and `chunks_free`.

This is useful for:

- Detecting leaks or unexpected retention,
- Monitoring internal fragmentation (e.g., `total` constant but `chunks_free` growing),
- Observability hooks in performance‑critical systems.

Example:

```rust
let stats = arena.stats();
println!(
    "used={} free={} total={} chunks_used={} chunks_free={}",
    stats.used(),
    stats.free(),
    stats.total(),
    stats.chunks_used(),
    stats.chunks_free(),
);
```

### `walk` (conditional)

```rust
#[cfg(ARENA_DEBUG)]
pub fn walk(&self) { /* … */ }
```

When compiled with `ARENA_DEBUG`, `walk` emits a human‑readable dump of the arena state:

- Lists all used chunks as `offset + size`,
- Lists all free chunks similarly.

This is primarily intended for interactive debugging or deep inspection of fragmentation patterns.

---

## Example usage

Below is an illustrative (unsafe) example showing how to back an `Arena` with a `Vec<u8>`.

```rust
use bitcoin_arena::Arena;
use std::ffi::c_void;

fn main() {
    // Backing storage: 1 MiB
    let mut backing = vec![0u8; 1024 * 1024];

    let base_ptr = backing.as_mut_ptr() as *mut c_void;
    let size = backing.len();
    let alignment = 8; // 8‑byte alignment for general data

    let mut arena = unsafe { Arena::new(base_ptr, size, alignment) };

    // Allocate 256 bytes
    let p1 = arena.alloc(256);
    assert!(!p1.is_null());

    // Allocate 128 bytes
    let p2 = arena.alloc(128);

    // Use the memory (cast as needed, respecting alignment)
    unsafe {
        let slice = std::slice::from_raw_parts_mut(p1 as *mut u8, 256);
        slice[0] = 42;
    }

    // Check stats
    let stats = arena.stats();
    println!("used={} free={}", stats.used(), stats.free());

    // Free in arbitrary order
    arena.free(p2);
    arena.free(p1);

    // After coalescing, stats should show all memory as free again.
    let stats2 = arena.stats();
    assert_eq!(stats2.used(), 0);
}
```

**Important:** The `Arena::new` call is marked unsafe in this example because the safety contract is conceptually unsafe: you must ensure the backing memory lives long enough. If the actual API is safe in the crate, treat this `unsafe` as conceptual.

---

## Integration patterns

### Bitcoin / blockchain infrastructure

The intended use case mirrors high‑load, low‑latency subsystems:

- Pre‑allocate large regions for:
  - Transaction graphs,
  - UTXO entries,
  - In‑memory indices or caches.
- Use `Arena` to avoid per‑object allocations on the global heap, reducing allocator contention.
- Track occupancy and fragmentation via `stats` and periodic logging.

### FFI interop

When bridging with C/C++ code that expects contiguous memory or arena‑style allocation:

- Provide the C side with `base` and pointer offsets,
- Or give out raw pointers from `alloc` directly,
- Ensure that FFI code never attempts to free through the C runtime but delegates deallocation back to `Arena::free`.

### Domain‑specific allocators

The `ArenaSizeToChunkSortedMap` / `ArenaChunkToSizeMap` alias types can be leveraged to build:

- Thread‑local sub‑arenas,
- Size‑segregated pools tuned to specific allocation patterns,
- Custom instrumentation layers that record allocation lifetimes.

---

## Performance characteristics

Abstract complexity (where `n` ≈ number of free chunks):

- Allocation: `O(log n)` for the best‑fit search, plus red‑black tree updates for removing/reinserting free chunks.
- Free: `O(log n)` for coalescing (two neighbor lookups + updates).
- Stats: `O(m)` where `m` ≈ number of used + free chunks (summing sizes and counting).

In well‑behaved workloads, `n` is bounded by fragmentation; with consistent coalescing, fragmentation remains moderate, giving stable performance.

Note that the allocator is **not thread‑safe** by itself. External synchronization is required if `Arena` is shared across threads.

---

## Error handling

- `alloc` returns a null pointer on out‑of‑memory; callers must check for this condition.
- `free` will:
  - Ignore `null` pointers,
  - Panic on pointers outside the arena,
  - Panic on invalid or double frees.

For hardened production systems, consider wrapping `Arena` to:

- Translate panics into explicit error handling strategies,
- Add stricter debug assertions or logging,
- Introduce domain‑specific invariants.

---

## License and authorship

- **License:** MIT
- **Authors:** `klebs6 <tpk3.mx@gmail.com>`

You are encouraged to review the source before deploying this allocator in security‑ or correctness‑critical environments, particularly where memory safety is central.
